{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for running ERDiff alignment step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model_functions.Diffusion import *\n",
    "from model_functions.VAE import *\n",
    "from model_functions.ERDiff_utils import *\n",
    "from model_functions.MLA_Model import *\n",
    "from model_functions.VAE_Readout import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Source Domain: Training\n",
    "\n",
    "Extract spatio-temporal structure for source domain  \n",
    "Code for training is `VAE_Diffusion_CoTrain.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load source domain data\n",
    "\n",
    "- trial spikes: neural firing rates\n",
    "- trial vel: velocity\n",
    "- trial dir: labels of direction, 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the trial length and number of neurons to use\n",
    "len_trial,num_neurons = 37, 187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/Neural_Source.pkl', 'rb') as f:\n",
    "    train_data1 = pickle.load(f)['data']\n",
    "train_trial_spikes1, train_trial_vel1, train_trial_dir1 = train_data1['firing_rates'], train_data1['velocity'], train_data1['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 209 trials, for each trial, the time length to use is 37 (minimal trial length - 1), number of neurons is 187, dimension of velocity is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal trial length is: 38\n"
     ]
    }
   ],
   "source": [
    "print(\"minimal trial length is:\", min(train_trial_spikes1[i].shape[0] for i in range(len(train_trial_spikes1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for training neural signal: (209, 37, 187)\n",
      "Shape for training velocity: (209, 37, 2)\n"
     ]
    }
   ],
   "source": [
    "start_pos = 1 \n",
    "\n",
    "train_trial_spikes_tide1 = np.array([spike[start_pos:len_trial+start_pos, :num_neurons] for spike in train_trial_spikes1])\n",
    "print(\"Shape for training neural signal:\", np.shape(train_trial_spikes_tide1))\n",
    "\n",
    "train_trial_vel_tide1 = np.array([spike[start_pos:len_trial+start_pos, :] for spike in train_trial_vel1])\n",
    "print(\"Shape for training velocity:\", np.shape(train_trial_vel_tide1))\n",
    "\n",
    "train_trial_spikes_tide = train_trial_spikes_tide1\n",
    "train_trial_vel_tide = train_trial_vel_tide1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209,)\n"
     ]
    }
   ],
   "source": [
    "array_train_trial_dir1 = np.expand_dims(np.array((train_trial_dir1), dtype=object),1)\n",
    "\n",
    "train_trial_spikes_tide = train_trial_spikes_tide1\n",
    "train_trial_vel_tide = train_trial_vel_tide1\n",
    "train_trial_dic_tide = np.squeeze(np.vstack([array_train_trial_dir1]))\n",
    "print(np.shape(train_trial_dic_tide))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data preprocessing\n",
    "\n",
    "Apply gaussian smoother  \n",
    "\n",
    "$$\n",
    "w(n)=\\exp^{-\\frac{1}{2}(\\frac{n}{\\sigma})^2}\n",
    "$$\n",
    "\n",
    "`scipy.signal.windows.gaussian(M, std, sym=True)`\n",
    "```\n",
    "Parameters:\t\n",
    "- M : int, number of points in the output window. If zero or less, an empty array is returned.\n",
    "- std : float, the standard deviation, sigma.\n",
    "- sym : bool, optional, when True (default), generates a symmetric window, for use in filter design. When False, generates a periodic window, for use in spectral analysis.\n",
    "\n",
    "Returns:\t\n",
    "- w : ndarray, The window, with the maximum value normalized to 1 (though the value 1 does not appear if M is even and sym is True).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = std = 5\n",
      "window: [0.19205063 0.20392638 0.20804597 0.20392638 0.19205063]\n"
     ]
    }
   ],
   "source": [
    "bin_width = float(0.02) * 1000 # /ms\n",
    "\n",
    "kern_sd_ms = 100\n",
    "kern_sd = int(round(kern_sd_ms / bin_width))\n",
    "print(\"M = std =\", kern_sd)\n",
    "window = signal.windows.gaussian(kern_sd, kern_sd, sym=True)\n",
    "window /= np.sum(window)\n",
    "print(\"window:\", window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply convolution filter  \n",
    "\n",
    "$$(v1 * v2) [n] = \\sum v1[m] v2[n-m]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = lambda x: np.convolve(x, window, 'same')\n",
    "train_trial_spikes_smoothed = np.apply_along_axis(filt, 1, train_trial_spikes_tide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Randomly shuffle and split to train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(train_trial_spikes_tide.shape[0])\n",
    "np.random.seed(2023) \n",
    "np.random.shuffle(indices)\n",
    "train_len = round(len(indices) * 0.80)\n",
    "real_train_trial_spikes_smed, val_trial_spikes_smed = train_trial_spikes_smoothed[indices[:train_len]], train_trial_spikes_smoothed[indices[train_len:]]\n",
    "real_train_trial_vel_tide, val_trial_vel_tide = train_trial_vel_tide[indices[:train_len]], train_trial_vel_tide[indices[train_len:]]\n",
    "real_train_trial_dic_tide, val_trial_dic_tide = train_trial_dic_tide[indices[:train_len]], train_trial_dic_tide[indices[train_len:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_trial_spikes_stand = (real_train_trial_spikes_smed)\n",
    "val_trial_spikes_stand = (val_trial_spikes_smed)\n",
    "\n",
    "spike_train = Variable(torch.from_numpy(real_train_trial_spikes_stand)).float()\n",
    "spike_val = Variable(torch.from_numpy(val_trial_spikes_stand)).float()\n",
    "\n",
    "emg_train = Variable(torch.from_numpy(real_train_trial_vel_tide)).float()\n",
    "emg_val = Variable(torch.from_numpy(val_trial_vel_tide)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 10\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "batch_size = 16\n",
    "global_batch_size = 16\n",
    "ae_res_weight = 10 # scale for spike reconstruction loss\n",
    "kld_weight = 1 # scale for KL-Divergence loss\n",
    "n_batches = len(real_train_trial_spikes_smed)//batch_size\n",
    "print(\"number of batches:\", n_batches)\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "poisson_criterion = nn.PoissonNLLLoss(log_input=False)\n",
    "\n",
    "l_rate = 0.001 # learning rate\n",
    "timesteps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, AE reconstruction loss, emg loss, kl-divergence loss\n",
    "def get_loss(model, spike, emg):\n",
    "    re_sp_, vel_hat_, mu, log_var = model(spike, train_flag= True)\n",
    "    ae_loss = poisson_criterion(re_sp_, spike)\n",
    "    emg_loss = mse_criterion(vel_hat_, emg)\n",
    "    kld_loss = torch.mean(0.5 * (- log_var + mu ** 2 + log_var.exp() - 1))\n",
    "    total_loss = ae_res_weight * ae_loss + emg_loss + kld_weight * kld_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "setup_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "model = VAE_Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheny/anaconda3/envs/torch/lib/python3.11/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "dm_model = diff_STBlock(input_dim)\n",
    "dm_model.to(device)\n",
    "\n",
    "dm_optimizer = Adam(dm_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_loss = 1e10 # Save the loss\n",
    "pre_total_loss_ = 1e18\n",
    "vae_loss_list = []\n",
    "diff_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1408/643933650.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(n_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b01c23d9054f4384ca401a7193db80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0  Loss: 0.8186498284339905\n",
      "Step 1  Loss: 0.8044883608818054\n",
      "Step 2  Loss: 0.8157080411911011\n",
      "Step 3  Loss: 0.7985531687736511\n",
      "Step 4  Loss: 0.7848435640335083\n",
      "Step 5  Loss: 0.7880468368530273\n",
      "Step 6  Loss: 0.7691790461540222\n",
      "Step 7  Loss: 0.7987245321273804\n",
      "Step 8  Loss: 0.7530821561813354\n",
      "Step 9  Loss: 0.7631310224533081\n",
      "Step 10  Loss: 0.7303891777992249\n",
      "total Loss of epoch  1  is  8.624795734882355\n",
      "Step 0  Loss: 0.7383076548576355\n",
      "Step 1  Loss: 0.7348331809043884\n",
      "Step 2  Loss: 0.7234946489334106\n",
      "Step 3  Loss: 0.7092326879501343\n",
      "Step 4  Loss: 0.7228999733924866\n",
      "Step 5  Loss: 0.7175029516220093\n",
      "Step 6  Loss: 0.6981369256973267\n",
      "Step 7  Loss: 0.6672934889793396\n",
      "Step 8  Loss: 0.6532304883003235\n",
      "Step 9  Loss: 0.6326757669448853\n",
      "Step 10  Loss: 0.6543505787849426\n",
      "total Loss of epoch  2  is  7.651958346366882\n",
      "Step 0  Loss: 0.6528767943382263\n",
      "Step 1  Loss: 0.5653676986694336\n",
      "Step 2  Loss: 0.613252580165863\n",
      "Step 3  Loss: 0.5598500370979309\n",
      "Step 4  Loss: 0.6057943105697632\n",
      "Step 5  Loss: 0.5613921284675598\n",
      "Step 6  Loss: 0.5253192782402039\n",
      "Step 7  Loss: 0.594080924987793\n",
      "Step 8  Loss: 0.5395373702049255\n",
      "Step 9  Loss: 0.5034772753715515\n",
      "Step 10  Loss: 0.49122875928878784\n",
      "total Loss of epoch  3  is  6.212177157402039\n",
      "Step 0  Loss: 0.565995991230011\n",
      "Step 1  Loss: 0.4686298370361328\n",
      "Step 2  Loss: 0.5330504179000854\n",
      "Step 3  Loss: 0.5203279256820679\n",
      "Step 4  Loss: 0.4500526189804077\n",
      "Step 5  Loss: 0.5482801198959351\n",
      "Step 6  Loss: 0.4754401445388794\n",
      "Step 7  Loss: 0.3874351978302002\n",
      "Step 8  Loss: 0.4423164129257202\n",
      "Step 9  Loss: 0.4506375193595886\n",
      "Step 10  Loss: 0.45622095465660095\n",
      "total Loss of epoch  4  is  5.298387140035629\n",
      "Step 0  Loss: 0.44963857531547546\n",
      "Step 1  Loss: 0.4951900243759155\n",
      "Step 2  Loss: 0.4202254116535187\n",
      "Step 3  Loss: 0.5405604839324951\n",
      "Step 4  Loss: 0.4744079113006592\n",
      "Step 5  Loss: 0.3551912009716034\n",
      "Step 6  Loss: 0.48504745960235596\n",
      "Step 7  Loss: 0.5114949941635132\n",
      "Step 8  Loss: 0.43773961067199707\n",
      "Step 9  Loss: 0.47027960419654846\n",
      "Step 10  Loss: 0.6618216037750244\n",
      "total Loss of epoch  5  is  5.3015968799591064\n",
      "Step 0  Loss: 0.5813354253768921\n",
      "Step 1  Loss: 0.506000816822052\n",
      "Step 2  Loss: 0.5019928216934204\n",
      "Step 3  Loss: 0.5085506439208984\n",
      "Step 4  Loss: 0.4045657217502594\n",
      "Step 5  Loss: 0.5129362940788269\n",
      "Step 6  Loss: 0.5549793243408203\n",
      "Step 7  Loss: 0.6279358267784119\n",
      "Step 8  Loss: 0.6569840908050537\n",
      "Step 9  Loss: 0.4503818154335022\n",
      "Step 10  Loss: 0.46904486417770386\n",
      "total Loss of epoch  6  is  5.774707645177841\n",
      "Step 0  Loss: 0.593610405921936\n",
      "Step 1  Loss: 0.495902955532074\n",
      "Step 2  Loss: 0.48472434282302856\n",
      "Step 3  Loss: 0.5744785070419312\n",
      "Step 4  Loss: 0.5662798285484314\n",
      "Step 5  Loss: 0.5412389039993286\n",
      "Step 6  Loss: 0.4200103282928467\n",
      "Step 7  Loss: 0.4568231403827667\n",
      "Step 8  Loss: 0.39875632524490356\n",
      "Step 9  Loss: 0.4434055685997009\n",
      "Step 10  Loss: 0.3834168314933777\n",
      "total Loss of epoch  7  is  5.358647137880325\n",
      "Step 0  Loss: 0.4841814935207367\n",
      "Step 1  Loss: 0.4606946110725403\n",
      "Step 2  Loss: 0.4816156327724457\n",
      "Step 3  Loss: 0.5263602137565613\n",
      "Step 4  Loss: 0.3039800524711609\n",
      "Step 5  Loss: 0.4545883536338806\n",
      "Step 6  Loss: 0.5286484956741333\n",
      "Step 7  Loss: 0.4867939352989197\n",
      "Step 8  Loss: 0.5023351907730103\n",
      "Step 9  Loss: 0.6291778683662415\n",
      "Step 10  Loss: 0.5574550032615662\n",
      "total Loss of epoch  8  is  5.415830850601196\n",
      "Step 0  Loss: 0.44553178548812866\n",
      "Step 1  Loss: 0.49472835659980774\n",
      "Step 2  Loss: 0.38256320357322693\n",
      "Step 3  Loss: 0.4698423445224762\n",
      "Step 4  Loss: 0.4729263186454773\n",
      "Step 5  Loss: 0.646721601486206\n",
      "Step 6  Loss: 0.4000144898891449\n",
      "Step 7  Loss: 0.5399463772773743\n",
      "Step 8  Loss: 0.46939560770988464\n",
      "Step 9  Loss: 0.4099350571632385\n",
      "Step 10  Loss: 0.5383784174919128\n",
      "total Loss of epoch  9  is  5.269983559846878\n",
      "Step 0  Loss: 0.5210827589035034\n",
      "Step 1  Loss: 0.4457796514034271\n",
      "Step 2  Loss: 0.5188344717025757\n",
      "Step 3  Loss: 0.5959522724151611\n",
      "Step 4  Loss: 0.4839193522930145\n",
      "Step 5  Loss: 0.5068855285644531\n",
      "Step 6  Loss: 0.41233018040657043\n",
      "Step 7  Loss: 0.5475764274597168\n",
      "Step 8  Loss: 0.46727055311203003\n",
      "Step 9  Loss: 0.41757407784461975\n",
      "Step 10  Loss: 0.6243790984153748\n",
      "total Loss of epoch  10  is  5.541584372520447\n",
      "Step 0  Loss: 0.4912589192390442\n",
      "Step 1  Loss: 0.4325917661190033\n",
      "Step 2  Loss: 0.4535047113895416\n",
      "Step 3  Loss: 0.5439471006393433\n",
      "Step 4  Loss: 0.5200080871582031\n",
      "Step 5  Loss: 0.48371484875679016\n",
      "Step 6  Loss: 0.4693376123905182\n",
      "Step 7  Loss: 0.5811511874198914\n",
      "Step 8  Loss: 0.4651331603527069\n",
      "Step 9  Loss: 0.5075238943099976\n",
      "Step 10  Loss: 0.4086712896823883\n",
      "total Loss of epoch  11  is  5.356842577457428\n",
      "Step 0  Loss: 0.4757900834083557\n",
      "Step 1  Loss: 0.4422217607498169\n",
      "Step 2  Loss: 0.4263221323490143\n",
      "Step 3  Loss: 0.5258908867835999\n",
      "Step 4  Loss: 0.5445677638053894\n",
      "Step 5  Loss: 0.4490174949169159\n",
      "Step 6  Loss: 0.48078933358192444\n",
      "Step 7  Loss: 0.3777443766593933\n",
      "Step 8  Loss: 0.531401515007019\n",
      "Step 9  Loss: 0.46663835644721985\n",
      "Step 10  Loss: 0.4099718928337097\n",
      "total Loss of epoch  12  is  5.130355596542358\n",
      "Step 0  Loss: 0.4807821214199066\n",
      "Step 1  Loss: 0.5149267315864563\n",
      "Step 2  Loss: 0.45147013664245605\n",
      "Step 3  Loss: 0.5632431507110596\n",
      "Step 4  Loss: 0.5255613923072815\n",
      "Step 5  Loss: 0.4521771967411041\n",
      "Step 6  Loss: 0.5537054538726807\n",
      "Step 7  Loss: 0.5855867266654968\n",
      "Step 8  Loss: 0.5622861981391907\n",
      "Step 9  Loss: 0.4007381796836853\n",
      "Step 10  Loss: 0.5496805310249329\n",
      "total Loss of epoch  13  is  5.6401578187942505\n",
      "Step 0  Loss: 0.464508980512619\n",
      "Step 1  Loss: 0.40435776114463806\n",
      "Step 2  Loss: 0.5918712019920349\n",
      "Step 3  Loss: 0.44507819414138794\n",
      "Step 4  Loss: 0.5563703775405884\n",
      "Step 5  Loss: 0.5433745980262756\n",
      "Step 6  Loss: 0.46047571301460266\n",
      "Step 7  Loss: 0.4195820391178131\n",
      "Step 8  Loss: 0.5195817351341248\n",
      "Step 9  Loss: 0.5549853444099426\n",
      "Step 10  Loss: 0.5211391448974609\n",
      "total Loss of epoch  14  is  5.481325089931488\n",
      "Step 0  Loss: 0.4991396367549896\n",
      "Step 1  Loss: 0.5021324753761292\n",
      "Step 2  Loss: 0.5475228428840637\n",
      "Step 3  Loss: 0.5823929905891418\n",
      "Step 4  Loss: 0.5365403890609741\n",
      "Step 5  Loss: 0.5208166241645813\n",
      "Step 6  Loss: 0.3990813195705414\n",
      "Step 7  Loss: 0.49962398409843445\n",
      "Step 8  Loss: 0.6068108677864075\n",
      "Step 9  Loss: 0.39695754647254944\n",
      "Step 10  Loss: 0.6059737205505371\n",
      "total Loss of epoch  15  is  5.69699239730835\n",
      "Step 0  Loss: 0.5624296069145203\n",
      "Step 1  Loss: 0.47483760118484497\n",
      "Step 2  Loss: 0.48684948682785034\n",
      "Step 3  Loss: 0.43019476532936096\n",
      "Step 4  Loss: 0.5538293719291687\n",
      "Step 5  Loss: 0.515830934047699\n",
      "Step 6  Loss: 0.4396943747997284\n",
      "Step 7  Loss: 0.5102212429046631\n",
      "Step 8  Loss: 0.47645455598831177\n",
      "Step 9  Loss: 0.5471784472465515\n",
      "Step 10  Loss: 0.3591154217720032\n",
      "total Loss of epoch  16  is  5.356635808944702\n",
      "Step 0  Loss: 0.4937765300273895\n",
      "Step 1  Loss: 0.45011281967163086\n",
      "Step 2  Loss: 0.5624998211860657\n",
      "Step 3  Loss: 0.3817831873893738\n",
      "Step 4  Loss: 0.517952024936676\n",
      "Step 5  Loss: 0.5103074908256531\n",
      "Step 6  Loss: 0.512535572052002\n",
      "Step 7  Loss: 0.45989248156547546\n",
      "Step 8  Loss: 0.527306318283081\n",
      "Step 9  Loss: 0.5405160784721375\n",
      "Step 10  Loss: 0.5140774250030518\n",
      "total Loss of epoch  17  is  5.470759749412537\n",
      "Step 0  Loss: 0.45959654450416565\n",
      "Step 1  Loss: 0.5437753200531006\n",
      "Step 2  Loss: 0.4896978437900543\n",
      "Step 3  Loss: 0.4953211843967438\n",
      "Step 4  Loss: 0.5352356433868408\n",
      "Step 5  Loss: 0.3894631862640381\n",
      "Step 6  Loss: 0.4796238839626312\n",
      "Step 7  Loss: 0.5227208733558655\n",
      "Step 8  Loss: 0.5068932771682739\n",
      "Step 9  Loss: 0.5031154751777649\n",
      "Step 10  Loss: 0.3945575952529907\n",
      "total Loss of epoch  18  is  5.3200008273124695\n",
      "Step 0  Loss: 0.5249256491661072\n",
      "Step 1  Loss: 0.4270855784416199\n",
      "Step 2  Loss: 0.4650327265262604\n",
      "Step 3  Loss: 0.5277325510978699\n",
      "Step 4  Loss: 0.44244012236595154\n",
      "Step 5  Loss: 0.4237506687641144\n",
      "Step 6  Loss: 0.6086000204086304\n",
      "Step 7  Loss: 0.571178138256073\n",
      "Step 8  Loss: 0.611665666103363\n",
      "Step 9  Loss: 0.5677086710929871\n",
      "Step 10  Loss: 0.6183493733406067\n",
      "total Loss of epoch  19  is  5.788469165563583\n",
      "Step 0  Loss: 0.5446585416793823\n",
      "Step 1  Loss: 0.4569045305252075\n",
      "Step 2  Loss: 0.5014620423316956\n",
      "Step 3  Loss: 0.5421925187110901\n",
      "Step 4  Loss: 0.5812955498695374\n",
      "Step 5  Loss: 0.5026301741600037\n",
      "Step 6  Loss: 0.45540180802345276\n",
      "Step 7  Loss: 0.5631120800971985\n",
      "Step 8  Loss: 0.26779451966285706\n",
      "Step 9  Loss: 0.494488388299942\n",
      "Step 10  Loss: 0.27930980920791626\n",
      "total Loss of epoch  20  is  5.189249962568283\n",
      "Step 0  Loss: 0.505458414554596\n",
      "Step 1  Loss: 0.545346736907959\n",
      "Step 2  Loss: 0.47771918773651123\n",
      "Step 3  Loss: 0.4560984969139099\n",
      "Step 4  Loss: 0.4667239189147949\n",
      "Step 5  Loss: 0.4523291289806366\n",
      "Step 6  Loss: 0.4329649806022644\n",
      "Step 7  Loss: 0.48894304037094116\n",
      "Step 8  Loss: 0.44783416390419006\n",
      "Step 9  Loss: 0.4402560591697693\n",
      "Step 10  Loss: 0.6631486415863037\n",
      "total Loss of epoch  21  is  5.376822769641876\n",
      "Step 0  Loss: 0.44320231676101685\n",
      "Step 1  Loss: 0.5448160767555237\n",
      "Step 2  Loss: 0.5056853890419006\n",
      "Step 3  Loss: 0.4674668312072754\n",
      "Step 4  Loss: 0.41438406705856323\n",
      "Step 5  Loss: 0.4336659610271454\n",
      "Step 6  Loss: 0.5928149223327637\n",
      "Step 7  Loss: 0.49295759201049805\n",
      "Step 8  Loss: 0.43584999442100525\n",
      "Step 9  Loss: 0.5373937487602234\n",
      "Step 10  Loss: 0.4888860583305359\n",
      "total Loss of epoch  22  is  5.357122957706451\n",
      "Step 0  Loss: 0.40675875544548035\n",
      "Step 1  Loss: 0.5897383093833923\n",
      "Step 2  Loss: 0.5049578547477722\n",
      "Step 3  Loss: 0.5757030844688416\n",
      "Step 4  Loss: 0.5402753949165344\n",
      "Step 5  Loss: 0.4516696035861969\n",
      "Step 6  Loss: 0.48392361402511597\n",
      "Step 7  Loss: 0.5936456918716431\n",
      "Step 8  Loss: 0.46358588337898254\n",
      "Step 9  Loss: 0.4841095209121704\n",
      "Step 10  Loss: 0.39044082164764404\n",
      "total Loss of epoch  23  is  5.484808534383774\n",
      "Step 0  Loss: 0.6226720809936523\n",
      "Step 1  Loss: 0.5674452185630798\n",
      "Step 2  Loss: 0.6038848161697388\n",
      "Step 3  Loss: 0.45222437381744385\n",
      "Step 4  Loss: 0.4129958748817444\n",
      "Step 5  Loss: 0.5502884387969971\n",
      "Step 6  Loss: 0.519043505191803\n",
      "Step 7  Loss: 0.5294539928436279\n",
      "Step 8  Loss: 0.4062836170196533\n",
      "Step 9  Loss: 0.534254252910614\n",
      "Step 10  Loss: 0.5426572561264038\n",
      "total Loss of epoch  24  is  5.741203427314758\n",
      "Step 0  Loss: 0.4608443081378937\n",
      "Step 1  Loss: 0.5831988453865051\n",
      "Step 2  Loss: 0.49512115120887756\n",
      "Step 3  Loss: 0.5424031615257263\n",
      "Step 4  Loss: 0.41570061445236206\n",
      "Step 5  Loss: 0.3841870129108429\n",
      "Step 6  Loss: 0.32152336835861206\n",
      "Step 7  Loss: 0.5723572969436646\n",
      "Step 8  Loss: 0.5852022767066956\n",
      "Step 9  Loss: 0.49544399976730347\n",
      "Step 10  Loss: 0.48330652713775635\n",
      "total Loss of epoch  25  is  5.33928856253624\n",
      "Step 0  Loss: 0.46206411719322205\n",
      "Step 1  Loss: 0.42857521772384644\n",
      "Step 2  Loss: 0.5738939046859741\n",
      "Step 3  Loss: 0.5656797289848328\n",
      "Step 4  Loss: 0.41857418417930603\n",
      "Step 5  Loss: 0.5174984931945801\n",
      "Step 6  Loss: 0.46656063199043274\n",
      "Step 7  Loss: 0.4873456656932831\n",
      "Step 8  Loss: 0.43499043583869934\n",
      "Step 9  Loss: 0.39342114329338074\n",
      "Step 10  Loss: 0.4731650948524475\n",
      "total Loss of epoch  26  is  5.221768617630005\n",
      "Step 0  Loss: 0.40644192695617676\n",
      "Step 1  Loss: 0.3894452154636383\n",
      "Step 2  Loss: 0.48786431550979614\n",
      "Step 3  Loss: 0.5111328959465027\n",
      "Step 4  Loss: 0.4897681474685669\n",
      "Step 5  Loss: 0.5803606510162354\n",
      "Step 6  Loss: 0.5143938064575195\n",
      "Step 7  Loss: 0.45256713032722473\n",
      "Step 8  Loss: 0.41415929794311523\n",
      "Step 9  Loss: 0.49312710762023926\n",
      "Step 10  Loss: 0.5318458080291748\n",
      "total Loss of epoch  27  is  5.27110630273819\n",
      "Step 0  Loss: 0.4047526717185974\n",
      "Step 1  Loss: 0.46117812395095825\n",
      "Step 2  Loss: 0.5098114013671875\n",
      "Step 3  Loss: 0.38811808824539185\n",
      "Step 4  Loss: 0.47318965196609497\n",
      "Step 5  Loss: 0.6477541327476501\n",
      "Step 6  Loss: 0.43034085631370544\n",
      "Step 7  Loss: 0.5390425324440002\n",
      "Step 8  Loss: 0.3810209035873413\n",
      "Step 9  Loss: 0.5444263815879822\n",
      "Step 10  Loss: 0.36908048391342163\n",
      "total Loss of epoch  28  is  5.148715227842331\n",
      "Step 0  Loss: 0.4938298761844635\n",
      "Step 1  Loss: 0.491014689207077\n",
      "Step 2  Loss: 0.47101640701293945\n",
      "Step 3  Loss: 0.5122872591018677\n",
      "Step 4  Loss: 0.45084232091903687\n",
      "Step 5  Loss: 0.45345640182495117\n",
      "Step 6  Loss: 0.51630699634552\n",
      "Step 7  Loss: 0.4692775011062622\n",
      "Step 8  Loss: 0.4395175576210022\n",
      "Step 9  Loss: 0.4398193359375\n",
      "Step 10  Loss: 0.63691246509552\n",
      "total Loss of epoch  29  is  5.37428081035614\n",
      "Step 0  Loss: 0.5927993655204773\n",
      "Step 1  Loss: 0.43249696493148804\n",
      "Step 2  Loss: 0.45753493905067444\n",
      "Step 3  Loss: 0.4549053907394409\n",
      "Step 4  Loss: 0.5015241503715515\n",
      "Step 5  Loss: 0.48752573132514954\n",
      "Step 6  Loss: 0.5375960469245911\n",
      "Step 7  Loss: 0.5237196683883667\n",
      "Step 8  Loss: 0.49393701553344727\n",
      "Step 9  Loss: 0.5397851467132568\n",
      "Step 10  Loss: 0.4548869729042053\n",
      "total Loss of epoch  30  is  5.476711392402649\n",
      "Step 0  Loss: 0.4761330783367157\n",
      "Step 1  Loss: 0.4918963313102722\n",
      "Step 2  Loss: 0.5667049884796143\n",
      "Step 3  Loss: 0.40425780415534973\n",
      "Step 4  Loss: 0.42478621006011963\n",
      "Step 5  Loss: 0.42974668741226196\n",
      "Step 6  Loss: 0.45960596203804016\n",
      "Step 7  Loss: 0.3850380778312683\n",
      "Step 8  Loss: 0.39996615052223206\n",
      "Step 9  Loss: 0.5255013704299927\n",
      "Step 10  Loss: 0.6130204796791077\n",
      "total Loss of epoch  31  is  5.176657140254974\n",
      "Step 0  Loss: 0.4672451615333557\n",
      "Step 1  Loss: 0.45221394300460815\n",
      "Step 2  Loss: 0.5479508638381958\n",
      "Step 3  Loss: 0.6054900288581848\n",
      "Step 4  Loss: 0.6549111604690552\n",
      "Step 5  Loss: 0.47902798652648926\n",
      "Step 6  Loss: 0.41370633244514465\n",
      "Step 7  Loss: 0.5202639698982239\n",
      "Step 8  Loss: 0.5142875909805298\n",
      "Step 9  Loss: 0.5122963786125183\n",
      "Step 10  Loss: 0.6224207282066345\n",
      "total Loss of epoch  32  is  5.78981414437294\n",
      "Step 0  Loss: 0.5143176913261414\n",
      "Step 1  Loss: 0.5665987133979797\n",
      "Step 2  Loss: 0.4284346103668213\n",
      "Step 3  Loss: 0.3172256350517273\n",
      "Step 4  Loss: 0.5501611828804016\n",
      "Step 5  Loss: 0.5060823559761047\n",
      "Step 6  Loss: 0.5426974296569824\n",
      "Step 7  Loss: 0.38905513286590576\n",
      "Step 8  Loss: 0.4581640064716339\n",
      "Step 9  Loss: 0.40131330490112305\n",
      "Step 10  Loss: 0.4397829473018646\n",
      "total Loss of epoch  33  is  5.113833010196686\n",
      "Step 0  Loss: 0.48584499955177307\n",
      "Step 1  Loss: 0.4392504394054413\n",
      "Step 2  Loss: 0.48280438780784607\n",
      "Step 3  Loss: 0.4884317219257355\n",
      "Step 4  Loss: 0.4032648205757141\n",
      "Step 5  Loss: 0.47963517904281616\n",
      "Step 6  Loss: 0.5702314972877502\n",
      "Step 7  Loss: 0.4360560476779938\n",
      "Step 8  Loss: 0.4442982077598572\n",
      "Step 9  Loss: 0.4413635730743408\n",
      "Step 10  Loss: 0.3414119780063629\n",
      "total Loss of epoch  34  is  5.012592852115631\n",
      "Step 0  Loss: 0.3664763569831848\n",
      "Step 1  Loss: 0.4991111159324646\n",
      "Step 2  Loss: 0.3831721544265747\n",
      "Step 3  Loss: 0.5287174582481384\n",
      "Step 4  Loss: 0.45990392565727234\n",
      "Step 5  Loss: 0.3383043110370636\n",
      "Step 6  Loss: 0.4830833673477173\n",
      "Step 7  Loss: 0.4269239604473114\n",
      "Step 8  Loss: 0.4881969094276428\n",
      "Step 9  Loss: 0.5448715090751648\n",
      "Step 10  Loss: 0.29146134853363037\n",
      "total Loss of epoch  35  is  4.810222417116165\n",
      "Step 0  Loss: 0.47064507007598877\n",
      "Step 1  Loss: 0.300576388835907\n",
      "Step 2  Loss: 0.4312499165534973\n",
      "Step 3  Loss: 0.4041871130466461\n",
      "Step 4  Loss: 0.40205398201942444\n",
      "Step 5  Loss: 0.48980966210365295\n",
      "Step 6  Loss: 0.5604219436645508\n",
      "Step 7  Loss: 0.4173806607723236\n",
      "Step 8  Loss: 0.5210147500038147\n",
      "Step 9  Loss: 0.47986841201782227\n",
      "Step 10  Loss: 0.5288859605789185\n",
      "total Loss of epoch  36  is  5.006093859672546\n",
      "Step 0  Loss: 0.5026791095733643\n",
      "Step 1  Loss: 0.4142606556415558\n",
      "Step 2  Loss: 0.36866527795791626\n",
      "Step 3  Loss: 0.4229561984539032\n",
      "Step 4  Loss: 0.5112830996513367\n",
      "Step 5  Loss: 0.39760157465934753\n",
      "Step 6  Loss: 0.40708261728286743\n",
      "Step 7  Loss: 0.39243537187576294\n",
      "Step 8  Loss: 0.4806954264640808\n",
      "Step 9  Loss: 0.5336689352989197\n",
      "Step 10  Loss: 0.5599566698074341\n",
      "total Loss of epoch  37  is  4.991284936666489\n",
      "Step 0  Loss: 0.440170019865036\n",
      "Step 1  Loss: 0.40655672550201416\n",
      "Step 2  Loss: 0.4156380593776703\n",
      "Step 3  Loss: 0.4378628730773926\n",
      "Step 4  Loss: 0.35230401158332825\n",
      "Step 5  Loss: 0.3493863344192505\n",
      "Step 6  Loss: 0.46644267439842224\n",
      "Step 7  Loss: 0.45563703775405884\n",
      "Step 8  Loss: 0.3886030316352844\n",
      "Step 9  Loss: 0.4873153567314148\n",
      "Step 10  Loss: 0.38995981216430664\n",
      "total Loss of epoch  38  is  4.589875936508179\n",
      "Step 0  Loss: 0.4399223327636719\n",
      "Step 1  Loss: 0.39006921648979187\n",
      "Step 2  Loss: 0.40486130118370056\n",
      "Step 3  Loss: 0.41240212321281433\n",
      "Step 4  Loss: 0.49789324402809143\n",
      "Step 5  Loss: 0.4727018177509308\n",
      "Step 6  Loss: 0.4110642969608307\n",
      "Step 7  Loss: 0.4815590977668762\n",
      "Step 8  Loss: 0.4496811032295227\n",
      "Step 9  Loss: 0.529132604598999\n",
      "Step 10  Loss: 0.48946258425712585\n",
      "total Loss of epoch  39  is  4.978749722242355\n",
      "Step 0  Loss: 0.43644723296165466\n",
      "Step 1  Loss: 0.3461678922176361\n",
      "Step 2  Loss: 0.4732576310634613\n",
      "Step 3  Loss: 0.4753803610801697\n",
      "Step 4  Loss: 0.566898763179779\n",
      "Step 5  Loss: 0.5628775358200073\n",
      "Step 6  Loss: 0.3896237909793854\n",
      "Step 7  Loss: 0.5271545648574829\n",
      "Step 8  Loss: 0.43894684314727783\n",
      "Step 9  Loss: 0.47464680671691895\n",
      "Step 10  Loss: 0.48266687989234924\n",
      "total Loss of epoch  40  is  5.174068301916122\n",
      "Step 0  Loss: 0.5177156329154968\n",
      "Step 1  Loss: 0.5405182242393494\n",
      "Step 2  Loss: 0.48064324259757996\n",
      "Step 3  Loss: 0.4619375169277191\n",
      "Step 4  Loss: 0.48970165848731995\n",
      "Step 5  Loss: 0.3990935683250427\n",
      "Step 6  Loss: 0.4742608070373535\n",
      "Step 7  Loss: 0.41365480422973633\n",
      "Step 8  Loss: 0.4503577947616577\n",
      "Step 9  Loss: 0.4466121792793274\n",
      "Step 10  Loss: 0.4262143075466156\n",
      "total Loss of epoch  41  is  5.1007097363471985\n",
      "Step 0  Loss: 0.40845540165901184\n",
      "Step 1  Loss: 0.5141938924789429\n",
      "Step 2  Loss: 0.4226332902908325\n",
      "Step 3  Loss: 0.39313235878944397\n",
      "Step 4  Loss: 0.37056267261505127\n",
      "Step 5  Loss: 0.3277099132537842\n",
      "Step 6  Loss: 0.4921143651008606\n",
      "Step 7  Loss: 0.522589921951294\n",
      "Step 8  Loss: 0.47160741686820984\n",
      "Step 9  Loss: 0.42055732011795044\n",
      "Step 10  Loss: 0.451796293258667\n",
      "total Loss of epoch  42  is  4.7953528463840485\n",
      "Step 0  Loss: 0.5868021249771118\n",
      "Step 1  Loss: 0.42327365279197693\n",
      "Step 2  Loss: 0.33505871891975403\n",
      "Step 3  Loss: 0.5418297648429871\n",
      "Step 4  Loss: 0.3560401201248169\n",
      "Step 5  Loss: 0.3818996250629425\n",
      "Step 6  Loss: 0.5672600269317627\n",
      "Step 7  Loss: 0.3887530565261841\n",
      "Step 8  Loss: 0.46072912216186523\n",
      "Step 9  Loss: 0.4748747646808624\n",
      "Step 10  Loss: 0.46957090497016907\n",
      "total Loss of epoch  43  is  4.986091881990433\n",
      "Step 0  Loss: 0.3004162013530731\n",
      "Step 1  Loss: 0.4358942210674286\n",
      "Step 2  Loss: 0.5413405895233154\n",
      "Step 3  Loss: 0.45501554012298584\n",
      "Step 4  Loss: 0.4433508813381195\n",
      "Step 5  Loss: 0.466359943151474\n",
      "Step 6  Loss: 0.38403022289276123\n",
      "Step 7  Loss: 0.42295584082603455\n",
      "Step 8  Loss: 0.4922361671924591\n",
      "Step 9  Loss: 0.37409621477127075\n",
      "Step 10  Loss: 0.5394716858863831\n",
      "total Loss of epoch  44  is  4.855167508125305\n",
      "Step 0  Loss: 0.39944323897361755\n",
      "Step 1  Loss: 0.39469942450523376\n",
      "Step 2  Loss: 0.5147616863250732\n",
      "Step 3  Loss: 0.4176730513572693\n",
      "Step 4  Loss: 0.4999338686466217\n",
      "Step 5  Loss: 0.42396870255470276\n",
      "Step 6  Loss: 0.42741459608078003\n",
      "Step 7  Loss: 0.5582396388053894\n",
      "Step 8  Loss: 0.39351433515548706\n",
      "Step 9  Loss: 0.43874797224998474\n",
      "Step 10  Loss: 0.4922821819782257\n",
      "total Loss of epoch  45  is  4.960678696632385\n",
      "Step 0  Loss: 0.4312591254711151\n",
      "Step 1  Loss: 0.5154339671134949\n",
      "Step 2  Loss: 0.3981005549430847\n",
      "Step 3  Loss: 0.4408533275127411\n",
      "Step 4  Loss: 0.4989035427570343\n",
      "Step 5  Loss: 0.46499669551849365\n",
      "Step 6  Loss: 0.5874829292297363\n",
      "Step 7  Loss: 0.5444890856742859\n",
      "Step 8  Loss: 0.5449566841125488\n",
      "Step 9  Loss: 0.4667275846004486\n",
      "Step 10  Loss: 0.5221893191337585\n",
      "total Loss of epoch  46  is  5.415392816066742\n",
      "Step 0  Loss: 0.49691107869148254\n",
      "Step 1  Loss: 0.4454592764377594\n",
      "Step 2  Loss: 0.49546176195144653\n",
      "Step 3  Loss: 0.503407895565033\n",
      "Step 4  Loss: 0.5208686590194702\n",
      "Step 5  Loss: 0.34597814083099365\n",
      "Step 6  Loss: 0.525704562664032\n",
      "Step 7  Loss: 0.5560566782951355\n",
      "Step 8  Loss: 0.44112637639045715\n",
      "Step 9  Loss: 0.37564677000045776\n",
      "Step 10  Loss: 0.40154048800468445\n",
      "total Loss of epoch  47  is  5.108161687850952\n",
      "Step 0  Loss: 0.46771296858787537\n",
      "Step 1  Loss: 0.4135708510875702\n",
      "Step 2  Loss: 0.5103620290756226\n",
      "Step 3  Loss: 0.4337065517902374\n",
      "Step 4  Loss: 0.44369104504585266\n",
      "Step 5  Loss: 0.4438461661338806\n",
      "Step 6  Loss: 0.3995908498764038\n",
      "Step 7  Loss: 0.37526077032089233\n",
      "Step 8  Loss: 0.5222240686416626\n",
      "Step 9  Loss: 0.4777088761329651\n",
      "Step 10  Loss: 0.40071189403533936\n",
      "total Loss of epoch  48  is  4.888386070728302\n",
      "Step 0  Loss: 0.4097321033477783\n",
      "Step 1  Loss: 0.44632184505462646\n",
      "Step 2  Loss: 0.36657363176345825\n",
      "Step 3  Loss: 0.4354569613933563\n",
      "Step 4  Loss: 0.4599655866622925\n",
      "Step 5  Loss: 0.438157320022583\n",
      "Step 6  Loss: 0.41042953729629517\n",
      "Step 7  Loss: 0.3143976926803589\n",
      "Step 8  Loss: 0.4492632746696472\n",
      "Step 9  Loss: 0.4379996359348297\n",
      "Step 10  Loss: 0.5156657099723816\n",
      "total Loss of epoch  49  is  4.683963298797607\n",
      "Step 0  Loss: 0.6033704280853271\n",
      "Step 1  Loss: 0.5670725703239441\n",
      "Step 2  Loss: 0.48619869351387024\n",
      "Step 3  Loss: 0.34715962409973145\n",
      "Step 4  Loss: 0.4305083453655243\n",
      "Step 5  Loss: 0.45324021577835083\n",
      "Step 6  Loss: 0.551089346408844\n",
      "Step 7  Loss: 0.4106755256652832\n",
      "Step 8  Loss: 0.5376618504524231\n",
      "Step 9  Loss: 0.4739972651004791\n",
      "Step 10  Loss: 0.6128360033035278\n",
      "total Loss of epoch  50  is  5.473809868097305\n",
      "Step 0  Loss: 0.4509021043777466\n",
      "Step 1  Loss: 0.4608071446418762\n",
      "Step 2  Loss: 0.3933974504470825\n",
      "Step 3  Loss: 0.30474889278411865\n",
      "Step 4  Loss: 0.42454826831817627\n",
      "Step 5  Loss: 0.4356454908847809\n",
      "Step 6  Loss: 0.3867068588733673\n",
      "Step 7  Loss: 0.39997145533561707\n",
      "Step 8  Loss: 0.4893338084220886\n",
      "Step 9  Loss: 0.48586809635162354\n",
      "Step 10  Loss: 0.506249725818634\n",
      "total Loss of epoch  51  is  4.738179296255112\n",
      "Step 0  Loss: 0.4021221995353699\n",
      "Step 1  Loss: 0.4577024281024933\n",
      "Step 2  Loss: 0.40203943848609924\n",
      "Step 3  Loss: 0.36194586753845215\n",
      "Step 4  Loss: 0.4182225167751312\n",
      "Step 5  Loss: 0.5051979422569275\n",
      "Step 6  Loss: 0.3982754647731781\n",
      "Step 7  Loss: 0.4315696656703949\n",
      "Step 8  Loss: 0.3111360967159271\n",
      "Step 9  Loss: 0.47734883427619934\n",
      "Step 10  Loss: 0.48246535658836365\n",
      "total Loss of epoch  52  is  4.648025810718536\n",
      "Step 0  Loss: 0.3450676500797272\n",
      "Step 1  Loss: 0.41965875029563904\n",
      "Step 2  Loss: 0.3973279595375061\n",
      "Step 3  Loss: 0.3708890974521637\n",
      "Step 4  Loss: 0.3252947926521301\n",
      "Step 5  Loss: 0.6053205132484436\n",
      "Step 6  Loss: 0.2834467887878418\n",
      "Step 7  Loss: 0.35901495814323425\n",
      "Step 8  Loss: 0.5322427749633789\n",
      "Step 9  Loss: 0.4497853219509125\n",
      "Step 10  Loss: 0.40940290689468384\n",
      "total Loss of epoch  53  is  4.497451514005661\n",
      "Step 0  Loss: 0.48810943961143494\n",
      "Step 1  Loss: 0.41462305188179016\n",
      "Step 2  Loss: 0.3426106572151184\n",
      "Step 3  Loss: 0.3679795265197754\n",
      "Step 4  Loss: 0.3567114770412445\n",
      "Step 5  Loss: 0.29680168628692627\n",
      "Step 6  Loss: 0.3576565682888031\n",
      "Step 7  Loss: 0.4339678883552551\n",
      "Step 8  Loss: 0.4060898721218109\n",
      "Step 9  Loss: 0.42672988772392273\n",
      "Step 10  Loss: 0.42592811584472656\n",
      "total Loss of epoch  54  is  4.317208170890808\n",
      "Step 0  Loss: 0.3089670240879059\n",
      "Step 1  Loss: 0.4346155822277069\n",
      "Step 2  Loss: 0.4624720513820648\n",
      "Step 3  Loss: 0.4265533983707428\n",
      "Step 4  Loss: 0.43890106678009033\n",
      "Step 5  Loss: 0.44104236364364624\n",
      "Step 6  Loss: 0.48264080286026\n",
      "Step 7  Loss: 0.4047597050666809\n",
      "Step 8  Loss: 0.41528597474098206\n",
      "Step 9  Loss: 0.43400049209594727\n",
      "Step 10  Loss: 0.4064563512802124\n",
      "total Loss of epoch  55  is  4.65569481253624\n",
      "Step 0  Loss: 0.3113483786582947\n",
      "Step 1  Loss: 0.4063037037849426\n",
      "Step 2  Loss: 0.39533910155296326\n",
      "Step 3  Loss: 0.3630504608154297\n",
      "Step 4  Loss: 0.3594324588775635\n",
      "Step 5  Loss: 0.3996725082397461\n",
      "Step 6  Loss: 0.412447988986969\n",
      "Step 7  Loss: 0.3273159861564636\n",
      "Step 8  Loss: 0.34238824248313904\n",
      "Step 9  Loss: 0.4524846374988556\n",
      "Step 10  Loss: 0.4197750687599182\n",
      "total Loss of epoch  56  is  4.189558535814285\n",
      "Step 0  Loss: 0.44607481360435486\n",
      "Step 1  Loss: 0.47525230050086975\n",
      "Step 2  Loss: 0.45861414074897766\n",
      "Step 3  Loss: 0.3761373460292816\n",
      "Step 4  Loss: 0.4646831154823303\n",
      "Step 5  Loss: 0.42468610405921936\n",
      "Step 6  Loss: 0.3889264762401581\n",
      "Step 7  Loss: 0.48649391531944275\n",
      "Step 8  Loss: 0.4052693247795105\n",
      "Step 9  Loss: 0.46236029267311096\n",
      "Step 10  Loss: 0.42468029260635376\n",
      "total Loss of epoch  57  is  4.81317812204361\n",
      "Step 0  Loss: 0.43269234895706177\n",
      "Step 1  Loss: 0.4405907392501831\n",
      "Step 2  Loss: 0.4077077805995941\n",
      "Step 3  Loss: 0.41348695755004883\n",
      "Step 4  Loss: 0.4827733337879181\n",
      "Step 5  Loss: 0.47995227575302124\n",
      "Step 6  Loss: 0.3808247745037079\n",
      "Step 7  Loss: 0.4083423316478729\n",
      "Step 8  Loss: 0.4577084183692932\n",
      "Step 9  Loss: 0.4585385322570801\n",
      "Step 10  Loss: 0.39263445138931274\n",
      "total Loss of epoch  58  is  4.755251944065094\n",
      "Step 0  Loss: 0.32828691601753235\n",
      "Step 1  Loss: 0.39850637316703796\n",
      "Step 2  Loss: 0.4599132537841797\n",
      "Step 3  Loss: 0.43537795543670654\n",
      "Step 4  Loss: 0.4205087721347809\n",
      "Step 5  Loss: 0.34668025374412537\n",
      "Step 6  Loss: 0.4129349887371063\n",
      "Step 7  Loss: 0.5549182891845703\n",
      "Step 8  Loss: 0.4301910400390625\n",
      "Step 9  Loss: 0.46064749360084534\n",
      "Step 10  Loss: 0.476054310798645\n",
      "total Loss of epoch  59  is  4.724019646644592\n",
      "Step 0  Loss: 0.43058714270591736\n",
      "Step 1  Loss: 0.4115722179412842\n",
      "Step 2  Loss: 0.4044228196144104\n",
      "Step 3  Loss: 0.41129279136657715\n",
      "Step 4  Loss: 0.3292336165904999\n",
      "Step 5  Loss: 0.3799038529396057\n",
      "Step 6  Loss: 0.451171875\n",
      "Step 7  Loss: 0.5818979144096375\n",
      "Step 8  Loss: 0.44719943404197693\n",
      "Step 9  Loss: 0.40997400879859924\n",
      "Step 10  Loss: 0.4521297514438629\n",
      "total Loss of epoch  60  is  4.709385424852371\n",
      "Step 0  Loss: 0.4449279308319092\n",
      "Step 1  Loss: 0.45397821068763733\n",
      "Step 2  Loss: 0.5008435249328613\n",
      "Step 3  Loss: 0.3549627959728241\n",
      "Step 4  Loss: 0.43833738565444946\n",
      "Step 5  Loss: 0.4452207386493683\n",
      "Step 6  Loss: 0.3798234760761261\n",
      "Step 7  Loss: 0.4261784553527832\n",
      "Step 8  Loss: 0.34838980436325073\n",
      "Step 9  Loss: 0.4808315634727478\n",
      "Step 10  Loss: 0.4040820896625519\n",
      "total Loss of epoch  61  is  4.677575975656509\n",
      "Step 0  Loss: 0.32022255659103394\n",
      "Step 1  Loss: 0.4367234408855438\n",
      "Step 2  Loss: 0.4092506170272827\n",
      "Step 3  Loss: 0.42079493403434753\n",
      "Step 4  Loss: 0.3569777309894562\n",
      "Step 5  Loss: 0.40711891651153564\n",
      "Step 6  Loss: 0.48193275928497314\n",
      "Step 7  Loss: 0.4084911048412323\n",
      "Step 8  Loss: 0.38361114263534546\n",
      "Step 9  Loss: 0.4115762412548065\n",
      "Step 10  Loss: 0.3525012731552124\n",
      "total Loss of epoch  62  is  4.38920071721077\n",
      "Step 0  Loss: 0.4436274468898773\n",
      "Step 1  Loss: 0.4623250663280487\n",
      "Step 2  Loss: 0.37679609656333923\n",
      "Step 3  Loss: 0.351493239402771\n",
      "Step 4  Loss: 0.38149067759513855\n",
      "Step 5  Loss: 0.5381411910057068\n",
      "Step 6  Loss: 0.43827494978904724\n",
      "Step 7  Loss: 0.3156374990940094\n",
      "Step 8  Loss: 0.4450610876083374\n",
      "Step 9  Loss: 0.3169478178024292\n",
      "Step 10  Loss: 0.370452880859375\n",
      "total Loss of epoch  63  is  4.44024795293808\n",
      "Step 0  Loss: 0.30742397904396057\n",
      "Step 1  Loss: 0.4398079514503479\n",
      "Step 2  Loss: 0.4152105748653412\n",
      "Step 3  Loss: 0.3478785753250122\n",
      "Step 4  Loss: 0.5103098154067993\n",
      "Step 5  Loss: 0.3792967200279236\n",
      "Step 6  Loss: 0.4441038966178894\n",
      "Step 7  Loss: 0.44635283946990967\n",
      "Step 8  Loss: 0.3751595914363861\n",
      "Step 9  Loss: 0.48364171385765076\n",
      "Step 10  Loss: 0.34525421261787415\n",
      "total Loss of epoch  64  is  4.494439870119095\n",
      "Step 0  Loss: 0.4272480309009552\n",
      "Step 1  Loss: 0.3642374873161316\n",
      "Step 2  Loss: 0.42932185530662537\n",
      "Step 3  Loss: 0.3874996602535248\n",
      "Step 4  Loss: 0.5263715386390686\n",
      "Step 5  Loss: 0.46861740946769714\n",
      "Step 6  Loss: 0.4373498260974884\n",
      "Step 7  Loss: 0.3533480763435364\n",
      "Step 8  Loss: 0.3347751796245575\n",
      "Step 9  Loss: 0.4909428656101227\n",
      "Step 10  Loss: 0.39782729744911194\n",
      "total Loss of epoch  65  is  4.61753922700882\n",
      "Step 0  Loss: 0.2136700451374054\n",
      "Step 1  Loss: 0.40989580750465393\n",
      "Step 2  Loss: 0.33912980556488037\n",
      "Step 3  Loss: 0.4100245535373688\n",
      "Step 4  Loss: 0.401593953371048\n",
      "Step 5  Loss: 0.3937129080295563\n",
      "Step 6  Loss: 0.4459042549133301\n",
      "Step 7  Loss: 0.5475743412971497\n",
      "Step 8  Loss: 0.45280084013938904\n",
      "Step 9  Loss: 0.3548271656036377\n",
      "Step 10  Loss: 0.6117287874221802\n",
      "total Loss of epoch  66  is  4.580862462520599\n",
      "Step 0  Loss: 0.38394981622695923\n",
      "Step 1  Loss: 0.4244946539402008\n",
      "Step 2  Loss: 0.3497643768787384\n",
      "Step 3  Loss: 0.359728068113327\n",
      "Step 4  Loss: 0.3493213951587677\n",
      "Step 5  Loss: 0.36509519815444946\n",
      "Step 6  Loss: 0.4988667368888855\n",
      "Step 7  Loss: 0.5221017599105835\n",
      "Step 8  Loss: 0.43213558197021484\n",
      "Step 9  Loss: 0.46642714738845825\n",
      "Step 10  Loss: 0.42212411761283875\n",
      "total Loss of epoch  67  is  4.5740088522434235\n",
      "Step 0  Loss: 0.4698895514011383\n",
      "Step 1  Loss: 0.3610702157020569\n",
      "Step 2  Loss: 0.327438622713089\n",
      "Step 3  Loss: 0.49918103218078613\n",
      "Step 4  Loss: 0.4408540427684784\n",
      "Step 5  Loss: 0.4218277931213379\n",
      "Step 6  Loss: 0.3887385427951813\n",
      "Step 7  Loss: 0.45034950971603394\n",
      "Step 8  Loss: 0.43219271302223206\n",
      "Step 9  Loss: 0.36047863960266113\n",
      "Step 10  Loss: 0.4150523841381073\n",
      "total Loss of epoch  68  is  4.567073047161102\n",
      "Step 0  Loss: 0.4454100430011749\n",
      "Step 1  Loss: 0.4855164587497711\n",
      "Step 2  Loss: 0.3711293637752533\n",
      "Step 3  Loss: 0.43500158190727234\n",
      "Step 4  Loss: 0.444559246301651\n",
      "Step 5  Loss: 0.4447633922100067\n",
      "Step 6  Loss: 0.43882834911346436\n",
      "Step 7  Loss: 0.42110952734947205\n",
      "Step 8  Loss: 0.47767895460128784\n",
      "Step 9  Loss: 0.4428044855594635\n",
      "Step 10  Loss: 0.3595986068248749\n",
      "total Loss of epoch  69  is  4.766400009393692\n",
      "Step 0  Loss: 0.4547477960586548\n",
      "Step 1  Loss: 0.5283395648002625\n",
      "Step 2  Loss: 0.45435723662376404\n",
      "Step 3  Loss: 0.3446640372276306\n",
      "Step 4  Loss: 0.4909990131855011\n",
      "Step 5  Loss: 0.37697136402130127\n",
      "Step 6  Loss: 0.4688819646835327\n",
      "Step 7  Loss: 0.37142515182495117\n",
      "Step 8  Loss: 0.39979100227355957\n",
      "Step 9  Loss: 0.49258318543434143\n",
      "Step 10  Loss: 0.38732025027275085\n",
      "total Loss of epoch  70  is  4.77008056640625\n",
      "Step 0  Loss: 0.4444788992404938\n",
      "Step 1  Loss: 0.5131151676177979\n",
      "Step 2  Loss: 0.324768602848053\n",
      "Step 3  Loss: 0.2884546220302582\n",
      "Step 4  Loss: 0.5078596472740173\n",
      "Step 5  Loss: 0.3230006992816925\n",
      "Step 6  Loss: 0.34464558959007263\n",
      "Step 7  Loss: 0.398657888174057\n",
      "Step 8  Loss: 0.4011610746383667\n",
      "Step 9  Loss: 0.3520674407482147\n",
      "Step 10  Loss: 0.46961790323257446\n",
      "total Loss of epoch  71  is  4.367827534675598\n",
      "Step 0  Loss: 0.2849101424217224\n",
      "Step 1  Loss: 0.4990147650241852\n",
      "Step 2  Loss: 0.4061374068260193\n",
      "Step 3  Loss: 0.3938372731208801\n",
      "Step 4  Loss: 0.36952370405197144\n",
      "Step 5  Loss: 0.3129247725009918\n",
      "Step 6  Loss: 0.45223307609558105\n",
      "Step 7  Loss: 0.36758410930633545\n",
      "Step 8  Loss: 0.4593107998371124\n",
      "Step 9  Loss: 0.44505104422569275\n",
      "Step 10  Loss: 0.3696831464767456\n",
      "total Loss of epoch  72  is  4.3602102398872375\n",
      "Step 0  Loss: 0.35238051414489746\n",
      "Step 1  Loss: 0.5112954378128052\n",
      "Step 2  Loss: 0.4157754182815552\n",
      "Step 3  Loss: 0.4135558605194092\n",
      "Step 4  Loss: 0.3778267502784729\n",
      "Step 5  Loss: 0.3311424255371094\n",
      "Step 6  Loss: 0.4621913433074951\n",
      "Step 7  Loss: 0.4460589289665222\n",
      "Step 8  Loss: 0.4703517556190491\n",
      "Step 9  Loss: 0.4553104639053345\n",
      "Step 10  Loss: 0.6366000771522522\n",
      "total Loss of epoch  73  is  4.872488975524902\n",
      "Step 0  Loss: 0.36269769072532654\n",
      "Step 1  Loss: 0.40902891755104065\n",
      "Step 2  Loss: 0.36389121413230896\n",
      "Step 3  Loss: 0.43764448165893555\n",
      "Step 4  Loss: 0.4792958199977875\n",
      "Step 5  Loss: 0.4977341592311859\n",
      "Step 6  Loss: 0.27964964509010315\n",
      "Step 7  Loss: 0.45319846272468567\n",
      "Step 8  Loss: 0.5339687466621399\n",
      "Step 9  Loss: 0.33173272013664246\n",
      "Step 10  Loss: 0.3050040006637573\n",
      "total Loss of epoch  74  is  4.453845858573914\n",
      "Step 0  Loss: 0.4569655656814575\n",
      "Step 1  Loss: 0.34619584679603577\n",
      "Step 2  Loss: 0.4246046543121338\n",
      "Step 3  Loss: 0.32379719614982605\n",
      "Step 4  Loss: 0.3293059766292572\n",
      "Step 5  Loss: 0.44487473368644714\n",
      "Step 6  Loss: 0.3764399588108063\n",
      "Step 7  Loss: 0.46579211950302124\n",
      "Step 8  Loss: 0.2899922728538513\n",
      "Step 9  Loss: 0.35923588275909424\n",
      "Step 10  Loss: 0.2525961995124817\n",
      "total Loss of epoch  75  is  4.069800406694412\n",
      "Step 0  Loss: 0.4164956510066986\n",
      "Step 1  Loss: 0.4297235310077667\n",
      "Step 2  Loss: 0.420680433511734\n",
      "Step 3  Loss: 0.5050279498100281\n",
      "Step 4  Loss: 0.4700886011123657\n",
      "Step 5  Loss: 0.42477473616600037\n",
      "Step 6  Loss: 0.40280914306640625\n",
      "Step 7  Loss: 0.38028648495674133\n",
      "Step 8  Loss: 0.3757539391517639\n",
      "Step 9  Loss: 0.2884771525859833\n",
      "Step 10  Loss: 0.5896059274673462\n",
      "total Loss of epoch  76  is  4.7037235498428345\n",
      "Step 0  Loss: 0.3753359615802765\n",
      "Step 1  Loss: 0.5161822438240051\n",
      "Step 2  Loss: 0.4598846435546875\n",
      "Step 3  Loss: 0.41506779193878174\n",
      "Step 4  Loss: 0.2970321476459503\n",
      "Step 5  Loss: 0.39246666431427\n",
      "Step 6  Loss: 0.5029420256614685\n",
      "Step 7  Loss: 0.350928395986557\n",
      "Step 8  Loss: 0.29598018527030945\n",
      "Step 9  Loss: 0.42241552472114563\n",
      "Step 10  Loss: 0.3793957233428955\n",
      "total Loss of epoch  77  is  4.407631307840347\n",
      "Step 0  Loss: 0.44925108551979065\n",
      "Step 1  Loss: 0.4458085000514984\n",
      "Step 2  Loss: 0.39138755202293396\n",
      "Step 3  Loss: 0.44461432099342346\n",
      "Step 4  Loss: 0.5094553232192993\n",
      "Step 5  Loss: 0.39948856830596924\n",
      "Step 6  Loss: 0.34816834330558777\n",
      "Step 7  Loss: 0.26322317123413086\n",
      "Step 8  Loss: 0.4041317105293274\n",
      "Step 9  Loss: 0.3623902201652527\n",
      "Step 10  Loss: 0.529273271560669\n",
      "total Loss of epoch  78  is  4.547192066907883\n",
      "Step 0  Loss: 0.3916339576244354\n",
      "Step 1  Loss: 0.4691894054412842\n",
      "Step 2  Loss: 0.4408763647079468\n",
      "Step 3  Loss: 0.36698246002197266\n",
      "Step 4  Loss: 0.37877151370048523\n",
      "Step 5  Loss: 0.38115137815475464\n",
      "Step 6  Loss: 0.36786776781082153\n",
      "Step 7  Loss: 0.477886825799942\n",
      "Step 8  Loss: 0.3738812804222107\n",
      "Step 9  Loss: 0.4777628779411316\n",
      "Step 10  Loss: 0.41890043020248413\n",
      "total Loss of epoch  79  is  4.544904261827469\n",
      "Step 0  Loss: 0.5168390274047852\n",
      "Step 1  Loss: 0.29411065578460693\n",
      "Step 2  Loss: 0.3904394507408142\n",
      "Step 3  Loss: 0.4672279357910156\n",
      "Step 4  Loss: 0.3845341205596924\n",
      "Step 5  Loss: 0.4402768909931183\n",
      "Step 6  Loss: 0.34143251180648804\n",
      "Step 7  Loss: 0.38628849387168884\n",
      "Step 8  Loss: 0.3667733073234558\n",
      "Step 9  Loss: 0.3717813789844513\n",
      "Step 10  Loss: 0.5833120346069336\n",
      "total Loss of epoch  80  is  4.54301580786705\n",
      "Step 0  Loss: 0.3953366279602051\n",
      "Step 1  Loss: 0.34069210290908813\n",
      "Step 2  Loss: 0.5354024767875671\n",
      "Step 3  Loss: 0.3415665328502655\n",
      "Step 4  Loss: 0.3428519368171692\n",
      "Step 5  Loss: 0.3483525216579437\n",
      "Step 6  Loss: 0.5445733070373535\n",
      "Step 7  Loss: 0.3912377953529358\n",
      "Step 8  Loss: 0.4334147572517395\n",
      "Step 9  Loss: 0.47964560985565186\n",
      "Step 10  Loss: 0.37365034222602844\n",
      "total Loss of epoch  81  is  4.526724010705948\n",
      "Step 0  Loss: 0.5725433826446533\n",
      "Step 1  Loss: 0.33850324153900146\n",
      "Step 2  Loss: 0.34636005759239197\n",
      "Step 3  Loss: 0.3644561469554901\n",
      "Step 4  Loss: 0.356133371591568\n",
      "Step 5  Loss: 0.48012933135032654\n",
      "Step 6  Loss: 0.36739203333854675\n",
      "Step 7  Loss: 0.39483773708343506\n",
      "Step 8  Loss: 0.43864601850509644\n",
      "Step 9  Loss: 0.29560309648513794\n",
      "Step 10  Loss: 0.32397088408470154\n",
      "total Loss of epoch  82  is  4.278575301170349\n",
      "Step 0  Loss: 0.44350525736808777\n",
      "Step 1  Loss: 0.3383669853210449\n",
      "Step 2  Loss: 0.39862826466560364\n",
      "Step 3  Loss: 0.4185793101787567\n",
      "Step 4  Loss: 0.42620348930358887\n",
      "Step 5  Loss: 0.33937177062034607\n",
      "Step 6  Loss: 0.4251810312271118\n",
      "Step 7  Loss: 0.320449560880661\n",
      "Step 8  Loss: 0.41629523038864136\n",
      "Step 9  Loss: 0.38047972321510315\n",
      "Step 10  Loss: 0.36320796608924866\n",
      "total Loss of epoch  83  is  4.270268589258194\n",
      "Step 0  Loss: 0.38488873839378357\n",
      "Step 1  Loss: 0.39390620589256287\n",
      "Step 2  Loss: 0.33751267194747925\n",
      "Step 3  Loss: 0.4609997868537903\n",
      "Step 4  Loss: 0.3338662087917328\n",
      "Step 5  Loss: 0.4417809247970581\n",
      "Step 6  Loss: 0.5237196087837219\n",
      "Step 7  Loss: 0.3360562324523926\n",
      "Step 8  Loss: 0.3232661187648773\n",
      "Step 9  Loss: 0.4770449995994568\n",
      "Step 10  Loss: 0.3560740649700165\n",
      "total Loss of epoch  84  is  4.369115561246872\n",
      "Step 0  Loss: 0.29249489307403564\n",
      "Step 1  Loss: 0.4890228509902954\n",
      "Step 2  Loss: 0.3241594135761261\n",
      "Step 3  Loss: 0.40290290117263794\n",
      "Step 4  Loss: 0.44318687915802\n",
      "Step 5  Loss: 0.4975561201572418\n",
      "Step 6  Loss: 0.41413870453834534\n",
      "Step 7  Loss: 0.3030051589012146\n",
      "Step 8  Loss: 0.5224729776382446\n",
      "Step 9  Loss: 0.38990500569343567\n",
      "Step 10  Loss: 0.4965451657772064\n",
      "total Loss of epoch  85  is  4.575390070676804\n",
      "Step 0  Loss: 0.43342843651771545\n",
      "Step 1  Loss: 0.34994450211524963\n",
      "Step 2  Loss: 0.48593664169311523\n",
      "Step 3  Loss: 0.5145020484924316\n",
      "Step 4  Loss: 0.39875051379203796\n",
      "Step 5  Loss: 0.2619543969631195\n",
      "Step 6  Loss: 0.38461238145828247\n",
      "Step 7  Loss: 0.452667236328125\n",
      "Step 8  Loss: 0.43341177701950073\n",
      "Step 9  Loss: 0.5168207287788391\n",
      "Step 10  Loss: 0.474164217710495\n",
      "total Loss of epoch  86  is  4.706192880868912\n",
      "Step 0  Loss: 0.4473898112773895\n",
      "Step 1  Loss: 0.40359190106391907\n",
      "Step 2  Loss: 0.5475286245346069\n",
      "Step 3  Loss: 0.42955830693244934\n",
      "Step 4  Loss: 0.4162176549434662\n",
      "Step 5  Loss: 0.32933661341667175\n",
      "Step 6  Loss: 0.3934967517852783\n",
      "Step 7  Loss: 0.4779590964317322\n",
      "Step 8  Loss: 0.5575246214866638\n",
      "Step 9  Loss: 0.3638688027858734\n",
      "Step 10  Loss: 0.4659276008605957\n",
      "total Loss of epoch  87  is  4.832399785518646\n",
      "Step 0  Loss: 0.4478740096092224\n",
      "Step 1  Loss: 0.4665677845478058\n",
      "Step 2  Loss: 0.48265668749809265\n",
      "Step 3  Loss: 0.4946715831756592\n",
      "Step 4  Loss: 0.3832305073738098\n",
      "Step 5  Loss: 0.4124983549118042\n",
      "Step 6  Loss: 0.3500726521015167\n",
      "Step 7  Loss: 0.35413846373558044\n",
      "Step 8  Loss: 0.4312681555747986\n",
      "Step 9  Loss: 0.37330758571624756\n",
      "Step 10  Loss: 0.3707478642463684\n",
      "total Loss of epoch  88  is  4.567033648490906\n",
      "Step 0  Loss: 0.48364192247390747\n",
      "Step 1  Loss: 0.47531792521476746\n",
      "Step 2  Loss: 0.5093538165092468\n",
      "Step 3  Loss: 0.4062565863132477\n",
      "Step 4  Loss: 0.38350236415863037\n",
      "Step 5  Loss: 0.3270253539085388\n",
      "Step 6  Loss: 0.4661817252635956\n",
      "Step 7  Loss: 0.4562138617038727\n",
      "Step 8  Loss: 0.47404909133911133\n",
      "Step 9  Loss: 0.48213866353034973\n",
      "Step 10  Loss: 0.4782518148422241\n",
      "total Loss of epoch  89  is  4.941933125257492\n",
      "Step 0  Loss: 0.2859169840812683\n",
      "Step 1  Loss: 0.5147514343261719\n",
      "Step 2  Loss: 0.3485489785671234\n",
      "Step 3  Loss: 0.4543732702732086\n",
      "Step 4  Loss: 0.43337735533714294\n",
      "Step 5  Loss: 0.4742791950702667\n",
      "Step 6  Loss: 0.4701070487499237\n",
      "Step 7  Loss: 0.48430246114730835\n",
      "Step 8  Loss: 0.43106967210769653\n",
      "Step 9  Loss: 0.44505423307418823\n",
      "Step 10  Loss: 0.4741363823413849\n",
      "total Loss of epoch  90  is  4.815917015075684\n",
      "Step 0  Loss: 0.4333193898200989\n",
      "Step 1  Loss: 0.4145105183124542\n",
      "Step 2  Loss: 0.2852543294429779\n",
      "Step 3  Loss: 0.37383389472961426\n",
      "Step 4  Loss: 0.37373360991477966\n",
      "Step 5  Loss: 0.3526245951652527\n",
      "Step 6  Loss: 0.5328927040100098\n",
      "Step 7  Loss: 0.3674752712249756\n",
      "Step 8  Loss: 0.33207425475120544\n",
      "Step 9  Loss: 0.44133004546165466\n",
      "Step 10  Loss: 0.32686522603034973\n",
      "total Loss of epoch  91  is  4.233913838863373\n",
      "Step 0  Loss: 0.32558315992355347\n",
      "Step 1  Loss: 0.3744569420814514\n",
      "Step 2  Loss: 0.5880782008171082\n",
      "Step 3  Loss: 0.33324262499809265\n",
      "Step 4  Loss: 0.35132861137390137\n",
      "Step 5  Loss: 0.5092553496360779\n",
      "Step 6  Loss: 0.4046113193035126\n",
      "Step 7  Loss: 0.4841758608818054\n",
      "Step 8  Loss: 0.4585767090320587\n",
      "Step 9  Loss: 0.34365686774253845\n",
      "Step 10  Loss: 0.6062330007553101\n",
      "total Loss of epoch  92  is  4.77919864654541\n",
      "Step 0  Loss: 0.42778441309928894\n",
      "Step 1  Loss: 0.3810614347457886\n",
      "Step 2  Loss: 0.4924459159374237\n",
      "Step 3  Loss: 0.43184804916381836\n",
      "Step 4  Loss: 0.38554707169532776\n",
      "Step 5  Loss: 0.4916459619998932\n",
      "Step 6  Loss: 0.44866856932640076\n",
      "Step 7  Loss: 0.4529980421066284\n",
      "Step 8  Loss: 0.4624595642089844\n",
      "Step 9  Loss: 0.32288655638694763\n",
      "Step 10  Loss: 0.4360016882419586\n",
      "total Loss of epoch  93  is  4.73334726691246\n",
      "Step 0  Loss: 0.5011911392211914\n",
      "Step 1  Loss: 0.44631427526474\n",
      "Step 2  Loss: 0.3795136511325836\n",
      "Step 3  Loss: 0.36168402433395386\n",
      "Step 4  Loss: 0.37264060974121094\n",
      "Step 5  Loss: 0.4643406271934509\n",
      "Step 6  Loss: 0.458724707365036\n",
      "Step 7  Loss: 0.40482625365257263\n",
      "Step 8  Loss: 0.49958866834640503\n",
      "Step 9  Loss: 0.5009078979492188\n",
      "Step 10  Loss: 0.49720168113708496\n",
      "total Loss of epoch  94  is  4.886933535337448\n",
      "Step 0  Loss: 0.43837031722068787\n",
      "Step 1  Loss: 0.395559698343277\n",
      "Step 2  Loss: 0.4914458096027374\n",
      "Step 3  Loss: 0.38706859946250916\n",
      "Step 4  Loss: 0.3766055107116699\n",
      "Step 5  Loss: 0.4624965190887451\n",
      "Step 6  Loss: 0.3684559762477875\n",
      "Step 7  Loss: 0.4327665865421295\n",
      "Step 8  Loss: 0.4505259096622467\n",
      "Step 9  Loss: 0.45501765608787537\n",
      "Step 10  Loss: 0.4251370429992676\n",
      "total Loss of epoch  95  is  4.683449625968933\n",
      "Step 0  Loss: 0.3696885108947754\n",
      "Step 1  Loss: 0.40488356351852417\n",
      "Step 2  Loss: 0.49173855781555176\n",
      "Step 3  Loss: 0.44408169388771057\n",
      "Step 4  Loss: 0.4713587462902069\n",
      "Step 5  Loss: 0.3863477110862732\n",
      "Step 6  Loss: 0.5122241377830505\n",
      "Step 7  Loss: 0.4621766209602356\n",
      "Step 8  Loss: 0.43723368644714355\n",
      "Step 9  Loss: 0.3307948112487793\n",
      "Step 10  Loss: 0.36370649933815\n",
      "total Loss of epoch  96  is  4.674234539270401\n",
      "Step 0  Loss: 0.3440921902656555\n",
      "Step 1  Loss: 0.441897988319397\n",
      "Step 2  Loss: 0.4316979944705963\n",
      "Step 3  Loss: 0.4754198491573334\n",
      "Step 4  Loss: 0.35660192370414734\n",
      "Step 5  Loss: 0.3675161302089691\n",
      "Step 6  Loss: 0.33806076645851135\n",
      "Step 7  Loss: 0.3271201252937317\n",
      "Step 8  Loss: 0.363226979970932\n",
      "Step 9  Loss: 0.36832451820373535\n",
      "Step 10  Loss: 0.41891735792160034\n",
      "total Loss of epoch  97  is  4.232875823974609\n",
      "Step 0  Loss: 0.40996283292770386\n",
      "Step 1  Loss: 0.42784836888313293\n",
      "Step 2  Loss: 0.29529330134391785\n",
      "Step 3  Loss: 0.46885979175567627\n",
      "Step 4  Loss: 0.34839677810668945\n",
      "Step 5  Loss: 0.46293097734451294\n",
      "Step 6  Loss: 0.3964195251464844\n",
      "Step 7  Loss: 0.4715832471847534\n",
      "Step 8  Loss: 0.4288223385810852\n",
      "Step 9  Loss: 0.32216373085975647\n",
      "Step 10  Loss: 0.48618897795677185\n",
      "total Loss of epoch  98  is  4.518469870090485\n",
      "Step 0  Loss: 0.3744073212146759\n",
      "Step 1  Loss: 0.36093512177467346\n",
      "Step 2  Loss: 0.39130717515945435\n",
      "Step 3  Loss: 0.47087469696998596\n",
      "Step 4  Loss: 0.39461591839790344\n",
      "Step 5  Loss: 0.48627352714538574\n",
      "Step 6  Loss: 0.4468715190887451\n",
      "Step 7  Loss: 0.46553441882133484\n",
      "Step 8  Loss: 0.4055825173854828\n",
      "Step 9  Loss: 0.3768092691898346\n",
      "Step 10  Loss: 0.5422123074531555\n",
      "total Loss of epoch  99  is  4.715423792600632\n",
      "Step 0  Loss: 0.42736920714378357\n",
      "Step 1  Loss: 0.3144444525241852\n",
      "Step 2  Loss: 0.5329351425170898\n",
      "Step 3  Loss: 0.3667476177215576\n",
      "Step 4  Loss: 0.5035780668258667\n",
      "Step 5  Loss: 0.37807607650756836\n",
      "Step 6  Loss: 0.47261130809783936\n",
      "Step 7  Loss: 0.3581618666648865\n",
      "Step 8  Loss: 0.3356187045574188\n",
      "Step 9  Loss: 0.3750738799571991\n",
      "Step 10  Loss: 0.3938966691493988\n",
      "total Loss of epoch  100  is  4.458512991666794\n",
      "Step 0  Loss: 0.51763916015625\n",
      "Step 1  Loss: 0.5262895822525024\n",
      "Step 2  Loss: 0.3499627113342285\n",
      "Step 3  Loss: 0.460359662771225\n",
      "Step 4  Loss: 0.4876023232936859\n",
      "Step 5  Loss: 0.49577662348747253\n",
      "Step 6  Loss: 0.3677169680595398\n",
      "Step 7  Loss: 0.4639810621738434\n",
      "Step 8  Loss: 0.3390239179134369\n",
      "Step 9  Loss: 0.4063756763935089\n",
      "Step 10  Loss: 0.5266165137290955\n",
      "total Loss of epoch  101  is  4.941344201564789\n",
      "Step 0  Loss: 0.47851428389549255\n",
      "Step 1  Loss: 0.3311959207057953\n",
      "Step 2  Loss: 0.42294809222221375\n",
      "Step 3  Loss: 0.3840809464454651\n",
      "Step 4  Loss: 0.3877202272415161\n",
      "Step 5  Loss: 0.36826401948928833\n",
      "Step 6  Loss: 0.45056936144828796\n",
      "Step 7  Loss: 0.43282198905944824\n",
      "Step 8  Loss: 0.4267934262752533\n",
      "Step 9  Loss: 0.4618336856365204\n",
      "Step 10  Loss: 0.2961096167564392\n",
      "total Loss of epoch  102  is  4.44085156917572\n",
      "Step 0  Loss: 0.38112956285476685\n",
      "Step 1  Loss: 0.3110867440700531\n",
      "Step 2  Loss: 0.3151964545249939\n",
      "Step 3  Loss: 0.41028618812561035\n",
      "Step 4  Loss: 0.40639010071754456\n",
      "Step 5  Loss: 0.37719807028770447\n",
      "Step 6  Loss: 0.2573579251766205\n",
      "Step 7  Loss: 0.47252339124679565\n",
      "Step 8  Loss: 0.3814617991447449\n",
      "Step 9  Loss: 0.4180510640144348\n",
      "Step 10  Loss: 0.4697708189487457\n",
      "total Loss of epoch  103  is  4.200452119112015\n",
      "Step 0  Loss: 0.41700515151023865\n",
      "Step 1  Loss: 0.4103051424026489\n",
      "Step 2  Loss: 0.34541627764701843\n",
      "Step 3  Loss: 0.45229628682136536\n",
      "Step 4  Loss: 0.38413286209106445\n",
      "Step 5  Loss: 0.47831565141677856\n",
      "Step 6  Loss: 0.4644947052001953\n",
      "Step 7  Loss: 0.5199782848358154\n",
      "Step 8  Loss: 0.3901585340499878\n",
      "Step 9  Loss: 0.3841772973537445\n",
      "Step 10  Loss: 0.5199265480041504\n",
      "total Loss of epoch  104  is  4.766206741333008\n",
      "Step 0  Loss: 0.4341129660606384\n",
      "Step 1  Loss: 0.3693658411502838\n",
      "Step 2  Loss: 0.49769386649131775\n",
      "Step 3  Loss: 0.43416285514831543\n",
      "Step 4  Loss: 0.5202410221099854\n",
      "Step 5  Loss: 0.34450283646583557\n",
      "Step 6  Loss: 0.40103358030319214\n",
      "Step 7  Loss: 0.42077532410621643\n",
      "Step 8  Loss: 0.36048728227615356\n",
      "Step 9  Loss: 0.42761868238449097\n",
      "Step 10  Loss: 0.4750405251979828\n",
      "total Loss of epoch  105  is  4.685034781694412\n",
      "Step 0  Loss: 0.3716258704662323\n",
      "Step 1  Loss: 0.3864558935165405\n",
      "Step 2  Loss: 0.40852800011634827\n",
      "Step 3  Loss: 0.377602756023407\n",
      "Step 4  Loss: 0.3901500999927521\n",
      "Step 5  Loss: 0.3786109387874603\n",
      "Step 6  Loss: 0.34318140149116516\n",
      "Step 7  Loss: 0.46305784583091736\n",
      "Step 8  Loss: 0.3956681191921234\n",
      "Step 9  Loss: 0.4807763993740082\n",
      "Step 10  Loss: 0.4543900787830353\n",
      "total Loss of epoch  106  is  4.45004740357399\n",
      "Step 0  Loss: 0.4264655113220215\n",
      "Step 1  Loss: 0.34589555859565735\n",
      "Step 2  Loss: 0.3101882040500641\n",
      "Step 3  Loss: 0.5079210996627808\n",
      "Step 4  Loss: 0.42411375045776367\n",
      "Step 5  Loss: 0.4423927664756775\n",
      "Step 6  Loss: 0.3364819586277008\n",
      "Step 7  Loss: 0.40648314356803894\n",
      "Step 8  Loss: 0.5632399320602417\n",
      "Step 9  Loss: 0.21588028967380524\n",
      "Step 10  Loss: 0.4761386811733246\n",
      "total Loss of epoch  107  is  4.455200895667076\n",
      "Step 0  Loss: 0.3572661876678467\n",
      "Step 1  Loss: 0.4589660167694092\n",
      "Step 2  Loss: 0.4254566729068756\n",
      "Step 3  Loss: 0.4327278733253479\n",
      "Step 4  Loss: 0.4608304798603058\n",
      "Step 5  Loss: 0.37684115767478943\n",
      "Step 6  Loss: 0.47161751985549927\n",
      "Step 7  Loss: 0.4176158010959625\n",
      "Step 8  Loss: 0.40861666202545166\n",
      "Step 9  Loss: 0.2583737373352051\n",
      "Step 10  Loss: 0.32725563645362854\n",
      "total Loss of epoch  108  is  4.395567744970322\n",
      "Step 0  Loss: 0.3339316248893738\n",
      "Step 1  Loss: 0.351169615983963\n",
      "Step 2  Loss: 0.3790394961833954\n",
      "Step 3  Loss: 0.45780444145202637\n",
      "Step 4  Loss: 0.3182135820388794\n",
      "Step 5  Loss: 0.41231608390808105\n",
      "Step 6  Loss: 0.45875662565231323\n",
      "Step 7  Loss: 0.5188436508178711\n",
      "Step 8  Loss: 0.44230228662490845\n",
      "Step 9  Loss: 0.36026936769485474\n",
      "Step 10  Loss: 0.22406421601772308\n",
      "total Loss of epoch  109  is  4.25671099126339\n",
      "Step 0  Loss: 0.5015347599983215\n",
      "Step 1  Loss: 0.39840763807296753\n",
      "Step 2  Loss: 0.49898508191108704\n",
      "Step 3  Loss: 0.43442386388778687\n",
      "Step 4  Loss: 0.4567996859550476\n",
      "Step 5  Loss: 0.4266050159931183\n",
      "Step 6  Loss: 0.3793984353542328\n",
      "Step 7  Loss: 0.4790027141571045\n",
      "Step 8  Loss: 0.4919459819793701\n",
      "Step 9  Loss: 0.3039957880973816\n",
      "Step 10  Loss: 0.40132200717926025\n",
      "total Loss of epoch  110  is  4.772420972585678\n",
      "Step 0  Loss: 0.4427514970302582\n",
      "Step 1  Loss: 0.3405162990093231\n",
      "Step 2  Loss: 0.4416017234325409\n",
      "Step 3  Loss: 0.3482995629310608\n",
      "Step 4  Loss: 0.37644413113594055\n",
      "Step 5  Loss: 0.5236291885375977\n",
      "Step 6  Loss: 0.3750218152999878\n",
      "Step 7  Loss: 0.4244912564754486\n",
      "Step 8  Loss: 0.3796456754207611\n",
      "Step 9  Loss: 0.42299771308898926\n",
      "Step 10  Loss: 0.4366898834705353\n",
      "total Loss of epoch  111  is  4.512088745832443\n",
      "Step 0  Loss: 0.3704056739807129\n",
      "Step 1  Loss: 0.5044347643852234\n",
      "Step 2  Loss: 0.40532824397087097\n",
      "Step 3  Loss: 0.41571709513664246\n",
      "Step 4  Loss: 0.2537040710449219\n",
      "Step 5  Loss: 0.3189152181148529\n",
      "Step 6  Loss: 0.42894405126571655\n",
      "Step 7  Loss: 0.36231058835983276\n",
      "Step 8  Loss: 0.39020082354545593\n",
      "Step 9  Loss: 0.4382373094558716\n",
      "Step 10  Loss: 0.35441312193870544\n",
      "total Loss of epoch  112  is  4.242610961198807\n",
      "Step 0  Loss: 0.32142335176467896\n",
      "Step 1  Loss: 0.4834408164024353\n",
      "Step 2  Loss: 0.5363053679466248\n",
      "Step 3  Loss: 0.5381020903587341\n",
      "Step 4  Loss: 0.4953159689903259\n",
      "Step 5  Loss: 0.3045940697193146\n",
      "Step 6  Loss: 0.4183996915817261\n",
      "Step 7  Loss: 0.3126658499240875\n",
      "Step 8  Loss: 0.28743186593055725\n",
      "Step 9  Loss: 0.48860254883766174\n",
      "Step 10  Loss: 0.3780818283557892\n",
      "total Loss of epoch  113  is  4.564363449811935\n",
      "Step 0  Loss: 0.3820894658565521\n",
      "Step 1  Loss: 0.41135668754577637\n",
      "Step 2  Loss: 0.41682255268096924\n",
      "Step 3  Loss: 0.4576398432254791\n",
      "Step 4  Loss: 0.4046650826931\n",
      "Step 5  Loss: 0.3485676050186157\n",
      "Step 6  Loss: 0.3914702534675598\n",
      "Step 7  Loss: 0.3750653564929962\n",
      "Step 8  Loss: 0.43353983759880066\n",
      "Step 9  Loss: 0.39398452639579773\n",
      "Step 10  Loss: 0.33168351650238037\n",
      "total Loss of epoch  114  is  4.346884727478027\n",
      "Step 0  Loss: 0.46721839904785156\n",
      "Step 1  Loss: 0.28448331356048584\n",
      "Step 2  Loss: 0.362485408782959\n",
      "Step 3  Loss: 0.5144341588020325\n",
      "Step 4  Loss: 0.3409195840358734\n",
      "Step 5  Loss: 0.3148045539855957\n",
      "Step 6  Loss: 0.4001709222793579\n",
      "Step 7  Loss: 0.39870545268058777\n",
      "Step 8  Loss: 0.5057582259178162\n",
      "Step 9  Loss: 0.4652755856513977\n",
      "Step 10  Loss: 0.35727137327194214\n",
      "total Loss of epoch  115  is  4.4115269780159\n",
      "Step 0  Loss: 0.47645124793052673\n",
      "Step 1  Loss: 0.30259209871292114\n",
      "Step 2  Loss: 0.4150954782962799\n",
      "Step 3  Loss: 0.35531267523765564\n",
      "Step 4  Loss: 0.3850627541542053\n",
      "Step 5  Loss: 0.4030309021472931\n",
      "Step 6  Loss: 0.3985210657119751\n",
      "Step 7  Loss: 0.4670196771621704\n",
      "Step 8  Loss: 0.43129149079322815\n",
      "Step 9  Loss: 0.5042839646339417\n",
      "Step 10  Loss: 0.2524515986442566\n",
      "total Loss of epoch  116  is  4.391112953424454\n",
      "Step 0  Loss: 0.3887789845466614\n",
      "Step 1  Loss: 0.4178040623664856\n",
      "Step 2  Loss: 0.4056861698627472\n",
      "Step 3  Loss: 0.43005478382110596\n",
      "Step 4  Loss: 0.35220059752464294\n",
      "Step 5  Loss: 0.3903197646141052\n",
      "Step 6  Loss: 0.3452015817165375\n",
      "Step 7  Loss: 0.3968839943408966\n",
      "Step 8  Loss: 0.5658331513404846\n",
      "Step 9  Loss: 0.4504217803478241\n",
      "Step 10  Loss: 0.4282292127609253\n",
      "total Loss of epoch  117  is  4.571414083242416\n",
      "Step 0  Loss: 0.34250351786613464\n",
      "Step 1  Loss: 0.3074423670768738\n",
      "Step 2  Loss: 0.3540293574333191\n",
      "Step 3  Loss: 0.3672410845756531\n",
      "Step 4  Loss: 0.3133411407470703\n",
      "Step 5  Loss: 0.30310022830963135\n",
      "Step 6  Loss: 0.3610977232456207\n",
      "Step 7  Loss: 0.5200453400611877\n",
      "Step 8  Loss: 0.4848257005214691\n",
      "Step 9  Loss: 0.43765950202941895\n",
      "Step 10  Loss: 0.20352914929389954\n",
      "total Loss of epoch  118  is  3.9948151111602783\n",
      "Step 0  Loss: 0.3754800856113434\n",
      "Step 1  Loss: 0.5001381039619446\n",
      "Step 2  Loss: 0.36989593505859375\n",
      "Step 3  Loss: 0.4092726707458496\n",
      "Step 4  Loss: 0.46194520592689514\n",
      "Step 5  Loss: 0.5585933923721313\n",
      "Step 6  Loss: 0.4017294943332672\n",
      "Step 7  Loss: 0.42780473828315735\n",
      "Step 8  Loss: 0.3665107488632202\n",
      "Step 9  Loss: 0.4532044529914856\n",
      "Step 10  Loss: 0.2706224024295807\n",
      "total Loss of epoch  119  is  4.595197230577469\n",
      "Step 0  Loss: 0.3591375946998596\n",
      "Step 1  Loss: 0.49887168407440186\n",
      "Step 2  Loss: 0.42280033230781555\n",
      "Step 3  Loss: 0.38679438829421997\n",
      "Step 4  Loss: 0.36978623270988464\n",
      "Step 5  Loss: 0.34306514263153076\n",
      "Step 6  Loss: 0.3660454750061035\n",
      "Step 7  Loss: 0.40074023604393005\n",
      "Step 8  Loss: 0.522779643535614\n",
      "Step 9  Loss: 0.4204702377319336\n",
      "Step 10  Loss: 0.39034852385520935\n",
      "total Loss of epoch  120  is  4.480839490890503\n",
      "Step 0  Loss: 0.3759472668170929\n",
      "Step 1  Loss: 0.31804823875427246\n",
      "Step 2  Loss: 0.3674623370170593\n",
      "Step 3  Loss: 0.3516761362552643\n",
      "Step 4  Loss: 0.41954201459884644\n",
      "Step 5  Loss: 0.37793534994125366\n",
      "Step 6  Loss: 0.29923829436302185\n",
      "Step 7  Loss: 0.44077569246292114\n",
      "Step 8  Loss: 0.42967548966407776\n",
      "Step 9  Loss: 0.3910619020462036\n",
      "Step 10  Loss: 0.42819854617118835\n",
      "total Loss of epoch  121  is  4.199561268091202\n",
      "Step 0  Loss: 0.4334096610546112\n",
      "Step 1  Loss: 0.5275594592094421\n",
      "Step 2  Loss: 0.47577887773513794\n",
      "Step 3  Loss: 0.2818607687950134\n",
      "Step 4  Loss: 0.42235031723976135\n",
      "Step 5  Loss: 0.4321680963039398\n",
      "Step 6  Loss: 0.442825585603714\n",
      "Step 7  Loss: 0.45335155725479126\n",
      "Step 8  Loss: 0.4638688266277313\n",
      "Step 9  Loss: 0.4100126624107361\n",
      "Step 10  Loss: 0.2655743956565857\n",
      "total Loss of epoch  122  is  4.608760207891464\n",
      "Step 0  Loss: 0.3066740334033966\n",
      "Step 1  Loss: 0.35208749771118164\n",
      "Step 2  Loss: 0.49947232007980347\n",
      "Step 3  Loss: 0.49616706371307373\n",
      "Step 4  Loss: 0.33863624930381775\n",
      "Step 5  Loss: 0.3101919889450073\n",
      "Step 6  Loss: 0.3182764947414398\n",
      "Step 7  Loss: 0.47172608971595764\n",
      "Step 8  Loss: 0.4032030403614044\n",
      "Step 9  Loss: 0.48539990186691284\n",
      "Step 10  Loss: 0.2949371039867401\n",
      "total Loss of epoch  123  is  4.276771783828735\n",
      "Step 0  Loss: 0.47812163829803467\n",
      "Step 1  Loss: 0.37978649139404297\n",
      "Step 2  Loss: 0.36347559094429016\n",
      "Step 3  Loss: 0.44905710220336914\n",
      "Step 4  Loss: 0.4333411157131195\n",
      "Step 5  Loss: 0.41548284888267517\n",
      "Step 6  Loss: 0.34977102279663086\n",
      "Step 7  Loss: 0.4359307587146759\n",
      "Step 8  Loss: 0.44385379552841187\n",
      "Step 9  Loss: 0.5366755127906799\n",
      "Step 10  Loss: 0.3638679087162018\n",
      "total Loss of epoch  124  is  4.649363785982132\n",
      "Step 0  Loss: 0.35478147864341736\n",
      "Step 1  Loss: 0.2130432426929474\n",
      "Step 2  Loss: 0.4287922978401184\n",
      "Step 3  Loss: 0.26740992069244385\n",
      "Step 4  Loss: 0.26765233278274536\n",
      "Step 5  Loss: 0.3244692087173462\n",
      "Step 6  Loss: 0.32225921750068665\n",
      "Step 7  Loss: 0.4178085923194885\n",
      "Step 8  Loss: 0.3875654935836792\n",
      "Step 9  Loss: 0.3415316045284271\n",
      "Step 10  Loss: 0.3896476924419403\n",
      "total Loss of epoch  125  is  3.7149610817432404\n",
      "Step 0  Loss: 0.378265917301178\n",
      "Step 1  Loss: 0.35563287138938904\n",
      "Step 2  Loss: 0.49562159180641174\n",
      "Step 3  Loss: 0.42596235871315\n",
      "Step 4  Loss: 0.46755415201187134\n",
      "Step 5  Loss: 0.40366119146347046\n",
      "Step 6  Loss: 0.38027048110961914\n",
      "Step 7  Loss: 0.4834524989128113\n",
      "Step 8  Loss: 0.2560059428215027\n",
      "Step 9  Loss: 0.492087185382843\n",
      "Step 10  Loss: 0.2819833755493164\n",
      "total Loss of epoch  126  is  4.420497566461563\n",
      "Step 0  Loss: 0.3935158848762512\n",
      "Step 1  Loss: 0.437763512134552\n",
      "Step 2  Loss: 0.39510291814804077\n",
      "Step 3  Loss: 0.4093710780143738\n",
      "Step 4  Loss: 0.3674565851688385\n",
      "Step 5  Loss: 0.4418162703514099\n",
      "Step 6  Loss: 0.4121226966381073\n",
      "Step 7  Loss: 0.2967691719532013\n",
      "Step 8  Loss: 0.47451281547546387\n",
      "Step 9  Loss: 0.43861281871795654\n",
      "Step 10  Loss: 0.43016406893730164\n",
      "total Loss of epoch  127  is  4.497207820415497\n",
      "Step 0  Loss: 0.3248726725578308\n",
      "Step 1  Loss: 0.2958669066429138\n",
      "Step 2  Loss: 0.42277058959007263\n",
      "Step 3  Loss: 0.3667168915271759\n",
      "Step 4  Loss: 0.412416011095047\n",
      "Step 5  Loss: 0.3806915879249573\n",
      "Step 6  Loss: 0.46121469140052795\n",
      "Step 7  Loss: 0.46745753288269043\n",
      "Step 8  Loss: 0.3654666841030121\n",
      "Step 9  Loss: 0.42388826608657837\n",
      "Step 10  Loss: 0.27186375856399536\n",
      "total Loss of epoch  128  is  4.193225592374802\n",
      "Step 0  Loss: 0.34235680103302\n",
      "Step 1  Loss: 0.30011415481567383\n",
      "Step 2  Loss: 0.48230671882629395\n",
      "Step 3  Loss: 0.377365380525589\n",
      "Step 4  Loss: 0.48003479838371277\n",
      "Step 5  Loss: 0.30321788787841797\n",
      "Step 6  Loss: 0.3741854727268219\n",
      "Step 7  Loss: 0.34880462288856506\n",
      "Step 8  Loss: 0.35508283972740173\n",
      "Step 9  Loss: 0.42624661326408386\n",
      "Step 10  Loss: 0.2678057551383972\n",
      "total Loss of epoch  129  is  4.057521045207977\n",
      "Step 0  Loss: 0.38623496890068054\n",
      "Step 1  Loss: 0.5135087966918945\n",
      "Step 2  Loss: 0.304916650056839\n",
      "Step 3  Loss: 0.33841028809547424\n",
      "Step 4  Loss: 0.31657037138938904\n",
      "Step 5  Loss: 0.4582968056201935\n",
      "Step 6  Loss: 0.449001669883728\n",
      "Step 7  Loss: 0.4337896406650543\n",
      "Step 8  Loss: 0.3734167218208313\n",
      "Step 9  Loss: 0.45323190093040466\n",
      "Step 10  Loss: 0.3256548345088959\n",
      "total Loss of epoch  130  is  4.353032648563385\n",
      "Step 0  Loss: 0.5397185683250427\n",
      "Step 1  Loss: 0.38297051191329956\n",
      "Step 2  Loss: 0.44296666979789734\n",
      "Step 3  Loss: 0.37820520997047424\n",
      "Step 4  Loss: 0.3792126178741455\n",
      "Step 5  Loss: 0.37486547231674194\n",
      "Step 6  Loss: 0.2803851068019867\n",
      "Step 7  Loss: 0.37859484553337097\n",
      "Step 8  Loss: 0.3056778907775879\n",
      "Step 9  Loss: 0.3497370183467865\n",
      "Step 10  Loss: 0.446687251329422\n",
      "total Loss of epoch  131  is  4.259021162986755\n",
      "Step 0  Loss: 0.3490232527256012\n",
      "Step 1  Loss: 0.40492990612983704\n",
      "Step 2  Loss: 0.41817978024482727\n",
      "Step 3  Loss: 0.4017884135246277\n",
      "Step 4  Loss: 0.3141601085662842\n",
      "Step 5  Loss: 0.39676371216773987\n",
      "Step 6  Loss: 0.49807408452033997\n",
      "Step 7  Loss: 0.5715849995613098\n",
      "Step 8  Loss: 0.342739075422287\n",
      "Step 9  Loss: 0.38501977920532227\n",
      "Step 10  Loss: 0.5002514719963074\n",
      "total Loss of epoch  132  is  4.582514584064484\n",
      "Step 0  Loss: 0.39806902408599854\n",
      "Step 1  Loss: 0.324046790599823\n",
      "Step 2  Loss: 0.35568955540657043\n",
      "Step 3  Loss: 0.43982553482055664\n",
      "Step 4  Loss: 0.3685559630393982\n",
      "Step 5  Loss: 0.3438158929347992\n",
      "Step 6  Loss: 0.45992469787597656\n",
      "Step 7  Loss: 0.40798208117485046\n",
      "Step 8  Loss: 0.3723752796649933\n",
      "Step 9  Loss: 0.4056096076965332\n",
      "Step 10  Loss: 0.5485967993736267\n",
      "total Loss of epoch  133  is  4.424491226673126\n",
      "Step 0  Loss: 0.41637086868286133\n",
      "Step 1  Loss: 0.3588522970676422\n",
      "Step 2  Loss: 0.32099780440330505\n",
      "Step 3  Loss: 0.2429841011762619\n",
      "Step 4  Loss: 0.4032215476036072\n",
      "Step 5  Loss: 0.4586118757724762\n",
      "Step 6  Loss: 0.34182411432266235\n",
      "Step 7  Loss: 0.4481213092803955\n",
      "Step 8  Loss: 0.39763861894607544\n",
      "Step 9  Loss: 0.42205968499183655\n",
      "Step 10  Loss: 0.44383829832077026\n",
      "total Loss of epoch  134  is  4.254520520567894\n",
      "Step 0  Loss: 0.3675564229488373\n",
      "Step 1  Loss: 0.4723733067512512\n",
      "Step 2  Loss: 0.34020960330963135\n",
      "Step 3  Loss: 0.4755953252315521\n",
      "Step 4  Loss: 0.3243350088596344\n",
      "Step 5  Loss: 0.36497777700424194\n",
      "Step 6  Loss: 0.3528840243816376\n",
      "Step 7  Loss: 0.44948577880859375\n",
      "Step 8  Loss: 0.37091150879859924\n",
      "Step 9  Loss: 0.35945942997932434\n",
      "Step 10  Loss: 0.2598187029361725\n",
      "total Loss of epoch  135  is  4.137606889009476\n",
      "Step 0  Loss: 0.4435645341873169\n",
      "Step 1  Loss: 0.3809720575809479\n",
      "Step 2  Loss: 0.48107776045799255\n",
      "Step 3  Loss: 0.39883339405059814\n",
      "Step 4  Loss: 0.3389769494533539\n",
      "Step 5  Loss: 0.39318984746932983\n",
      "Step 6  Loss: 0.31238478422164917\n",
      "Step 7  Loss: 0.37851816415786743\n",
      "Step 8  Loss: 0.36623936891555786\n",
      "Step 9  Loss: 0.34835419058799744\n",
      "Step 10  Loss: 0.44093647599220276\n",
      "total Loss of epoch  136  is  4.283047527074814\n",
      "Step 0  Loss: 0.41218408942222595\n",
      "Step 1  Loss: 0.38115692138671875\n",
      "Step 2  Loss: 0.47186294198036194\n",
      "Step 3  Loss: 0.4448064863681793\n",
      "Step 4  Loss: 0.38071009516716003\n",
      "Step 5  Loss: 0.48837748169898987\n",
      "Step 6  Loss: 0.48978909850120544\n",
      "Step 7  Loss: 0.4223758280277252\n",
      "Step 8  Loss: 0.5500006079673767\n",
      "Step 9  Loss: 0.4402000308036804\n",
      "Step 10  Loss: 0.4338948726654053\n",
      "total Loss of epoch  137  is  4.915358453989029\n",
      "Step 0  Loss: 0.359091192483902\n",
      "Step 1  Loss: 0.49578672647476196\n",
      "Step 2  Loss: 0.33011364936828613\n",
      "Step 3  Loss: 0.4104621410369873\n",
      "Step 4  Loss: 0.2978118360042572\n",
      "Step 5  Loss: 0.4872663915157318\n",
      "Step 6  Loss: 0.41320061683654785\n",
      "Step 7  Loss: 0.42285221815109253\n",
      "Step 8  Loss: 0.37428537011146545\n",
      "Step 9  Loss: 0.381944477558136\n",
      "Step 10  Loss: 0.36201176047325134\n",
      "total Loss of epoch  138  is  4.3348263800144196\n",
      "Step 0  Loss: 0.43579694628715515\n",
      "Step 1  Loss: 0.4097508192062378\n",
      "Step 2  Loss: 0.484170138835907\n",
      "Step 3  Loss: 0.46388867497444153\n",
      "Step 4  Loss: 0.3227134048938751\n",
      "Step 5  Loss: 0.2507287263870239\n",
      "Step 6  Loss: 0.5144128203392029\n",
      "Step 7  Loss: 0.42957550287246704\n",
      "Step 8  Loss: 0.3438076376914978\n",
      "Step 9  Loss: 0.4735703468322754\n",
      "Step 10  Loss: 0.2913556694984436\n",
      "total Loss of epoch  139  is  4.419770687818527\n",
      "Step 0  Loss: 0.2650727331638336\n",
      "Step 1  Loss: 0.384250670671463\n",
      "Step 2  Loss: 0.3363347053527832\n",
      "Step 3  Loss: 0.3474217355251312\n",
      "Step 4  Loss: 0.32787853479385376\n",
      "Step 5  Loss: 0.39014336466789246\n",
      "Step 6  Loss: 0.41906747221946716\n",
      "Step 7  Loss: 0.4177611470222473\n",
      "Step 8  Loss: 0.3800375759601593\n",
      "Step 9  Loss: 0.3091380000114441\n",
      "Step 10  Loss: 0.34535542130470276\n",
      "total Loss of epoch  140  is  3.922461360692978\n",
      "Step 0  Loss: 0.44685137271881104\n",
      "Step 1  Loss: 0.3605325222015381\n",
      "Step 2  Loss: 0.4411856234073639\n",
      "Step 3  Loss: 0.33796682953834534\n",
      "Step 4  Loss: 0.4623362123966217\n",
      "Step 5  Loss: 0.4109783470630646\n",
      "Step 6  Loss: 0.38369321823120117\n",
      "Step 7  Loss: 0.4506577253341675\n",
      "Step 8  Loss: 0.3968539237976074\n",
      "Step 9  Loss: 0.4148792028427124\n",
      "Step 10  Loss: 0.3313314616680145\n",
      "total Loss of epoch  141  is  4.437266439199448\n",
      "Step 0  Loss: 0.42962369322776794\n",
      "Step 1  Loss: 0.4499210715293884\n",
      "Step 2  Loss: 0.4251750111579895\n",
      "Step 3  Loss: 0.3918008506298065\n",
      "Step 4  Loss: 0.3253709077835083\n",
      "Step 5  Loss: 0.41532498598098755\n",
      "Step 6  Loss: 0.40382662415504456\n",
      "Step 7  Loss: 0.45370736718177795\n",
      "Step 8  Loss: 0.4284476041793823\n",
      "Step 9  Loss: 0.37823033332824707\n",
      "Step 10  Loss: 0.37041574716567993\n",
      "total Loss of epoch  142  is  4.47184419631958\n",
      "Step 0  Loss: 0.35945945978164673\n",
      "Step 1  Loss: 0.3664778172969818\n",
      "Step 2  Loss: 0.363776296377182\n",
      "Step 3  Loss: 0.38646459579467773\n",
      "Step 4  Loss: 0.3696320652961731\n",
      "Step 5  Loss: 0.3877665400505066\n",
      "Step 6  Loss: 0.4284178614616394\n",
      "Step 7  Loss: 0.3117278218269348\n",
      "Step 8  Loss: 0.36918115615844727\n",
      "Step 9  Loss: 0.32618188858032227\n",
      "Step 10  Loss: 0.4691484868526459\n",
      "total Loss of epoch  143  is  4.138233989477158\n",
      "Step 0  Loss: 0.31343165040016174\n",
      "Step 1  Loss: 0.3461562991142273\n",
      "Step 2  Loss: 0.3684007227420807\n",
      "Step 3  Loss: 0.32217511534690857\n",
      "Step 4  Loss: 0.4129696190357208\n",
      "Step 5  Loss: 0.3788784146308899\n",
      "Step 6  Loss: 0.37816816568374634\n",
      "Step 7  Loss: 0.3511420488357544\n",
      "Step 8  Loss: 0.41606149077415466\n",
      "Step 9  Loss: 0.42527905106544495\n",
      "Step 10  Loss: 0.3219219446182251\n",
      "total Loss of epoch  144  is  4.0345845222473145\n",
      "Step 0  Loss: 0.31299054622650146\n",
      "Step 1  Loss: 0.3179228901863098\n",
      "Step 2  Loss: 0.4014332592487335\n",
      "Step 3  Loss: 0.46152186393737793\n",
      "Step 4  Loss: 0.42026883363723755\n",
      "Step 5  Loss: 0.37168416380882263\n",
      "Step 6  Loss: 0.4894897937774658\n",
      "Step 7  Loss: 0.42320913076400757\n",
      "Step 8  Loss: 0.45938217639923096\n",
      "Step 9  Loss: 0.4139403998851776\n",
      "Step 10  Loss: 0.3600427210330963\n",
      "total Loss of epoch  145  is  4.431885778903961\n",
      "Step 0  Loss: 0.32152223587036133\n",
      "Step 1  Loss: 0.41695302724838257\n",
      "Step 2  Loss: 0.48891815543174744\n",
      "Step 3  Loss: 0.44477590918540955\n",
      "Step 4  Loss: 0.4236924350261688\n",
      "Step 5  Loss: 0.29291853308677673\n",
      "Step 6  Loss: 0.3969864845275879\n",
      "Step 7  Loss: 0.41753721237182617\n",
      "Step 8  Loss: 0.30607280135154724\n",
      "Step 9  Loss: 0.3765532672405243\n",
      "Step 10  Loss: 0.4657072126865387\n",
      "total Loss of epoch  146  is  4.351637274026871\n",
      "Step 0  Loss: 0.4368167519569397\n",
      "Step 1  Loss: 0.39115869998931885\n",
      "Step 2  Loss: 0.4226934313774109\n",
      "Step 3  Loss: 0.49818316102027893\n",
      "Step 4  Loss: 0.4172207713127136\n",
      "Step 5  Loss: 0.37994706630706787\n",
      "Step 6  Loss: 0.47891417145729065\n",
      "Step 7  Loss: 0.37352946400642395\n",
      "Step 8  Loss: 0.3361565172672272\n",
      "Step 9  Loss: 0.39566436409950256\n",
      "Step 10  Loss: 0.5553444027900696\n",
      "total Loss of epoch  147  is  4.685628801584244\n",
      "Step 0  Loss: 0.4306444227695465\n",
      "Step 1  Loss: 0.5084090232849121\n",
      "Step 2  Loss: 0.3405545651912689\n",
      "Step 3  Loss: 0.46655407547950745\n",
      "Step 4  Loss: 0.34456464648246765\n",
      "Step 5  Loss: 0.4617250859737396\n",
      "Step 6  Loss: 0.4187229573726654\n",
      "Step 7  Loss: 0.3641090393066406\n",
      "Step 8  Loss: 0.46125420928001404\n",
      "Step 9  Loss: 0.30762869119644165\n",
      "Step 10  Loss: 0.4789630174636841\n",
      "total Loss of epoch  148  is  4.583129733800888\n",
      "Step 0  Loss: 0.4425537586212158\n",
      "Step 1  Loss: 0.3321514129638672\n",
      "Step 2  Loss: 0.40332189202308655\n",
      "Step 3  Loss: 0.3502654433250427\n",
      "Step 4  Loss: 0.4370216131210327\n",
      "Step 5  Loss: 0.30150195956230164\n",
      "Step 6  Loss: 0.32427090406417847\n",
      "Step 7  Loss: 0.3203982412815094\n",
      "Step 8  Loss: 0.4545136094093323\n",
      "Step 9  Loss: 0.33803901076316833\n",
      "Step 10  Loss: 0.34713801741600037\n",
      "total Loss of epoch  149  is  4.0511758625507355\n",
      "Step 0  Loss: 0.39849337935447693\n",
      "Step 1  Loss: 0.3262779414653778\n",
      "Step 2  Loss: 0.3748607039451599\n",
      "Step 3  Loss: 0.45228999853134155\n",
      "Step 4  Loss: 0.30366575717926025\n",
      "Step 5  Loss: 0.39149898290634155\n",
      "Step 6  Loss: 0.3713475167751312\n",
      "Step 7  Loss: 0.3266585171222687\n",
      "Step 8  Loss: 0.3695799708366394\n",
      "Step 9  Loss: 0.49376875162124634\n",
      "Step 10  Loss: 0.36396780610084534\n",
      "total Loss of epoch  150  is  4.172409325838089\n",
      "Step 0  Loss: 0.23533135652542114\n",
      "Step 1  Loss: 0.37473830580711365\n",
      "Step 2  Loss: 0.47854727506637573\n",
      "Step 3  Loss: 0.3865584433078766\n",
      "Step 4  Loss: 0.35841092467308044\n",
      "Step 5  Loss: 0.36567530035972595\n",
      "Step 6  Loss: 0.44194385409355164\n",
      "Step 7  Loss: 0.37264713644981384\n",
      "Step 8  Loss: 0.43810129165649414\n",
      "Step 9  Loss: 0.36139434576034546\n",
      "Step 10  Loss: 0.3575113117694855\n",
      "total Loss of epoch  151  is  4.170859545469284\n",
      "Step 0  Loss: 0.512220025062561\n",
      "Step 1  Loss: 0.3841099739074707\n",
      "Step 2  Loss: 0.4882930517196655\n",
      "Step 3  Loss: 0.4285427927970886\n",
      "Step 4  Loss: 0.4493483603000641\n",
      "Step 5  Loss: 0.3001810908317566\n",
      "Step 6  Loss: 0.4106030762195587\n",
      "Step 7  Loss: 0.38879600167274475\n",
      "Step 8  Loss: 0.326363205909729\n",
      "Step 9  Loss: 0.34626564383506775\n",
      "Step 10  Loss: 0.3350185453891754\n",
      "total Loss of epoch  152  is  4.369741767644882\n",
      "Step 0  Loss: 0.4181773364543915\n",
      "Step 1  Loss: 0.44040030241012573\n",
      "Step 2  Loss: 0.36341553926467896\n",
      "Step 3  Loss: 0.377285897731781\n",
      "Step 4  Loss: 0.3658016324043274\n",
      "Step 5  Loss: 0.30321377515792847\n",
      "Step 6  Loss: 0.3256106674671173\n",
      "Step 7  Loss: 0.45088499784469604\n",
      "Step 8  Loss: 0.40639588236808777\n",
      "Step 9  Loss: 0.3091997802257538\n",
      "Step 10  Loss: 0.45904114842414856\n",
      "total Loss of epoch  153  is  4.2194269597530365\n",
      "Step 0  Loss: 0.4139706790447235\n",
      "Step 1  Loss: 0.29393070936203003\n",
      "Step 2  Loss: 0.2911228537559509\n",
      "Step 3  Loss: 0.42572182416915894\n",
      "Step 4  Loss: 0.29939496517181396\n",
      "Step 5  Loss: 0.34113457798957825\n",
      "Step 6  Loss: 0.2476673126220703\n",
      "Step 7  Loss: 0.42231911420822144\n",
      "Step 8  Loss: 0.33883053064346313\n",
      "Step 9  Loss: 0.43267086148262024\n",
      "Step 10  Loss: 0.3596690893173218\n",
      "total Loss of epoch  154  is  3.8664325177669525\n",
      "Step 0  Loss: 0.37060457468032837\n",
      "Step 1  Loss: 0.4482469856739044\n",
      "Step 2  Loss: 0.312946617603302\n",
      "Step 3  Loss: 0.3584580719470978\n",
      "Step 4  Loss: 0.3009091913700104\n",
      "Step 5  Loss: 0.21841368079185486\n",
      "Step 6  Loss: 0.32587888836860657\n",
      "Step 7  Loss: 0.3664085865020752\n",
      "Step 8  Loss: 0.3969866931438446\n",
      "Step 9  Loss: 0.4684367775917053\n",
      "Step 10  Loss: 0.5006734132766724\n",
      "total Loss of epoch  155  is  4.067963480949402\n",
      "Step 0  Loss: 0.29500672221183777\n",
      "Step 1  Loss: 0.3244067430496216\n",
      "Step 2  Loss: 0.35250693559646606\n",
      "Step 3  Loss: 0.332088440656662\n",
      "Step 4  Loss: 0.4955230951309204\n",
      "Step 5  Loss: 0.3805617690086365\n",
      "Step 6  Loss: 0.47967416048049927\n",
      "Step 7  Loss: 0.3746759593486786\n",
      "Step 8  Loss: 0.4426681399345398\n",
      "Step 9  Loss: 0.37658438086509705\n",
      "Step 10  Loss: 0.35272297263145447\n",
      "total Loss of epoch  156  is  4.2064193189144135\n",
      "Step 0  Loss: 0.36586469411849976\n",
      "Step 1  Loss: 0.35208702087402344\n",
      "Step 2  Loss: 0.3392617702484131\n",
      "Step 3  Loss: 0.36254289746284485\n",
      "Step 4  Loss: 0.28600335121154785\n",
      "Step 5  Loss: 0.43066200613975525\n",
      "Step 6  Loss: 0.3369285762310028\n",
      "Step 7  Loss: 0.5030631422996521\n",
      "Step 8  Loss: 0.3634430170059204\n",
      "Step 9  Loss: 0.35420918464660645\n",
      "Step 10  Loss: 0.22723479568958282\n",
      "total Loss of epoch  157  is  3.921300455927849\n",
      "Step 0  Loss: 0.34705373644828796\n",
      "Step 1  Loss: 0.3957057595252991\n",
      "Step 2  Loss: 0.3213890790939331\n",
      "Step 3  Loss: 0.24302715063095093\n",
      "Step 4  Loss: 0.5066520571708679\n",
      "Step 5  Loss: 0.4086717963218689\n",
      "Step 6  Loss: 0.32813191413879395\n",
      "Step 7  Loss: 0.37463900446891785\n",
      "Step 8  Loss: 0.4103993773460388\n",
      "Step 9  Loss: 0.41863933205604553\n",
      "Step 10  Loss: 0.4233109951019287\n",
      "total Loss of epoch  158  is  4.177620202302933\n",
      "Step 0  Loss: 0.2811359167098999\n",
      "Step 1  Loss: 0.46167895197868347\n",
      "Step 2  Loss: 0.3554067313671112\n",
      "Step 3  Loss: 0.36187878251075745\n",
      "Step 4  Loss: 0.3757709562778473\n",
      "Step 5  Loss: 0.4396856129169464\n",
      "Step 6  Loss: 0.37230807542800903\n",
      "Step 7  Loss: 0.4833148121833801\n",
      "Step 8  Loss: 0.40174180269241333\n",
      "Step 9  Loss: 0.43948039412498474\n",
      "Step 10  Loss: 0.5053727626800537\n",
      "total Loss of epoch  159  is  4.477774798870087\n",
      "Step 0  Loss: 0.37649571895599365\n",
      "Step 1  Loss: 0.2928773760795593\n",
      "Step 2  Loss: 0.4445604979991913\n",
      "Step 3  Loss: 0.3678537905216217\n",
      "Step 4  Loss: 0.43858131766319275\n",
      "Step 5  Loss: 0.3558094799518585\n",
      "Step 6  Loss: 0.3686995506286621\n",
      "Step 7  Loss: 0.4092969596385956\n",
      "Step 8  Loss: 0.3438034653663635\n",
      "Step 9  Loss: 0.2856782078742981\n",
      "Step 10  Loss: 0.3629223704338074\n",
      "total Loss of epoch  160  is  4.046578735113144\n",
      "Step 0  Loss: 0.31661155819892883\n",
      "Step 1  Loss: 0.33351531624794006\n",
      "Step 2  Loss: 0.4092370569705963\n",
      "Step 3  Loss: 0.2636371850967407\n",
      "Step 4  Loss: 0.4714004397392273\n",
      "Step 5  Loss: 0.3533608913421631\n",
      "Step 6  Loss: 0.5055012106895447\n",
      "Step 7  Loss: 0.4634658694267273\n",
      "Step 8  Loss: 0.41528257727622986\n",
      "Step 9  Loss: 0.35693028569221497\n",
      "Step 10  Loss: 0.3168127238750458\n",
      "total Loss of epoch  161  is  4.205755114555359\n",
      "Step 0  Loss: 0.43263867497444153\n",
      "Step 1  Loss: 0.4006388485431671\n",
      "Step 2  Loss: 0.35640019178390503\n",
      "Step 3  Loss: 0.3954908549785614\n",
      "Step 4  Loss: 0.28363093733787537\n",
      "Step 5  Loss: 0.4568638801574707\n",
      "Step 6  Loss: 0.3280348777770996\n",
      "Step 7  Loss: 0.3879276216030121\n",
      "Step 8  Loss: 0.3500001132488251\n",
      "Step 9  Loss: 0.388624906539917\n",
      "Step 10  Loss: 0.31195423007011414\n",
      "total Loss of epoch  162  is  4.092205137014389\n",
      "Step 0  Loss: 0.39820054173469543\n",
      "Step 1  Loss: 0.3843550980091095\n",
      "Step 2  Loss: 0.3241787850856781\n",
      "Step 3  Loss: 0.373497873544693\n",
      "Step 4  Loss: 0.35526418685913086\n",
      "Step 5  Loss: 0.36880967020988464\n",
      "Step 6  Loss: 0.3734886348247528\n",
      "Step 7  Loss: 0.33721813559532166\n",
      "Step 8  Loss: 0.36991915106773376\n",
      "Step 9  Loss: 0.39257270097732544\n",
      "Step 10  Loss: 0.48190006613731384\n",
      "total Loss of epoch  163  is  4.159404844045639\n",
      "Step 0  Loss: 0.4441801607608795\n",
      "Step 1  Loss: 0.4136385917663574\n",
      "Step 2  Loss: 0.4302634298801422\n",
      "Step 3  Loss: 0.49690499901771545\n",
      "Step 4  Loss: 0.4370862543582916\n",
      "Step 5  Loss: 0.37399056553840637\n",
      "Step 6  Loss: 0.40818947553634644\n",
      "Step 7  Loss: 0.36739835143089294\n",
      "Step 8  Loss: 0.3271637260913849\n",
      "Step 9  Loss: 0.3985922038555145\n",
      "Step 10  Loss: 0.37218526005744934\n",
      "total Loss of epoch  164  is  4.469593018293381\n",
      "Step 0  Loss: 0.32776156067848206\n",
      "Step 1  Loss: 0.3628554940223694\n",
      "Step 2  Loss: 0.42161357402801514\n",
      "Step 3  Loss: 0.32573437690734863\n",
      "Step 4  Loss: 0.3142053484916687\n",
      "Step 5  Loss: 0.37247562408447266\n",
      "Step 6  Loss: 0.40573492646217346\n",
      "Step 7  Loss: 0.4535870850086212\n",
      "Step 8  Loss: 0.4871027171611786\n",
      "Step 9  Loss: 0.35087156295776367\n",
      "Step 10  Loss: 0.2801634669303894\n",
      "total Loss of epoch  165  is  4.102105736732483\n",
      "Step 0  Loss: 0.3116912841796875\n",
      "Step 1  Loss: 0.30189627408981323\n",
      "Step 2  Loss: 0.431183785200119\n",
      "Step 3  Loss: 0.3761458098888397\n",
      "Step 4  Loss: 0.28007689118385315\n",
      "Step 5  Loss: 0.5229642391204834\n",
      "Step 6  Loss: 0.3826390206813812\n",
      "Step 7  Loss: 0.37331274151802063\n",
      "Step 8  Loss: 0.34031668305397034\n",
      "Step 9  Loss: 0.4909234344959259\n",
      "Step 10  Loss: 0.470630943775177\n",
      "total Loss of epoch  166  is  4.281781107187271\n",
      "Step 0  Loss: 0.32758471369743347\n",
      "Step 1  Loss: 0.44281908869743347\n",
      "Step 2  Loss: 0.4578298032283783\n",
      "Step 3  Loss: 0.4156399667263031\n",
      "Step 4  Loss: 0.31439778208732605\n",
      "Step 5  Loss: 0.45934513211250305\n",
      "Step 6  Loss: 0.5007072687149048\n",
      "Step 7  Loss: 0.37451764941215515\n",
      "Step 8  Loss: 0.4045005440711975\n",
      "Step 9  Loss: 0.4012551009654999\n",
      "Step 10  Loss: 0.30032962560653687\n",
      "total Loss of epoch  167  is  4.398926675319672\n",
      "Step 0  Loss: 0.3708912134170532\n",
      "Step 1  Loss: 0.3423563241958618\n",
      "Step 2  Loss: 0.4096514582633972\n",
      "Step 3  Loss: 0.454910010099411\n",
      "Step 4  Loss: 0.3914450407028198\n",
      "Step 5  Loss: 0.36717694997787476\n",
      "Step 6  Loss: 0.5010842680931091\n",
      "Step 7  Loss: 0.31171801686286926\n",
      "Step 8  Loss: 0.3113170266151428\n",
      "Step 9  Loss: 0.3556059002876282\n",
      "Step 10  Loss: 0.24471604824066162\n",
      "total Loss of epoch  168  is  4.060872256755829\n",
      "Step 0  Loss: 0.4424952566623688\n",
      "Step 1  Loss: 0.43291881680488586\n",
      "Step 2  Loss: 0.328316330909729\n",
      "Step 3  Loss: 0.40712887048721313\n",
      "Step 4  Loss: 0.28885072469711304\n",
      "Step 5  Loss: 0.27236127853393555\n",
      "Step 6  Loss: 0.3874518573284149\n",
      "Step 7  Loss: 0.42541685700416565\n",
      "Step 8  Loss: 0.4741222858428955\n",
      "Step 9  Loss: 0.35216301679611206\n",
      "Step 10  Loss: 0.42886215448379517\n",
      "total Loss of epoch  169  is  4.240087449550629\n",
      "Step 0  Loss: 0.43242043256759644\n",
      "Step 1  Loss: 0.3759646713733673\n",
      "Step 2  Loss: 0.36736875772476196\n",
      "Step 3  Loss: 0.41220495104789734\n",
      "Step 4  Loss: 0.46817898750305176\n",
      "Step 5  Loss: 0.38948872685432434\n",
      "Step 6  Loss: 0.4195822775363922\n",
      "Step 7  Loss: 0.3726288080215454\n",
      "Step 8  Loss: 0.36824676394462585\n",
      "Step 9  Loss: 0.41144219040870667\n",
      "Step 10  Loss: 0.34457558393478394\n",
      "total Loss of epoch  170  is  4.362102150917053\n",
      "Step 0  Loss: 0.387943297624588\n",
      "Step 1  Loss: 0.3292770981788635\n",
      "Step 2  Loss: 0.4442571699619293\n",
      "Step 3  Loss: 0.25610044598579407\n",
      "Step 4  Loss: 0.4729236960411072\n",
      "Step 5  Loss: 0.42183008790016174\n",
      "Step 6  Loss: 0.40412962436676025\n",
      "Step 7  Loss: 0.22477026283740997\n",
      "Step 8  Loss: 0.5009597539901733\n",
      "Step 9  Loss: 0.3813394606113434\n",
      "Step 10  Loss: 0.39480897784233093\n",
      "total Loss of epoch  171  is  4.218339875340462\n",
      "Step 0  Loss: 0.32870930433273315\n",
      "Step 1  Loss: 0.3461177945137024\n",
      "Step 2  Loss: 0.309087872505188\n",
      "Step 3  Loss: 0.32748156785964966\n",
      "Step 4  Loss: 0.24343673884868622\n",
      "Step 5  Loss: 0.32698488235473633\n",
      "Step 6  Loss: 0.4591623842716217\n",
      "Step 7  Loss: 0.349274605512619\n",
      "Step 8  Loss: 0.3889247477054596\n",
      "Step 9  Loss: 0.25532978773117065\n",
      "Step 10  Loss: 0.3322519361972809\n",
      "total Loss of epoch  172  is  3.6667616218328476\n",
      "Step 0  Loss: 0.2720494270324707\n",
      "Step 1  Loss: 0.3354592025279999\n",
      "Step 2  Loss: 0.25122347474098206\n",
      "Step 3  Loss: 0.33821573853492737\n",
      "Step 4  Loss: 0.3867971897125244\n",
      "Step 5  Loss: 0.354997843503952\n",
      "Step 6  Loss: 0.4031139314174652\n",
      "Step 7  Loss: 0.29570549726486206\n",
      "Step 8  Loss: 0.33984827995300293\n",
      "Step 9  Loss: 0.3463016748428345\n",
      "Step 10  Loss: 0.45507147908210754\n",
      "total Loss of epoch  173  is  3.7787837386131287\n",
      "Step 0  Loss: 0.432538777589798\n",
      "Step 1  Loss: 0.3638267517089844\n",
      "Step 2  Loss: 0.37025952339172363\n",
      "Step 3  Loss: 0.33966413140296936\n",
      "Step 4  Loss: 0.43493178486824036\n",
      "Step 5  Loss: 0.4514209032058716\n",
      "Step 6  Loss: 0.31371834874153137\n",
      "Step 7  Loss: 0.4035067558288574\n",
      "Step 8  Loss: 0.398025780916214\n",
      "Step 9  Loss: 0.4061194360256195\n",
      "Step 10  Loss: 0.3860931992530823\n",
      "total Loss of epoch  174  is  4.300105392932892\n",
      "Step 0  Loss: 0.318581759929657\n",
      "Step 1  Loss: 0.42335787415504456\n",
      "Step 2  Loss: 0.34803494811058044\n",
      "Step 3  Loss: 0.4803425967693329\n",
      "Step 4  Loss: 0.3598804473876953\n",
      "Step 5  Loss: 0.44894325733184814\n",
      "Step 6  Loss: 0.3430640995502472\n",
      "Step 7  Loss: 0.36218786239624023\n",
      "Step 8  Loss: 0.30439651012420654\n",
      "Step 9  Loss: 0.3528381884098053\n",
      "Step 10  Loss: 0.3819516599178314\n",
      "total Loss of epoch  175  is  4.123579204082489\n",
      "Step 0  Loss: 0.28866270184516907\n",
      "Step 1  Loss: 0.4588201344013214\n",
      "Step 2  Loss: 0.4371354281902313\n",
      "Step 3  Loss: 0.4756791293621063\n",
      "Step 4  Loss: 0.3513873815536499\n",
      "Step 5  Loss: 0.46736985445022583\n",
      "Step 6  Loss: 0.4344448447227478\n",
      "Step 7  Loss: 0.3619462847709656\n",
      "Step 8  Loss: 0.5071235299110413\n",
      "Step 9  Loss: 0.3600684106349945\n",
      "Step 10  Loss: 0.33048340678215027\n",
      "total Loss of epoch  176  is  4.473121106624603\n",
      "Step 0  Loss: 0.3135431408882141\n",
      "Step 1  Loss: 0.31362828612327576\n",
      "Step 2  Loss: 0.35350844264030457\n",
      "Step 3  Loss: 0.3234679400920868\n",
      "Step 4  Loss: 0.3867690861225128\n",
      "Step 5  Loss: 0.27781859040260315\n",
      "Step 6  Loss: 0.4034646451473236\n",
      "Step 7  Loss: 0.39340561628341675\n",
      "Step 8  Loss: 0.29111146926879883\n",
      "Step 9  Loss: 0.4050929546356201\n",
      "Step 10  Loss: 0.5250378251075745\n",
      "total Loss of epoch  177  is  3.986847996711731\n",
      "Step 0  Loss: 0.40276840329170227\n",
      "Step 1  Loss: 0.342071533203125\n",
      "Step 2  Loss: 0.37300699949264526\n",
      "Step 3  Loss: 0.4244513511657715\n",
      "Step 4  Loss: 0.311136931180954\n",
      "Step 5  Loss: 0.4813237488269806\n",
      "Step 6  Loss: 0.4020480811595917\n",
      "Step 7  Loss: 0.37392938137054443\n",
      "Step 8  Loss: 0.4351012706756592\n",
      "Step 9  Loss: 0.29269325733184814\n",
      "Step 10  Loss: 0.4776476323604584\n",
      "total Loss of epoch  178  is  4.31617859005928\n",
      "Step 0  Loss: 0.42894691228866577\n",
      "Step 1  Loss: 0.41788458824157715\n",
      "Step 2  Loss: 0.23799362778663635\n",
      "Step 3  Loss: 0.43689656257629395\n",
      "Step 4  Loss: 0.44114622473716736\n",
      "Step 5  Loss: 0.3029053211212158\n",
      "Step 6  Loss: 0.35690972208976746\n",
      "Step 7  Loss: 0.3019081652164459\n",
      "Step 8  Loss: 0.256280779838562\n",
      "Step 9  Loss: 0.31123480200767517\n",
      "Step 10  Loss: 0.3957711160182953\n",
      "total Loss of epoch  179  is  3.8878778219223022\n",
      "Step 0  Loss: 0.32757413387298584\n",
      "Step 1  Loss: 0.30923667550086975\n",
      "Step 2  Loss: 0.4621855318546295\n",
      "Step 3  Loss: 0.36848214268684387\n",
      "Step 4  Loss: 0.35536709427833557\n",
      "Step 5  Loss: 0.3832554817199707\n",
      "Step 6  Loss: 0.40553799271583557\n",
      "Step 7  Loss: 0.5118227005004883\n",
      "Step 8  Loss: 0.367753267288208\n",
      "Step 9  Loss: 0.37115174531936646\n",
      "Step 10  Loss: 0.5275564193725586\n",
      "total Loss of epoch  180  is  4.389923185110092\n",
      "Step 0  Loss: 0.3695918023586273\n",
      "Step 1  Loss: 0.41529330611228943\n",
      "Step 2  Loss: 0.43242430686950684\n",
      "Step 3  Loss: 0.3913983106613159\n",
      "Step 4  Loss: 0.4062902629375458\n",
      "Step 5  Loss: 0.4437158405780792\n",
      "Step 6  Loss: 0.3799659013748169\n",
      "Step 7  Loss: 0.4046193063259125\n",
      "Step 8  Loss: 0.33458757400512695\n",
      "Step 9  Loss: 0.3931046724319458\n",
      "Step 10  Loss: 0.27728018164634705\n",
      "total Loss of epoch  181  is  4.248271465301514\n",
      "Step 0  Loss: 0.177005335688591\n",
      "Step 1  Loss: 0.40040600299835205\n",
      "Step 2  Loss: 0.40310102701187134\n",
      "Step 3  Loss: 0.4764132499694824\n",
      "Step 4  Loss: 0.32341185212135315\n",
      "Step 5  Loss: 0.3729209005832672\n",
      "Step 6  Loss: 0.355265736579895\n",
      "Step 7  Loss: 0.31602469086647034\n",
      "Step 8  Loss: 0.5168445110321045\n",
      "Step 9  Loss: 0.508944034576416\n",
      "Step 10  Loss: 0.4560548663139343\n",
      "total Loss of epoch  182  is  4.306392207741737\n",
      "Step 0  Loss: 0.38984426856040955\n",
      "Step 1  Loss: 0.19155272841453552\n",
      "Step 2  Loss: 0.38598501682281494\n",
      "Step 3  Loss: 0.3071965277194977\n",
      "Step 4  Loss: 0.37846818566322327\n",
      "Step 5  Loss: 0.5006093978881836\n",
      "Step 6  Loss: 0.31683170795440674\n",
      "Step 7  Loss: 0.3948502838611603\n",
      "Step 8  Loss: 0.46600341796875\n",
      "Step 9  Loss: 0.40262314677238464\n",
      "Step 10  Loss: 0.32037606835365295\n",
      "total Loss of epoch  183  is  4.054340749979019\n",
      "Step 0  Loss: 0.3201146125793457\n",
      "Step 1  Loss: 0.42206111550331116\n",
      "Step 2  Loss: 0.5009176135063171\n",
      "Step 3  Loss: 0.37311995029449463\n",
      "Step 4  Loss: 0.3223213851451874\n",
      "Step 5  Loss: 0.2977364659309387\n",
      "Step 6  Loss: 0.4293905198574066\n",
      "Step 7  Loss: 0.4302534759044647\n",
      "Step 8  Loss: 0.4443468153476715\n",
      "Step 9  Loss: 0.3869290351867676\n",
      "Step 10  Loss: 0.34892892837524414\n",
      "total Loss of epoch  184  is  4.276119917631149\n",
      "Step 0  Loss: 0.3913235664367676\n",
      "Step 1  Loss: 0.47826632857322693\n",
      "Step 2  Loss: 0.33982983231544495\n",
      "Step 3  Loss: 0.4726124405860901\n",
      "Step 4  Loss: 0.3921930193901062\n",
      "Step 5  Loss: 0.45482146739959717\n",
      "Step 6  Loss: 0.36352869868278503\n",
      "Step 7  Loss: 0.3877895772457123\n",
      "Step 8  Loss: 0.46102410554885864\n",
      "Step 9  Loss: 0.2821643352508545\n",
      "Step 10  Loss: 0.4154140055179596\n",
      "total Loss of epoch  185  is  4.438967376947403\n",
      "Step 0  Loss: 0.2986071705818176\n",
      "Step 1  Loss: 0.3653793931007385\n",
      "Step 2  Loss: 0.22105830907821655\n",
      "Step 3  Loss: 0.3952184021472931\n",
      "Step 4  Loss: 0.45616596937179565\n",
      "Step 5  Loss: 0.39107993245124817\n",
      "Step 6  Loss: 0.3450589179992676\n",
      "Step 7  Loss: 0.3798830509185791\n",
      "Step 8  Loss: 0.2912311851978302\n",
      "Step 9  Loss: 0.35819903016090393\n",
      "Step 10  Loss: 0.3304262161254883\n",
      "total Loss of epoch  186  is  3.8323075771331787\n",
      "Step 0  Loss: 0.3457798957824707\n",
      "Step 1  Loss: 0.3166484236717224\n",
      "Step 2  Loss: 0.41053614020347595\n",
      "Step 3  Loss: 0.3556918501853943\n",
      "Step 4  Loss: 0.3463454842567444\n",
      "Step 5  Loss: 0.42168059945106506\n",
      "Step 6  Loss: 0.4110686182975769\n",
      "Step 7  Loss: 0.38549718260765076\n",
      "Step 8  Loss: 0.30655404925346375\n",
      "Step 9  Loss: 0.4778917133808136\n",
      "Step 10  Loss: 0.26216810941696167\n",
      "total Loss of epoch  187  is  4.0398620665073395\n",
      "Step 0  Loss: 0.35263168811798096\n",
      "Step 1  Loss: 0.34276753664016724\n",
      "Step 2  Loss: 0.4058833122253418\n",
      "Step 3  Loss: 0.3572721779346466\n",
      "Step 4  Loss: 0.437826931476593\n",
      "Step 5  Loss: 0.43520230054855347\n",
      "Step 6  Loss: 0.36242812871932983\n",
      "Step 7  Loss: 0.4081824719905853\n",
      "Step 8  Loss: 0.3907092809677124\n",
      "Step 9  Loss: 0.3139643669128418\n",
      "Step 10  Loss: 0.39459940791130066\n",
      "total Loss of epoch  188  is  4.201467603445053\n",
      "Step 0  Loss: 0.38103151321411133\n",
      "Step 1  Loss: 0.46405869722366333\n",
      "Step 2  Loss: 0.35252466797828674\n",
      "Step 3  Loss: 0.2526450455188751\n",
      "Step 4  Loss: 0.3290256857872009\n",
      "Step 5  Loss: 0.41181591153144836\n",
      "Step 6  Loss: 0.38245049118995667\n",
      "Step 7  Loss: 0.38803622126579285\n",
      "Step 8  Loss: 0.32977229356765747\n",
      "Step 9  Loss: 0.38426199555397034\n",
      "Step 10  Loss: 0.5275065302848816\n",
      "total Loss of epoch  189  is  4.203129053115845\n",
      "Step 0  Loss: 0.3185480535030365\n",
      "Step 1  Loss: 0.2909058630466461\n",
      "Step 2  Loss: 0.3748345971107483\n",
      "Step 3  Loss: 0.285462349653244\n",
      "Step 4  Loss: 0.3093702793121338\n",
      "Step 5  Loss: 0.4493294656276703\n",
      "Step 6  Loss: 0.36908596754074097\n",
      "Step 7  Loss: 0.3559504449367523\n",
      "Step 8  Loss: 0.30002129077911377\n",
      "Step 9  Loss: 0.5360701680183411\n",
      "Step 10  Loss: 0.44385966658592224\n",
      "total Loss of epoch  190  is  4.033438146114349\n",
      "Step 0  Loss: 0.4190726578235626\n",
      "Step 1  Loss: 0.3752988576889038\n",
      "Step 2  Loss: 0.36194857954978943\n",
      "Step 3  Loss: 0.3054395020008087\n",
      "Step 4  Loss: 0.28242096304893494\n",
      "Step 5  Loss: 0.32138246297836304\n",
      "Step 6  Loss: 0.315873920917511\n",
      "Step 7  Loss: 0.31816422939300537\n",
      "Step 8  Loss: 0.3705427944660187\n",
      "Step 9  Loss: 0.44401735067367554\n",
      "Step 10  Loss: 0.5066499710083008\n",
      "total Loss of epoch  191  is  4.020811289548874\n",
      "Step 0  Loss: 0.3410605490207672\n",
      "Step 1  Loss: 0.3435574471950531\n",
      "Step 2  Loss: 0.3380574882030487\n",
      "Step 3  Loss: 0.41920948028564453\n",
      "Step 4  Loss: 0.38736915588378906\n",
      "Step 5  Loss: 0.4860285818576813\n",
      "Step 6  Loss: 0.33318933844566345\n",
      "Step 7  Loss: 0.44422170519828796\n",
      "Step 8  Loss: 0.4139140546321869\n",
      "Step 9  Loss: 0.31096982955932617\n",
      "Step 10  Loss: 0.1898101270198822\n",
      "total Loss of epoch  192  is  4.007387757301331\n",
      "Step 0  Loss: 0.3257601857185364\n",
      "Step 1  Loss: 0.29962557554244995\n",
      "Step 2  Loss: 0.366206556558609\n",
      "Step 3  Loss: 0.24060150980949402\n",
      "Step 4  Loss: 0.35564106702804565\n",
      "Step 5  Loss: 0.4596311151981354\n",
      "Step 6  Loss: 0.41710978746414185\n",
      "Step 7  Loss: 0.3397778570652008\n",
      "Step 8  Loss: 0.4411374628543854\n",
      "Step 9  Loss: 0.37596604228019714\n",
      "Step 10  Loss: 0.4009869694709778\n",
      "total Loss of epoch  193  is  4.022444128990173\n",
      "Step 0  Loss: 0.30045774579048157\n",
      "Step 1  Loss: 0.44330886006355286\n",
      "Step 2  Loss: 0.18514440953731537\n",
      "Step 3  Loss: 0.41859087347984314\n",
      "Step 4  Loss: 0.4614712595939636\n",
      "Step 5  Loss: 0.4414285123348236\n",
      "Step 6  Loss: 0.34553447365760803\n",
      "Step 7  Loss: 0.36761775612831116\n",
      "Step 8  Loss: 0.3964909315109253\n",
      "Step 9  Loss: 0.3737097382545471\n",
      "Step 10  Loss: 0.34946709871292114\n",
      "total Loss of epoch  194  is  4.083221659064293\n",
      "Step 0  Loss: 0.18462014198303223\n",
      "Step 1  Loss: 0.2755756378173828\n",
      "Step 2  Loss: 0.3617953658103943\n",
      "Step 3  Loss: 0.39576178789138794\n",
      "Step 4  Loss: 0.27372458577156067\n",
      "Step 5  Loss: 0.4922141134738922\n",
      "Step 6  Loss: 0.31893542408943176\n",
      "Step 7  Loss: 0.3629538416862488\n",
      "Step 8  Loss: 0.36734333634376526\n",
      "Step 9  Loss: 0.4726036787033081\n",
      "Step 10  Loss: 0.24509349465370178\n",
      "total Loss of epoch  195  is  3.750621408224106\n",
      "Step 0  Loss: 0.37303414940834045\n",
      "Step 1  Loss: 0.2963971495628357\n",
      "Step 2  Loss: 0.4995659589767456\n",
      "Step 3  Loss: 0.3878365755081177\n",
      "Step 4  Loss: 0.3755769431591034\n",
      "Step 5  Loss: 0.3886650800704956\n",
      "Step 6  Loss: 0.3442526161670685\n",
      "Step 7  Loss: 0.33314308524131775\n",
      "Step 8  Loss: 0.4181918799877167\n",
      "Step 9  Loss: 0.35977306962013245\n",
      "Step 10  Loss: 0.38108962774276733\n",
      "total Loss of epoch  196  is  4.157526135444641\n",
      "Step 0  Loss: 0.2265976071357727\n",
      "Step 1  Loss: 0.4160420298576355\n",
      "Step 2  Loss: 0.400439590215683\n",
      "Step 3  Loss: 0.38651928305625916\n",
      "Step 4  Loss: 0.4116927683353424\n",
      "Step 5  Loss: 0.35969287157058716\n",
      "Step 6  Loss: 0.36978691816329956\n",
      "Step 7  Loss: 0.37229031324386597\n",
      "Step 8  Loss: 0.4143308699131012\n",
      "Step 9  Loss: 0.320544958114624\n",
      "Step 10  Loss: 0.27871501445770264\n",
      "total Loss of epoch  197  is  3.9566522240638733\n",
      "Step 0  Loss: 0.43732303380966187\n",
      "Step 1  Loss: 0.42338281869888306\n",
      "Step 2  Loss: 0.3418617248535156\n",
      "Step 3  Loss: 0.3369087278842926\n",
      "Step 4  Loss: 0.37063154578208923\n",
      "Step 5  Loss: 0.3381306529045105\n",
      "Step 6  Loss: 0.344724178314209\n",
      "Step 7  Loss: 0.37928855419158936\n",
      "Step 8  Loss: 0.4691200852394104\n",
      "Step 9  Loss: 0.41566067934036255\n",
      "Step 10  Loss: 0.4411582350730896\n",
      "total Loss of epoch  198  is  4.298190236091614\n",
      "Step 0  Loss: 0.4276004433631897\n",
      "Step 1  Loss: 0.3257024884223938\n",
      "Step 2  Loss: 0.4313693642616272\n",
      "Step 3  Loss: 0.36471691727638245\n",
      "Step 4  Loss: 0.40993794798851013\n",
      "Step 5  Loss: 0.39054811000823975\n",
      "Step 6  Loss: 0.5008952617645264\n",
      "Step 7  Loss: 0.4027484655380249\n",
      "Step 8  Loss: 0.3655748963356018\n",
      "Step 9  Loss: 0.3287185728549957\n",
      "Step 10  Loss: 0.2854999005794525\n",
      "total Loss of epoch  199  is  4.233312368392944\n",
      "Step 0  Loss: 0.48338422179222107\n",
      "Step 1  Loss: 0.3751521408557892\n",
      "Step 2  Loss: 0.369505375623703\n",
      "Step 3  Loss: 0.3996627926826477\n",
      "Step 4  Loss: 0.3337492048740387\n",
      "Step 5  Loss: 0.34369006752967834\n",
      "Step 6  Loss: 0.3554728329181671\n",
      "Step 7  Loss: 0.48487064242362976\n",
      "Step 8  Loss: 0.3319823443889618\n",
      "Step 9  Loss: 0.3863835036754608\n",
      "Step 10  Loss: 0.5766432881355286\n",
      "total Loss of epoch  200  is  4.440496414899826\n",
      "Step 0  Loss: 0.4305758476257324\n",
      "Step 1  Loss: 0.3647822141647339\n",
      "Step 2  Loss: 0.3384951055049896\n",
      "Step 3  Loss: 0.4000183045864105\n",
      "Step 4  Loss: 0.43756696581840515\n",
      "Step 5  Loss: 0.2144370824098587\n",
      "Step 6  Loss: 0.36297813057899475\n",
      "Step 7  Loss: 0.3999558985233307\n",
      "Step 8  Loss: 0.2786637544631958\n",
      "Step 9  Loss: 0.33066266775131226\n",
      "Step 10  Loss: 0.3073433041572571\n",
      "total Loss of epoch  201  is  3.865479275584221\n",
      "Step 0  Loss: 0.43117275834083557\n",
      "Step 1  Loss: 0.3611379563808441\n",
      "Step 2  Loss: 0.291827529668808\n",
      "Step 3  Loss: 0.2645692825317383\n",
      "Step 4  Loss: 0.4337148368358612\n",
      "Step 5  Loss: 0.2081366628408432\n",
      "Step 6  Loss: 0.34512805938720703\n",
      "Step 7  Loss: 0.41095471382141113\n",
      "Step 8  Loss: 0.28643572330474854\n",
      "Step 9  Loss: 0.3355690836906433\n",
      "Step 10  Loss: 0.30303576588630676\n",
      "total Loss of epoch  202  is  3.671682372689247\n",
      "Step 0  Loss: 0.45516395568847656\n",
      "Step 1  Loss: 0.4052911698818207\n",
      "Step 2  Loss: 0.5071554780006409\n",
      "Step 3  Loss: 0.2995758354663849\n",
      "Step 4  Loss: 0.4292011260986328\n",
      "Step 5  Loss: 0.43152564764022827\n",
      "Step 6  Loss: 0.44077935814857483\n",
      "Step 7  Loss: 0.3561299443244934\n",
      "Step 8  Loss: 0.31271180510520935\n",
      "Step 9  Loss: 0.32183998823165894\n",
      "Step 10  Loss: 0.39625754952430725\n",
      "total Loss of epoch  203  is  4.355631858110428\n",
      "Step 0  Loss: 0.3190668523311615\n",
      "Step 1  Loss: 0.2788778841495514\n",
      "Step 2  Loss: 0.39286279678344727\n",
      "Step 3  Loss: 0.3250733017921448\n",
      "Step 4  Loss: 0.3362169861793518\n",
      "Step 5  Loss: 0.40044182538986206\n",
      "Step 6  Loss: 0.3039228618144989\n",
      "Step 7  Loss: 0.4025624394416809\n",
      "Step 8  Loss: 0.35180169343948364\n",
      "Step 9  Loss: 0.29727238416671753\n",
      "Step 10  Loss: 0.5296369791030884\n",
      "total Loss of epoch  204  is  3.937736004590988\n",
      "Step 0  Loss: 0.3282080590724945\n",
      "Step 1  Loss: 0.38016629219055176\n",
      "Step 2  Loss: 0.38375863432884216\n",
      "Step 3  Loss: 0.3460204005241394\n",
      "Step 4  Loss: 0.3064236640930176\n",
      "Step 5  Loss: 0.3917713165283203\n",
      "Step 6  Loss: 0.4052925109863281\n",
      "Step 7  Loss: 0.4444325566291809\n",
      "Step 8  Loss: 0.4027193486690521\n",
      "Step 9  Loss: 0.3160841763019562\n",
      "Step 10  Loss: 0.3825174570083618\n",
      "total Loss of epoch  205  is  4.087394416332245\n",
      "Step 0  Loss: 0.31017598509788513\n",
      "Step 1  Loss: 0.38784652948379517\n",
      "Step 2  Loss: 0.40175265073776245\n",
      "Step 3  Loss: 0.2811169922351837\n",
      "Step 4  Loss: 0.4271486699581146\n",
      "Step 5  Loss: 0.3742067217826843\n",
      "Step 6  Loss: 0.43744412064552307\n",
      "Step 7  Loss: 0.3966885507106781\n",
      "Step 8  Loss: 0.3154352009296417\n",
      "Step 9  Loss: 0.2725807726383209\n",
      "Step 10  Loss: 0.5881369113922119\n",
      "total Loss of epoch  206  is  4.192533105611801\n",
      "Step 0  Loss: 0.505848228931427\n",
      "Step 1  Loss: 0.35367101430892944\n",
      "Step 2  Loss: 0.4020254611968994\n",
      "Step 3  Loss: 0.3052671253681183\n",
      "Step 4  Loss: 0.30966490507125854\n",
      "Step 5  Loss: 0.33361172676086426\n",
      "Step 6  Loss: 0.3499269187450409\n",
      "Step 7  Loss: 0.3120647668838501\n",
      "Step 8  Loss: 0.23802997171878815\n",
      "Step 9  Loss: 0.38342103362083435\n",
      "Step 10  Loss: 0.32766690850257874\n",
      "total Loss of epoch  207  is  3.821198061108589\n",
      "Step 0  Loss: 0.31490597128868103\n",
      "Step 1  Loss: 0.4379626214504242\n",
      "Step 2  Loss: 0.34814679622650146\n",
      "Step 3  Loss: 0.4162634313106537\n",
      "Step 4  Loss: 0.35752740502357483\n",
      "Step 5  Loss: 0.24723713099956512\n",
      "Step 6  Loss: 0.2634578049182892\n",
      "Step 7  Loss: 0.3071476221084595\n",
      "Step 8  Loss: 0.45478355884552\n",
      "Step 9  Loss: 0.401983380317688\n",
      "Step 10  Loss: 0.4744139015674591\n",
      "total Loss of epoch  208  is  4.023829624056816\n",
      "Step 0  Loss: 0.36619195342063904\n",
      "Step 1  Loss: 0.3465563654899597\n",
      "Step 2  Loss: 0.34008681774139404\n",
      "Step 3  Loss: 0.3685343563556671\n",
      "Step 4  Loss: 0.433821439743042\n",
      "Step 5  Loss: 0.3376412093639374\n",
      "Step 6  Loss: 0.39135485887527466\n",
      "Step 7  Loss: 0.28832894563674927\n",
      "Step 8  Loss: 0.41865015029907227\n",
      "Step 9  Loss: 0.44865885376930237\n",
      "Step 10  Loss: 0.3723134398460388\n",
      "total Loss of epoch  209  is  4.112138390541077\n",
      "Step 0  Loss: 0.4122582674026489\n",
      "Step 1  Loss: 0.40586164593696594\n",
      "Step 2  Loss: 0.4443346858024597\n",
      "Step 3  Loss: 0.3402024805545807\n",
      "Step 4  Loss: 0.44102180004119873\n",
      "Step 5  Loss: 0.32539013028144836\n",
      "Step 6  Loss: 0.44963330030441284\n",
      "Step 7  Loss: 0.48583588004112244\n",
      "Step 8  Loss: 0.4685000777244568\n",
      "Step 9  Loss: 0.3868010938167572\n",
      "Step 10  Loss: 0.4623994529247284\n",
      "total Loss of epoch  210  is  4.62223881483078\n",
      "Step 0  Loss: 0.37673985958099365\n",
      "Step 1  Loss: 0.42466744780540466\n",
      "Step 2  Loss: 0.465960830450058\n",
      "Step 3  Loss: 0.39736396074295044\n",
      "Step 4  Loss: 0.392393559217453\n",
      "Step 5  Loss: 0.3581594228744507\n",
      "Step 6  Loss: 0.3931718170642853\n",
      "Step 7  Loss: 0.3907548487186432\n",
      "Step 8  Loss: 0.4635566473007202\n",
      "Step 9  Loss: 0.332322895526886\n",
      "Step 10  Loss: 0.4488639831542969\n",
      "total Loss of epoch  211  is  4.443955272436142\n",
      "Step 0  Loss: 0.3073663115501404\n",
      "Step 1  Loss: 0.37329980731010437\n",
      "Step 2  Loss: 0.3923895061016083\n",
      "Step 3  Loss: 0.3291483521461487\n",
      "Step 4  Loss: 0.40678852796554565\n",
      "Step 5  Loss: 0.332186758518219\n",
      "Step 6  Loss: 0.4209985136985779\n",
      "Step 7  Loss: 0.42429953813552856\n",
      "Step 8  Loss: 0.3184375762939453\n",
      "Step 9  Loss: 0.4021466374397278\n",
      "Step 10  Loss: 0.3635466396808624\n",
      "total Loss of epoch  212  is  4.070608168840408\n",
      "Step 0  Loss: 0.288118839263916\n",
      "Step 1  Loss: 0.3923799395561218\n",
      "Step 2  Loss: 0.3566068708896637\n",
      "Step 3  Loss: 0.3535211384296417\n",
      "Step 4  Loss: 0.41636306047439575\n",
      "Step 5  Loss: 0.42948779463768005\n",
      "Step 6  Loss: 0.47587332129478455\n",
      "Step 7  Loss: 0.30780544877052307\n",
      "Step 8  Loss: 0.3868681490421295\n",
      "Step 9  Loss: 0.42560258507728577\n",
      "Step 10  Loss: 0.24037215113639832\n",
      "total Loss of epoch  213  is  4.07299929857254\n",
      "Step 0  Loss: 0.312038779258728\n",
      "Step 1  Loss: 0.39710718393325806\n",
      "Step 2  Loss: 0.4205946922302246\n",
      "Step 3  Loss: 0.4313832223415375\n",
      "Step 4  Loss: 0.2843013405799866\n",
      "Step 5  Loss: 0.3157839775085449\n",
      "Step 6  Loss: 0.2091354876756668\n",
      "Step 7  Loss: 0.36882486939430237\n",
      "Step 8  Loss: 0.2604255676269531\n",
      "Step 9  Loss: 0.3551311790943146\n",
      "Step 10  Loss: 0.425068199634552\n",
      "total Loss of epoch  214  is  3.7797944992780685\n",
      "Step 0  Loss: 0.34826332330703735\n",
      "Step 1  Loss: 0.40513741970062256\n",
      "Step 2  Loss: 0.4209103584289551\n",
      "Step 3  Loss: 0.29247206449508667\n",
      "Step 4  Loss: 0.33181771636009216\n",
      "Step 5  Loss: 0.33842241764068604\n",
      "Step 6  Loss: 0.42802131175994873\n",
      "Step 7  Loss: 0.4015900790691376\n",
      "Step 8  Loss: 0.31814977526664734\n",
      "Step 9  Loss: 0.3639318645000458\n",
      "Step 10  Loss: 0.37183094024658203\n",
      "total Loss of epoch  215  is  4.020547270774841\n",
      "Step 0  Loss: 0.2917320132255554\n",
      "Step 1  Loss: 0.4271546006202698\n",
      "Step 2  Loss: 0.2650638818740845\n",
      "Step 3  Loss: 0.3182949423789978\n",
      "Step 4  Loss: 0.42620784044265747\n",
      "Step 5  Loss: 0.4130616784095764\n",
      "Step 6  Loss: 0.26749446988105774\n",
      "Step 7  Loss: 0.36817696690559387\n",
      "Step 8  Loss: 0.3383336067199707\n",
      "Step 9  Loss: 0.46844667196273804\n",
      "Step 10  Loss: 0.4094085693359375\n",
      "total Loss of epoch  216  is  3.993375241756439\n",
      "Step 0  Loss: 0.2814273536205292\n",
      "Step 1  Loss: 0.4328872561454773\n",
      "Step 2  Loss: 0.38028794527053833\n",
      "Step 3  Loss: 0.2401845008134842\n",
      "Step 4  Loss: 0.37014031410217285\n",
      "Step 5  Loss: 0.35409051179885864\n",
      "Step 6  Loss: 0.38479700684547424\n",
      "Step 7  Loss: 0.3234170973300934\n",
      "Step 8  Loss: 0.3151547610759735\n",
      "Step 9  Loss: 0.4315407872200012\n",
      "Step 10  Loss: 0.3887157738208771\n",
      "total Loss of epoch  217  is  3.90264330804348\n",
      "Step 0  Loss: 0.4375856816768646\n",
      "Step 1  Loss: 0.46389898657798767\n",
      "Step 2  Loss: 0.3885924816131592\n",
      "Step 3  Loss: 0.3933999836444855\n",
      "Step 4  Loss: 0.406308650970459\n",
      "Step 5  Loss: 0.27621498703956604\n",
      "Step 6  Loss: 0.366851270198822\n",
      "Step 7  Loss: 0.3821769952774048\n",
      "Step 8  Loss: 0.3462144434452057\n",
      "Step 9  Loss: 0.4492188096046448\n",
      "Step 10  Loss: 0.3269595503807068\n",
      "total Loss of epoch  218  is  4.237421840429306\n",
      "Step 0  Loss: 0.4409634470939636\n",
      "Step 1  Loss: 0.39439868927001953\n",
      "Step 2  Loss: 0.4648756980895996\n",
      "Step 3  Loss: 0.2985295355319977\n",
      "Step 4  Loss: 0.42091232538223267\n",
      "Step 5  Loss: 0.3501012921333313\n",
      "Step 6  Loss: 0.39761674404144287\n",
      "Step 7  Loss: 0.41327208280563354\n",
      "Step 8  Loss: 0.2754976153373718\n",
      "Step 9  Loss: 0.3677885830402374\n",
      "Step 10  Loss: 0.5164359211921692\n",
      "total Loss of epoch  219  is  4.340391933917999\n",
      "Step 0  Loss: 0.44442036747932434\n",
      "Step 1  Loss: 0.26862964034080505\n",
      "Step 2  Loss: 0.4156634211540222\n",
      "Step 3  Loss: 0.32569146156311035\n",
      "Step 4  Loss: 0.4323714077472687\n",
      "Step 5  Loss: 0.3611096441745758\n",
      "Step 6  Loss: 0.3121291399002075\n",
      "Step 7  Loss: 0.23441363871097565\n",
      "Step 8  Loss: 0.23470750451087952\n",
      "Step 9  Loss: 0.35237202048301697\n",
      "Step 10  Loss: 0.518883466720581\n",
      "total Loss of epoch  220  is  3.900391712784767\n",
      "Step 0  Loss: 0.4147997200489044\n",
      "Step 1  Loss: 0.3991704285144806\n",
      "Step 2  Loss: 0.2464575171470642\n",
      "Step 3  Loss: 0.4739112854003906\n",
      "Step 4  Loss: 0.3732420802116394\n",
      "Step 5  Loss: 0.3501388132572174\n",
      "Step 6  Loss: 0.41949403285980225\n",
      "Step 7  Loss: 0.4227589964866638\n",
      "Step 8  Loss: 0.24440602958202362\n",
      "Step 9  Loss: 0.2778955399990082\n",
      "Step 10  Loss: 0.3934592008590698\n",
      "total Loss of epoch  221  is  4.015733644366264\n",
      "Step 0  Loss: 0.255595862865448\n",
      "Step 1  Loss: 0.39940306544303894\n",
      "Step 2  Loss: 0.37562933564186096\n",
      "Step 3  Loss: 0.3753279447555542\n",
      "Step 4  Loss: 0.4034528136253357\n",
      "Step 5  Loss: 0.3376614451408386\n",
      "Step 6  Loss: 0.38915500044822693\n",
      "Step 7  Loss: 0.4103780686855316\n",
      "Step 8  Loss: 0.39849725365638733\n",
      "Step 9  Loss: 0.29316750168800354\n",
      "Step 10  Loss: 0.25458523631095886\n",
      "total Loss of epoch  222  is  3.8928535282611847\n",
      "Step 0  Loss: 0.42623138427734375\n",
      "Step 1  Loss: 0.45141884684562683\n",
      "Step 2  Loss: 0.4170996844768524\n",
      "Step 3  Loss: 0.3838067948818207\n",
      "Step 4  Loss: 0.29074737429618835\n",
      "Step 5  Loss: 0.376950740814209\n",
      "Step 6  Loss: 0.45766356587409973\n",
      "Step 7  Loss: 0.27331286668777466\n",
      "Step 8  Loss: 0.4373016953468323\n",
      "Step 9  Loss: 0.4611283242702484\n",
      "Step 10  Loss: 0.35713183879852295\n",
      "total Loss of epoch  223  is  4.332793116569519\n",
      "Step 0  Loss: 0.3975508213043213\n",
      "Step 1  Loss: 0.39157089591026306\n",
      "Step 2  Loss: 0.3361046016216278\n",
      "Step 3  Loss: 0.4288433790206909\n",
      "Step 4  Loss: 0.3373708128929138\n",
      "Step 5  Loss: 0.38596776127815247\n",
      "Step 6  Loss: 0.3409758508205414\n",
      "Step 7  Loss: 0.4314839243888855\n",
      "Step 8  Loss: 0.41198039054870605\n",
      "Step 9  Loss: 0.3196566104888916\n",
      "Step 10  Loss: 0.3872012495994568\n",
      "total Loss of epoch  224  is  4.168706297874451\n",
      "Step 0  Loss: 0.3748528063297272\n",
      "Step 1  Loss: 0.4619007706642151\n",
      "Step 2  Loss: 0.35358336567878723\n",
      "Step 3  Loss: 0.3767508566379547\n",
      "Step 4  Loss: 0.26988351345062256\n",
      "Step 5  Loss: 0.3122934401035309\n",
      "Step 6  Loss: 0.5117284059524536\n",
      "Step 7  Loss: 0.446918785572052\n",
      "Step 8  Loss: 0.34654513001441956\n",
      "Step 9  Loss: 0.26626142859458923\n",
      "Step 10  Loss: 0.4766707718372345\n",
      "total Loss of epoch  225  is  4.1973892748355865\n",
      "Step 0  Loss: 0.3892514705657959\n",
      "Step 1  Loss: 0.35298287868499756\n",
      "Step 2  Loss: 0.2727507948875427\n",
      "Step 3  Loss: 0.2674695551395416\n",
      "Step 4  Loss: 0.37368863821029663\n",
      "Step 5  Loss: 0.44238632917404175\n",
      "Step 6  Loss: 0.42731496691703796\n",
      "Step 7  Loss: 0.4582872688770294\n",
      "Step 8  Loss: 0.34019142389297485\n",
      "Step 9  Loss: 0.3223690390586853\n",
      "Step 10  Loss: 0.31390535831451416\n",
      "total Loss of epoch  226  is  3.960597723722458\n",
      "Step 0  Loss: 0.36821243166923523\n",
      "Step 1  Loss: 0.35825222730636597\n",
      "Step 2  Loss: 0.3138357698917389\n",
      "Step 3  Loss: 0.39056849479675293\n",
      "Step 4  Loss: 0.4010617434978485\n",
      "Step 5  Loss: 0.25910332798957825\n",
      "Step 6  Loss: 0.533754825592041\n",
      "Step 7  Loss: 0.3011583089828491\n",
      "Step 8  Loss: 0.33590978384017944\n",
      "Step 9  Loss: 0.42920586466789246\n",
      "Step 10  Loss: 0.3213019371032715\n",
      "total Loss of epoch  227  is  4.012364715337753\n",
      "Step 0  Loss: 0.280439555644989\n",
      "Step 1  Loss: 0.451387494802475\n",
      "Step 2  Loss: 0.45630547404289246\n",
      "Step 3  Loss: 0.41751334071159363\n",
      "Step 4  Loss: 0.2855099141597748\n",
      "Step 5  Loss: 0.3735240697860718\n",
      "Step 6  Loss: 0.2934028208255768\n",
      "Step 7  Loss: 0.3496951460838318\n",
      "Step 8  Loss: 0.42015841603279114\n",
      "Step 9  Loss: 0.36243098974227905\n",
      "Step 10  Loss: 0.3844203054904938\n",
      "total Loss of epoch  228  is  4.074787527322769\n",
      "Step 0  Loss: 0.4580228626728058\n",
      "Step 1  Loss: 0.38826724886894226\n",
      "Step 2  Loss: 0.22490687668323517\n",
      "Step 3  Loss: 0.2757861614227295\n",
      "Step 4  Loss: 0.46221429109573364\n",
      "Step 5  Loss: 0.43790924549102783\n",
      "Step 6  Loss: 0.31197842955589294\n",
      "Step 7  Loss: 0.3512914180755615\n",
      "Step 8  Loss: 0.4346138834953308\n",
      "Step 9  Loss: 0.3248968720436096\n",
      "Step 10  Loss: 0.3356384336948395\n",
      "total Loss of epoch  229  is  4.005525723099709\n",
      "Step 0  Loss: 0.31029200553894043\n",
      "Step 1  Loss: 0.29453107714653015\n",
      "Step 2  Loss: 0.3375377953052521\n",
      "Step 3  Loss: 0.357856810092926\n",
      "Step 4  Loss: 0.38582131266593933\n",
      "Step 5  Loss: 0.4268489181995392\n",
      "Step 6  Loss: 0.4477851390838623\n",
      "Step 7  Loss: 0.29722967743873596\n",
      "Step 8  Loss: 0.4110507071018219\n",
      "Step 9  Loss: 0.39158734679222107\n",
      "Step 10  Loss: 0.22700120508670807\n",
      "total Loss of epoch  230  is  3.8875419944524765\n",
      "Step 0  Loss: 0.3229760229587555\n",
      "Step 1  Loss: 0.42346256971359253\n",
      "Step 2  Loss: 0.39624929428100586\n",
      "Step 3  Loss: 0.23777802288532257\n",
      "Step 4  Loss: 0.46806591749191284\n",
      "Step 5  Loss: 0.36849457025527954\n",
      "Step 6  Loss: 0.3504652976989746\n",
      "Step 7  Loss: 0.39493757486343384\n",
      "Step 8  Loss: 0.3506752848625183\n",
      "Step 9  Loss: 0.43456020951271057\n",
      "Step 10  Loss: 0.20724551379680634\n",
      "total Loss of epoch  231  is  3.9549102783203125\n",
      "Step 0  Loss: 0.3299345374107361\n",
      "Step 1  Loss: 0.3495244085788727\n",
      "Step 2  Loss: 0.25045937299728394\n",
      "Step 3  Loss: 0.4337944984436035\n",
      "Step 4  Loss: 0.4514443576335907\n",
      "Step 5  Loss: 0.27329346537590027\n",
      "Step 6  Loss: 0.4406365156173706\n",
      "Step 7  Loss: 0.4671418368816376\n",
      "Step 8  Loss: 0.25299879908561707\n",
      "Step 9  Loss: 0.3654586970806122\n",
      "Step 10  Loss: 0.32988181710243225\n",
      "total Loss of epoch  232  is  3.944568306207657\n",
      "Step 0  Loss: 0.47628700733184814\n",
      "Step 1  Loss: 0.48974236845970154\n",
      "Step 2  Loss: 0.34112709760665894\n",
      "Step 3  Loss: 0.40559959411621094\n",
      "Step 4  Loss: 0.41561540961265564\n",
      "Step 5  Loss: 0.36014628410339355\n",
      "Step 6  Loss: 0.2983713448047638\n",
      "Step 7  Loss: 0.3568364083766937\n",
      "Step 8  Loss: 0.29688137769699097\n",
      "Step 9  Loss: 0.42305058240890503\n",
      "Step 10  Loss: 0.469475656747818\n",
      "total Loss of epoch  233  is  4.33313313126564\n",
      "Step 0  Loss: 0.3318922817707062\n",
      "Step 1  Loss: 0.4216271936893463\n",
      "Step 2  Loss: 0.4410209655761719\n",
      "Step 3  Loss: 0.3287333846092224\n",
      "Step 4  Loss: 0.36777809262275696\n",
      "Step 5  Loss: 0.3649638295173645\n",
      "Step 6  Loss: 0.3002849221229553\n",
      "Step 7  Loss: 0.44532477855682373\n",
      "Step 8  Loss: 0.2766352593898773\n",
      "Step 9  Loss: 0.32240062952041626\n",
      "Step 10  Loss: 0.49186795949935913\n",
      "total Loss of epoch  234  is  4.092529296875\n",
      "Step 0  Loss: 0.36730945110321045\n",
      "Step 1  Loss: 0.2801033854484558\n",
      "Step 2  Loss: 0.45796698331832886\n",
      "Step 3  Loss: 0.30136772990226746\n",
      "Step 4  Loss: 0.3887368440628052\n",
      "Step 5  Loss: 0.33560049533843994\n",
      "Step 6  Loss: 0.3401302695274353\n",
      "Step 7  Loss: 0.2643952965736389\n",
      "Step 8  Loss: 0.3803311288356781\n",
      "Step 9  Loss: 0.48337286710739136\n",
      "Step 10  Loss: 0.32250064611434937\n",
      "total Loss of epoch  235  is  3.9218150973320007\n",
      "Step 0  Loss: 0.2686402499675751\n",
      "Step 1  Loss: 0.39599111676216125\n",
      "Step 2  Loss: 0.3784518539905548\n",
      "Step 3  Loss: 0.30465325713157654\n",
      "Step 4  Loss: 0.40517452359199524\n",
      "Step 5  Loss: 0.3557165265083313\n",
      "Step 6  Loss: 0.35149848461151123\n",
      "Step 7  Loss: 0.3345530927181244\n",
      "Step 8  Loss: 0.34405145049095154\n",
      "Step 9  Loss: 0.40380698442459106\n",
      "Step 10  Loss: 0.42381206154823303\n",
      "total Loss of epoch  236  is  3.9663496017456055\n",
      "Step 0  Loss: 0.4310203194618225\n",
      "Step 1  Loss: 0.4180218577384949\n",
      "Step 2  Loss: 0.2916024923324585\n",
      "Step 3  Loss: 0.4528108537197113\n",
      "Step 4  Loss: 0.32863831520080566\n",
      "Step 5  Loss: 0.3732844889163971\n",
      "Step 6  Loss: 0.3780333399772644\n",
      "Step 7  Loss: 0.448406845331192\n",
      "Step 8  Loss: 0.33611318469047546\n",
      "Step 9  Loss: 0.40727749466896057\n",
      "Step 10  Loss: 0.46137750148773193\n",
      "total Loss of epoch  237  is  4.326586693525314\n",
      "Step 0  Loss: 0.3549939692020416\n",
      "Step 1  Loss: 0.3274536430835724\n",
      "Step 2  Loss: 0.4527304470539093\n",
      "Step 3  Loss: 0.39510321617126465\n",
      "Step 4  Loss: 0.420766144990921\n",
      "Step 5  Loss: 0.29156482219696045\n",
      "Step 6  Loss: 0.4070841670036316\n",
      "Step 7  Loss: 0.2613743841648102\n",
      "Step 8  Loss: 0.35173481702804565\n",
      "Step 9  Loss: 0.37793421745300293\n",
      "Step 10  Loss: 0.3813251852989197\n",
      "total Loss of epoch  238  is  4.0220650136470795\n",
      "Step 0  Loss: 0.41697609424591064\n",
      "Step 1  Loss: 0.33811154961586\n",
      "Step 2  Loss: 0.3799273371696472\n",
      "Step 3  Loss: 0.34403735399246216\n",
      "Step 4  Loss: 0.39706775546073914\n",
      "Step 5  Loss: 0.3499089181423187\n",
      "Step 6  Loss: 0.4012709856033325\n",
      "Step 7  Loss: 0.36198294162750244\n",
      "Step 8  Loss: 0.45361146330833435\n",
      "Step 9  Loss: 0.3579525947570801\n",
      "Step 10  Loss: 0.29436424374580383\n",
      "total Loss of epoch  239  is  4.095211237668991\n",
      "Step 0  Loss: 0.33700770139694214\n",
      "Step 1  Loss: 0.33050239086151123\n",
      "Step 2  Loss: 0.38374224305152893\n",
      "Step 3  Loss: 0.35179492831230164\n",
      "Step 4  Loss: 0.42735108733177185\n",
      "Step 5  Loss: 0.44639742374420166\n",
      "Step 6  Loss: 0.511281430721283\n",
      "Step 7  Loss: 0.3225606679916382\n",
      "Step 8  Loss: 0.31325313448905945\n",
      "Step 9  Loss: 0.3397018611431122\n",
      "Step 10  Loss: 0.21740224957466125\n",
      "total Loss of epoch  240  is  3.9809951186180115\n",
      "Step 0  Loss: 0.40678054094314575\n",
      "Step 1  Loss: 0.24311238527297974\n",
      "Step 2  Loss: 0.3624114394187927\n",
      "Step 3  Loss: 0.33357468247413635\n",
      "Step 4  Loss: 0.29288333654403687\n",
      "Step 5  Loss: 0.40628302097320557\n",
      "Step 6  Loss: 0.3333909809589386\n",
      "Step 7  Loss: 0.3861834406852722\n",
      "Step 8  Loss: 0.4074704051017761\n",
      "Step 9  Loss: 0.3187881410121918\n",
      "Step 10  Loss: 0.36137381196022034\n",
      "total Loss of epoch  241  is  3.852252185344696\n",
      "Step 0  Loss: 0.3592017889022827\n",
      "Step 1  Loss: 0.3655245006084442\n",
      "Step 2  Loss: 0.4106678366661072\n",
      "Step 3  Loss: 0.41455626487731934\n",
      "Step 4  Loss: 0.32244157791137695\n",
      "Step 5  Loss: 0.23109689354896545\n",
      "Step 6  Loss: 0.40123823285102844\n",
      "Step 7  Loss: 0.42716550827026367\n",
      "Step 8  Loss: 0.2377733588218689\n",
      "Step 9  Loss: 0.47074106335639954\n",
      "Step 10  Loss: 0.3566221594810486\n",
      "total Loss of epoch  242  is  3.997029185295105\n",
      "Step 0  Loss: 0.4129316508769989\n",
      "Step 1  Loss: 0.3296646475791931\n",
      "Step 2  Loss: 0.34319430589675903\n",
      "Step 3  Loss: 0.45598292350769043\n",
      "Step 4  Loss: 0.32742682099342346\n",
      "Step 5  Loss: 0.40334948897361755\n",
      "Step 6  Loss: 0.4588288366794586\n",
      "Step 7  Loss: 0.47599849104881287\n",
      "Step 8  Loss: 0.4328368902206421\n",
      "Step 9  Loss: 0.26966652274131775\n",
      "Step 10  Loss: 0.2706199884414673\n",
      "total Loss of epoch  243  is  4.180500566959381\n",
      "Step 0  Loss: 0.34487301111221313\n",
      "Step 1  Loss: 0.3713858723640442\n",
      "Step 2  Loss: 0.3285626471042633\n",
      "Step 3  Loss: 0.22050270438194275\n",
      "Step 4  Loss: 0.3947423994541168\n",
      "Step 5  Loss: 0.3396042287349701\n",
      "Step 6  Loss: 0.31721025705337524\n",
      "Step 7  Loss: 0.2587323486804962\n",
      "Step 8  Loss: 0.3129723072052002\n",
      "Step 9  Loss: 0.15310053527355194\n",
      "Step 10  Loss: 0.40687236189842224\n",
      "total Loss of epoch  244  is  3.448558673262596\n",
      "Step 0  Loss: 0.4407547116279602\n",
      "Step 1  Loss: 0.28141164779663086\n",
      "Step 2  Loss: 0.2709594964981079\n",
      "Step 3  Loss: 0.3596583604812622\n",
      "Step 4  Loss: 0.4613696038722992\n",
      "Step 5  Loss: 0.3758075535297394\n",
      "Step 6  Loss: 0.42365774512290955\n",
      "Step 7  Loss: 0.3164218068122864\n",
      "Step 8  Loss: 0.3567715883255005\n",
      "Step 9  Loss: 0.32271528244018555\n",
      "Step 10  Loss: 0.48351752758026123\n",
      "total Loss of epoch  245  is  4.093045324087143\n",
      "Step 0  Loss: 0.40804317593574524\n",
      "Step 1  Loss: 0.23329105973243713\n",
      "Step 2  Loss: 0.24110698699951172\n",
      "Step 3  Loss: 0.3848806917667389\n",
      "Step 4  Loss: 0.4161602854728699\n",
      "Step 5  Loss: 0.3504869341850281\n",
      "Step 6  Loss: 0.3659384846687317\n",
      "Step 7  Loss: 0.4380744695663452\n",
      "Step 8  Loss: 0.4891471862792969\n",
      "Step 9  Loss: 0.3320325016975403\n",
      "Step 10  Loss: 0.34219083189964294\n",
      "total Loss of epoch  246  is  4.001352608203888\n",
      "Step 0  Loss: 0.4198794662952423\n",
      "Step 1  Loss: 0.49249035120010376\n",
      "Step 2  Loss: 0.39338722825050354\n",
      "Step 3  Loss: 0.4033229351043701\n",
      "Step 4  Loss: 0.35636234283447266\n",
      "Step 5  Loss: 0.38415399193763733\n",
      "Step 6  Loss: 0.33574166893959045\n",
      "Step 7  Loss: 0.3176306188106537\n",
      "Step 8  Loss: 0.3501846194267273\n",
      "Step 9  Loss: 0.4085526466369629\n",
      "Step 10  Loss: 0.28583550453186035\n",
      "total Loss of epoch  247  is  4.147541373968124\n",
      "Step 0  Loss: 0.2667229175567627\n",
      "Step 1  Loss: 0.3765290677547455\n",
      "Step 2  Loss: 0.33987295627593994\n",
      "Step 3  Loss: 0.25632697343826294\n",
      "Step 4  Loss: 0.3333006799221039\n",
      "Step 5  Loss: 0.35051655769348145\n",
      "Step 6  Loss: 0.397668719291687\n",
      "Step 7  Loss: 0.3121819496154785\n",
      "Step 8  Loss: 0.3423452377319336\n",
      "Step 9  Loss: 0.3188963830471039\n",
      "Step 10  Loss: 0.35987117886543274\n",
      "total Loss of epoch  248  is  3.654232621192932\n",
      "Step 0  Loss: 0.4921360909938812\n",
      "Step 1  Loss: 0.44012099504470825\n",
      "Step 2  Loss: 0.42943161725997925\n",
      "Step 3  Loss: 0.34718838334083557\n",
      "Step 4  Loss: 0.33327504992485046\n",
      "Step 5  Loss: 0.35774996876716614\n",
      "Step 6  Loss: 0.3484646677970886\n",
      "Step 7  Loss: 0.38845396041870117\n",
      "Step 8  Loss: 0.3879140615463257\n",
      "Step 9  Loss: 0.3740289807319641\n",
      "Step 10  Loss: 0.34048834443092346\n",
      "total Loss of epoch  249  is  4.239252120256424\n",
      "Step 0  Loss: 0.3466765284538269\n",
      "Step 1  Loss: 0.42211174964904785\n",
      "Step 2  Loss: 0.35869359970092773\n",
      "Step 3  Loss: 0.32475608587265015\n",
      "Step 4  Loss: 0.3448495864868164\n",
      "Step 5  Loss: 0.40740153193473816\n",
      "Step 6  Loss: 0.4120144844055176\n",
      "Step 7  Loss: 0.39382806420326233\n",
      "Step 8  Loss: 0.3092213273048401\n",
      "Step 9  Loss: 0.3230329155921936\n",
      "Step 10  Loss: 0.4199868440628052\n",
      "total Loss of epoch  250  is  4.062572717666626\n",
      "Step 0  Loss: 0.3082053065299988\n",
      "Step 1  Loss: 0.3323627710342407\n",
      "Step 2  Loss: 0.42940059304237366\n",
      "Step 3  Loss: 0.28423207998275757\n",
      "Step 4  Loss: 0.4394063651561737\n",
      "Step 5  Loss: 0.39242926239967346\n",
      "Step 6  Loss: 0.29897022247314453\n",
      "Step 7  Loss: 0.3568810224533081\n",
      "Step 8  Loss: 0.35674265027046204\n",
      "Step 9  Loss: 0.29732757806777954\n",
      "Step 10  Loss: 0.5205644965171814\n",
      "total Loss of epoch  251  is  4.0165223479270935\n",
      "Step 0  Loss: 0.43601542711257935\n",
      "Step 1  Loss: 0.3094943165779114\n",
      "Step 2  Loss: 0.31292814016342163\n",
      "Step 3  Loss: 0.3296026587486267\n",
      "Step 4  Loss: 0.3609122335910797\n",
      "Step 5  Loss: 0.3861011564731598\n",
      "Step 6  Loss: 0.3165114223957062\n",
      "Step 7  Loss: 0.31993016600608826\n",
      "Step 8  Loss: 0.4622172713279724\n",
      "Step 9  Loss: 0.3613145351409912\n",
      "Step 10  Loss: 0.4073571264743805\n",
      "total Loss of epoch  252  is  4.002384454011917\n",
      "Step 0  Loss: 0.4107761085033417\n",
      "Step 1  Loss: 0.4197903871536255\n",
      "Step 2  Loss: 0.2913173735141754\n",
      "Step 3  Loss: 0.3852843940258026\n",
      "Step 4  Loss: 0.4755047559738159\n",
      "Step 5  Loss: 0.21787197887897491\n",
      "Step 6  Loss: 0.4379352927207947\n",
      "Step 7  Loss: 0.35445553064346313\n",
      "Step 8  Loss: 0.39448821544647217\n",
      "Step 9  Loss: 0.4283491373062134\n",
      "Step 10  Loss: 0.43445059657096863\n",
      "total Loss of epoch  253  is  4.250223770737648\n",
      "Step 0  Loss: 0.25101688504219055\n",
      "Step 1  Loss: 0.3495262563228607\n",
      "Step 2  Loss: 0.4507489502429962\n",
      "Step 3  Loss: 0.31269553303718567\n",
      "Step 4  Loss: 0.29219916462898254\n",
      "Step 5  Loss: 0.3343938887119293\n",
      "Step 6  Loss: 0.39706701040267944\n",
      "Step 7  Loss: 0.3570065200328827\n",
      "Step 8  Loss: 0.40566176176071167\n",
      "Step 9  Loss: 0.32110795378685\n",
      "Step 10  Loss: 0.44758060574531555\n",
      "total Loss of epoch  254  is  3.9190045297145844\n",
      "Step 0  Loss: 0.32920435070991516\n",
      "Step 1  Loss: 0.5070495009422302\n",
      "Step 2  Loss: 0.3428191840648651\n",
      "Step 3  Loss: 0.36640721559524536\n",
      "Step 4  Loss: 0.37155625224113464\n",
      "Step 5  Loss: 0.40914881229400635\n",
      "Step 6  Loss: 0.33677759766578674\n",
      "Step 7  Loss: 0.322437584400177\n",
      "Step 8  Loss: 0.4503211975097656\n",
      "Step 9  Loss: 0.39334508776664734\n",
      "Step 10  Loss: 0.360931396484375\n",
      "total Loss of epoch  255  is  4.189998179674149\n",
      "Step 0  Loss: 0.3735588788986206\n",
      "Step 1  Loss: 0.33860379457473755\n",
      "Step 2  Loss: 0.37268778681755066\n",
      "Step 3  Loss: 0.3048846125602722\n",
      "Step 4  Loss: 0.35554561018943787\n",
      "Step 5  Loss: 0.4124729633331299\n",
      "Step 6  Loss: 0.410163551568985\n",
      "Step 7  Loss: 0.397644579410553\n",
      "Step 8  Loss: 0.29699212312698364\n",
      "Step 9  Loss: 0.3933762311935425\n",
      "Step 10  Loss: 0.5855849385261536\n",
      "total Loss of epoch  256  is  4.241515070199966\n",
      "Step 0  Loss: 0.4279007315635681\n",
      "Step 1  Loss: 0.4982321560382843\n",
      "Step 2  Loss: 0.4294637143611908\n",
      "Step 3  Loss: 0.4530393183231354\n",
      "Step 4  Loss: 0.43294617533683777\n",
      "Step 5  Loss: 0.2195947766304016\n",
      "Step 6  Loss: 0.4569062888622284\n",
      "Step 7  Loss: 0.35053157806396484\n",
      "Step 8  Loss: 0.3465876579284668\n",
      "Step 9  Loss: 0.4272438585758209\n",
      "Step 10  Loss: 0.40652260184288025\n",
      "total Loss of epoch  257  is  4.448968857526779\n",
      "Step 0  Loss: 0.4269765615463257\n",
      "Step 1  Loss: 0.33790722489356995\n",
      "Step 2  Loss: 0.4591955840587616\n",
      "Step 3  Loss: 0.45186835527420044\n",
      "Step 4  Loss: 0.39495164155960083\n",
      "Step 5  Loss: 0.33846530318260193\n",
      "Step 6  Loss: 0.24133677780628204\n",
      "Step 7  Loss: 0.29980170726776123\n",
      "Step 8  Loss: 0.3250582814216614\n",
      "Step 9  Loss: 0.48312342166900635\n",
      "Step 10  Loss: 0.30936330556869507\n",
      "total Loss of epoch  258  is  4.0680481642484665\n",
      "Step 0  Loss: 0.29585543274879456\n",
      "Step 1  Loss: 0.3141937553882599\n",
      "Step 2  Loss: 0.3560003340244293\n",
      "Step 3  Loss: 0.3225843906402588\n",
      "Step 4  Loss: 0.34892261028289795\n",
      "Step 5  Loss: 0.38155099749565125\n",
      "Step 6  Loss: 0.38333597779273987\n",
      "Step 7  Loss: 0.3844144642353058\n",
      "Step 8  Loss: 0.3875523805618286\n",
      "Step 9  Loss: 0.4248989224433899\n",
      "Step 10  Loss: 0.36751431226730347\n",
      "total Loss of epoch  259  is  3.9668235778808594\n",
      "Step 0  Loss: 0.40815502405166626\n",
      "Step 1  Loss: 0.41470760107040405\n",
      "Step 2  Loss: 0.3349229693412781\n",
      "Step 3  Loss: 0.43016234040260315\n",
      "Step 4  Loss: 0.34216612577438354\n",
      "Step 5  Loss: 0.38829126954078674\n",
      "Step 6  Loss: 0.47046273946762085\n",
      "Step 7  Loss: 0.3101116716861725\n",
      "Step 8  Loss: 0.4158598780632019\n",
      "Step 9  Loss: 0.4471517503261566\n",
      "Step 10  Loss: 0.3976125717163086\n",
      "total Loss of epoch  260  is  4.359603941440582\n",
      "Step 0  Loss: 0.39474934339523315\n",
      "Step 1  Loss: 0.28138038516044617\n",
      "Step 2  Loss: 0.36476266384124756\n",
      "Step 3  Loss: 0.33360567688941956\n",
      "Step 4  Loss: 0.3962180018424988\n",
      "Step 5  Loss: 0.35599127411842346\n",
      "Step 6  Loss: 0.3632044792175293\n",
      "Step 7  Loss: 0.34934377670288086\n",
      "Step 8  Loss: 0.31060168147087097\n",
      "Step 9  Loss: 0.41649553179740906\n",
      "Step 10  Loss: 0.45486071705818176\n",
      "total Loss of epoch  261  is  4.021213531494141\n",
      "Step 0  Loss: 0.39821770787239075\n",
      "Step 1  Loss: 0.45372122526168823\n",
      "Step 2  Loss: 0.3133394420146942\n",
      "Step 3  Loss: 0.4644690752029419\n",
      "Step 4  Loss: 0.22001004219055176\n",
      "Step 5  Loss: 0.3862505257129669\n",
      "Step 6  Loss: 0.46954718232154846\n",
      "Step 7  Loss: 0.43388232588768005\n",
      "Step 8  Loss: 0.4674452841281891\n",
      "Step 9  Loss: 0.330916166305542\n",
      "Step 10  Loss: 0.22384443879127502\n",
      "total Loss of epoch  262  is  4.161643415689468\n",
      "Step 0  Loss: 0.47236406803131104\n",
      "Step 1  Loss: 0.3379000425338745\n",
      "Step 2  Loss: 0.4055185616016388\n",
      "Step 3  Loss: 0.337714284658432\n",
      "Step 4  Loss: 0.35433217883110046\n",
      "Step 5  Loss: 0.5104677081108093\n",
      "Step 6  Loss: 0.4680999517440796\n",
      "Step 7  Loss: 0.3508812487125397\n",
      "Step 8  Loss: 0.5023671388626099\n",
      "Step 9  Loss: 0.2825353741645813\n",
      "Step 10  Loss: 0.27906399965286255\n",
      "total Loss of epoch  263  is  4.301244556903839\n",
      "Step 0  Loss: 0.38544541597366333\n",
      "Step 1  Loss: 0.3485882878303528\n",
      "Step 2  Loss: 0.31736916303634644\n",
      "Step 3  Loss: 0.39948853850364685\n",
      "Step 4  Loss: 0.3902623951435089\n",
      "Step 5  Loss: 0.4187563955783844\n",
      "Step 6  Loss: 0.28525054454803467\n",
      "Step 7  Loss: 0.44270840287208557\n",
      "Step 8  Loss: 0.36542442440986633\n",
      "Step 9  Loss: 0.3672739863395691\n",
      "Step 10  Loss: 0.46083372831344604\n",
      "total Loss of epoch  264  is  4.181401282548904\n",
      "Step 0  Loss: 0.3100030720233917\n",
      "Step 1  Loss: 0.3788634240627289\n",
      "Step 2  Loss: 0.22626356780529022\n",
      "Step 3  Loss: 0.5292758345603943\n",
      "Step 4  Loss: 0.4362954795360565\n",
      "Step 5  Loss: 0.3974234461784363\n",
      "Step 6  Loss: 0.3227671682834625\n",
      "Step 7  Loss: 0.3958800435066223\n",
      "Step 8  Loss: 0.42888930439949036\n",
      "Step 9  Loss: 0.3405349552631378\n",
      "Step 10  Loss: 0.2721123993396759\n",
      "total Loss of epoch  265  is  4.038308694958687\n",
      "Step 0  Loss: 0.23514588177204132\n",
      "Step 1  Loss: 0.27638840675354004\n",
      "Step 2  Loss: 0.339495986700058\n",
      "Step 3  Loss: 0.4123576283454895\n",
      "Step 4  Loss: 0.27588722109794617\n",
      "Step 5  Loss: 0.33022984862327576\n",
      "Step 6  Loss: 0.386971652507782\n",
      "Step 7  Loss: 0.38336268067359924\n",
      "Step 8  Loss: 0.3850596845149994\n",
      "Step 9  Loss: 0.4002569019794464\n",
      "Step 10  Loss: 0.4581736922264099\n",
      "total Loss of epoch  266  is  3.8833295851945877\n",
      "Step 0  Loss: 0.2717510461807251\n",
      "Step 1  Loss: 0.3804163634777069\n",
      "Step 2  Loss: 0.3089407980442047\n",
      "Step 3  Loss: 0.3783486783504486\n",
      "Step 4  Loss: 0.3367125391960144\n",
      "Step 5  Loss: 0.26816320419311523\n",
      "Step 6  Loss: 0.3817845582962036\n",
      "Step 7  Loss: 0.3258563280105591\n",
      "Step 8  Loss: 0.39174216985702515\n",
      "Step 9  Loss: 0.2972407937049866\n",
      "Step 10  Loss: 0.33852696418762207\n",
      "total Loss of epoch  267  is  3.6794834434986115\n",
      "Step 0  Loss: 0.31793177127838135\n",
      "Step 1  Loss: 0.408466100692749\n",
      "Step 2  Loss: 0.29913389682769775\n",
      "Step 3  Loss: 0.27294862270355225\n",
      "Step 4  Loss: 0.3370022773742676\n",
      "Step 5  Loss: 0.29466357827186584\n",
      "Step 6  Loss: 0.29028427600860596\n",
      "Step 7  Loss: 0.3906889855861664\n",
      "Step 8  Loss: 0.31736573576927185\n",
      "Step 9  Loss: 0.28090086579322815\n",
      "Step 10  Loss: 0.3898138701915741\n",
      "total Loss of epoch  268  is  3.5991999804973602\n",
      "Step 0  Loss: 0.44255590438842773\n",
      "Step 1  Loss: 0.3679029643535614\n",
      "Step 2  Loss: 0.4287976324558258\n",
      "Step 3  Loss: 0.2782308757305145\n",
      "Step 4  Loss: 0.24389663338661194\n",
      "Step 5  Loss: 0.37866780161857605\n",
      "Step 6  Loss: 0.4186958074569702\n",
      "Step 7  Loss: 0.42752310633659363\n",
      "Step 8  Loss: 0.3429047763347626\n",
      "Step 9  Loss: 0.45776432752609253\n",
      "Step 10  Loss: 0.2449381798505783\n",
      "total Loss of epoch  269  is  4.031878009438515\n",
      "Step 0  Loss: 0.3104105591773987\n",
      "Step 1  Loss: 0.2903633415699005\n",
      "Step 2  Loss: 0.45460057258605957\n",
      "Step 3  Loss: 0.4381783902645111\n",
      "Step 4  Loss: 0.307081013917923\n",
      "Step 5  Loss: 0.24678830802440643\n",
      "Step 6  Loss: 0.324237585067749\n",
      "Step 7  Loss: 0.4072151184082031\n",
      "Step 8  Loss: 0.3410448133945465\n",
      "Step 9  Loss: 0.4659491777420044\n",
      "Step 10  Loss: 0.5233051180839539\n",
      "total Loss of epoch  270  is  4.109173998236656\n",
      "Step 0  Loss: 0.3640431761741638\n",
      "Step 1  Loss: 0.3570380210876465\n",
      "Step 2  Loss: 0.37976330518722534\n",
      "Step 3  Loss: 0.29717984795570374\n",
      "Step 4  Loss: 0.39028337597846985\n",
      "Step 5  Loss: 0.44983136653900146\n",
      "Step 6  Loss: 0.39192381501197815\n",
      "Step 7  Loss: 0.32598960399627686\n",
      "Step 8  Loss: 0.3988717496395111\n",
      "Step 9  Loss: 0.46764880418777466\n",
      "Step 10  Loss: 0.5652921199798584\n",
      "total Loss of epoch  271  is  4.38786518573761\n",
      "Step 0  Loss: 0.41030585765838623\n",
      "Step 1  Loss: 0.36086493730545044\n",
      "Step 2  Loss: 0.3278731107711792\n",
      "Step 3  Loss: 0.35667744278907776\n",
      "Step 4  Loss: 0.3461654484272003\n",
      "Step 5  Loss: 0.28227511048316956\n",
      "Step 6  Loss: 0.39058104157447815\n",
      "Step 7  Loss: 0.3623007833957672\n",
      "Step 8  Loss: 0.4801784157752991\n",
      "Step 9  Loss: 0.27281415462493896\n",
      "Step 10  Loss: 0.34868013858795166\n",
      "total Loss of epoch  272  is  3.9387164413928986\n",
      "Step 0  Loss: 0.27413010597229004\n",
      "Step 1  Loss: 0.3803786039352417\n",
      "Step 2  Loss: 0.3185500204563141\n",
      "Step 3  Loss: 0.37297409772872925\n",
      "Step 4  Loss: 0.42569226026535034\n",
      "Step 5  Loss: 0.3191184997558594\n",
      "Step 6  Loss: 0.3235453963279724\n",
      "Step 7  Loss: 0.4251657724380493\n",
      "Step 8  Loss: 0.3729618787765503\n",
      "Step 9  Loss: 0.31257662177085876\n",
      "Step 10  Loss: 0.40946322679519653\n",
      "total Loss of epoch  273  is  3.934556484222412\n",
      "Step 0  Loss: 0.4119413197040558\n",
      "Step 1  Loss: 0.402435839176178\n",
      "Step 2  Loss: 0.4091486930847168\n",
      "Step 3  Loss: 0.3412989675998688\n",
      "Step 4  Loss: 0.32114458084106445\n",
      "Step 5  Loss: 0.34331995248794556\n",
      "Step 6  Loss: 0.31374621391296387\n",
      "Step 7  Loss: 0.43757325410842896\n",
      "Step 8  Loss: 0.32059380412101746\n",
      "Step 9  Loss: 0.32960647344589233\n",
      "Step 10  Loss: 0.4010730981826782\n",
      "total Loss of epoch  274  is  4.03188219666481\n",
      "Step 0  Loss: 0.42869335412979126\n",
      "Step 1  Loss: 0.27865704894065857\n",
      "Step 2  Loss: 0.32653874158859253\n",
      "Step 3  Loss: 0.34830242395401\n",
      "Step 4  Loss: 0.4825417399406433\n",
      "Step 5  Loss: 0.48719462752342224\n",
      "Step 6  Loss: 0.32906627655029297\n",
      "Step 7  Loss: 0.2752801775932312\n",
      "Step 8  Loss: 0.3565603792667389\n",
      "Step 9  Loss: 0.35956352949142456\n",
      "Step 10  Loss: 0.46735721826553345\n",
      "total Loss of epoch  275  is  4.139755517244339\n",
      "Step 0  Loss: 0.4818948805332184\n",
      "Step 1  Loss: 0.4215160012245178\n",
      "Step 2  Loss: 0.32820481061935425\n",
      "Step 3  Loss: 0.32146984338760376\n",
      "Step 4  Loss: 0.28609326481819153\n",
      "Step 5  Loss: 0.34354329109191895\n",
      "Step 6  Loss: 0.30920931696891785\n",
      "Step 7  Loss: 0.3944591283798218\n",
      "Step 8  Loss: 0.3430713713169098\n",
      "Step 9  Loss: 0.2781476080417633\n",
      "Step 10  Loss: 0.3948754668235779\n",
      "total Loss of epoch  276  is  3.9024849832057953\n",
      "Step 0  Loss: 0.32746508717536926\n",
      "Step 1  Loss: 0.4286583364009857\n",
      "Step 2  Loss: 0.3430618345737457\n",
      "Step 3  Loss: 0.3743298649787903\n",
      "Step 4  Loss: 0.41965988278388977\n",
      "Step 5  Loss: 0.3345082700252533\n",
      "Step 6  Loss: 0.36462101340293884\n",
      "Step 7  Loss: 0.38555029034614563\n",
      "Step 8  Loss: 0.35967370867729187\n",
      "Step 9  Loss: 0.5066530108451843\n",
      "Step 10  Loss: 0.41099444031715393\n",
      "total Loss of epoch  277  is  4.255175739526749\n",
      "Step 0  Loss: 0.3364339768886566\n",
      "Step 1  Loss: 0.2905934453010559\n",
      "Step 2  Loss: 0.31359627842903137\n",
      "Step 3  Loss: 0.35645753145217896\n",
      "Step 4  Loss: 0.41013455390930176\n",
      "Step 5  Loss: 0.3086532950401306\n",
      "Step 6  Loss: 0.30857181549072266\n",
      "Step 7  Loss: 0.3974550664424896\n",
      "Step 8  Loss: 0.5240609645843506\n",
      "Step 9  Loss: 0.4150824546813965\n",
      "Step 10  Loss: 0.32125720381736755\n",
      "total Loss of epoch  278  is  3.982296586036682\n",
      "Step 0  Loss: 0.34191641211509705\n",
      "Step 1  Loss: 0.2250298708677292\n",
      "Step 2  Loss: 0.37514886260032654\n",
      "Step 3  Loss: 0.41398191452026367\n",
      "Step 4  Loss: 0.4334196150302887\n",
      "Step 5  Loss: 0.34541308879852295\n",
      "Step 6  Loss: 0.3943485617637634\n",
      "Step 7  Loss: 0.2943609654903412\n",
      "Step 8  Loss: 0.31177034974098206\n",
      "Step 9  Loss: 0.2719733715057373\n",
      "Step 10  Loss: 0.34523239731788635\n",
      "total Loss of epoch  279  is  3.7525954097509384\n",
      "Step 0  Loss: 0.24068409204483032\n",
      "Step 1  Loss: 0.3118916451931\n",
      "Step 2  Loss: 0.4932158291339874\n",
      "Step 3  Loss: 0.31254807114601135\n",
      "Step 4  Loss: 0.40366047620773315\n",
      "Step 5  Loss: 0.4109348654747009\n",
      "Step 6  Loss: 0.2617792785167694\n",
      "Step 7  Loss: 0.30384203791618347\n",
      "Step 8  Loss: 0.2608591914176941\n",
      "Step 9  Loss: 0.4117589294910431\n",
      "Step 10  Loss: 0.47313687205314636\n",
      "total Loss of epoch  280  is  3.8843112885951996\n",
      "Step 0  Loss: 0.3046042323112488\n",
      "Step 1  Loss: 0.430066853761673\n",
      "Step 2  Loss: 0.40281781554222107\n",
      "Step 3  Loss: 0.34706932306289673\n",
      "Step 4  Loss: 0.2943762242794037\n",
      "Step 5  Loss: 0.5143065452575684\n",
      "Step 6  Loss: 0.30088183283805847\n",
      "Step 7  Loss: 0.33653467893600464\n",
      "Step 8  Loss: 0.2505549490451813\n",
      "Step 9  Loss: 0.27843865752220154\n",
      "Step 10  Loss: 0.3293415904045105\n",
      "total Loss of epoch  281  is  3.788992702960968\n",
      "Step 0  Loss: 0.34124669432640076\n",
      "Step 1  Loss: 0.3241662383079529\n",
      "Step 2  Loss: 0.38786476850509644\n",
      "Step 3  Loss: 0.3700118660926819\n",
      "Step 4  Loss: 0.2842867970466614\n",
      "Step 5  Loss: 0.2866257429122925\n",
      "Step 6  Loss: 0.33076339960098267\n",
      "Step 7  Loss: 0.3875485360622406\n",
      "Step 8  Loss: 0.3673192262649536\n",
      "Step 9  Loss: 0.398167222738266\n",
      "Step 10  Loss: 0.28040221333503723\n",
      "total Loss of epoch  282  is  3.758402705192566\n",
      "Step 0  Loss: 0.24238321185112\n",
      "Step 1  Loss: 0.3900684416294098\n",
      "Step 2  Loss: 0.2648085355758667\n",
      "Step 3  Loss: 0.36807742714881897\n",
      "Step 4  Loss: 0.3024559020996094\n",
      "Step 5  Loss: 0.31153246760368347\n",
      "Step 6  Loss: 0.32768383622169495\n",
      "Step 7  Loss: 0.35115376114845276\n",
      "Step 8  Loss: 0.36444807052612305\n",
      "Step 9  Loss: 0.33401593565940857\n",
      "Step 10  Loss: 0.2743415832519531\n",
      "total Loss of epoch  283  is  3.5309691727161407\n",
      "Step 0  Loss: 0.32629889249801636\n",
      "Step 1  Loss: 0.43487364053726196\n",
      "Step 2  Loss: 0.2930210530757904\n",
      "Step 3  Loss: 0.42153477668762207\n",
      "Step 4  Loss: 0.4479529857635498\n",
      "Step 5  Loss: 0.423178106546402\n",
      "Step 6  Loss: 0.32481494545936584\n",
      "Step 7  Loss: 0.33820316195487976\n",
      "Step 8  Loss: 0.3622068762779236\n",
      "Step 9  Loss: 0.33954745531082153\n",
      "Step 10  Loss: 0.47900354862213135\n",
      "total Loss of epoch  284  is  4.190635442733765\n",
      "Step 0  Loss: 0.3406885266304016\n",
      "Step 1  Loss: 0.3530856668949127\n",
      "Step 2  Loss: 0.29018864035606384\n",
      "Step 3  Loss: 0.45846402645111084\n",
      "Step 4  Loss: 0.3742249608039856\n",
      "Step 5  Loss: 0.27837613224983215\n",
      "Step 6  Loss: 0.334982305765152\n",
      "Step 7  Loss: 0.31709808111190796\n",
      "Step 8  Loss: 0.3638029992580414\n",
      "Step 9  Loss: 0.3033175468444824\n",
      "Step 10  Loss: 0.4952053129673004\n",
      "total Loss of epoch  285  is  3.909434199333191\n",
      "Step 0  Loss: 0.3490474224090576\n",
      "Step 1  Loss: 0.39339324831962585\n",
      "Step 2  Loss: 0.2687661051750183\n",
      "Step 3  Loss: 0.3038733899593353\n",
      "Step 4  Loss: 0.34328532218933105\n",
      "Step 5  Loss: 0.2983747124671936\n",
      "Step 6  Loss: 0.39971867203712463\n",
      "Step 7  Loss: 0.4091443419456482\n",
      "Step 8  Loss: 0.28539249300956726\n",
      "Step 9  Loss: 0.3592107594013214\n",
      "Step 10  Loss: 0.32859858870506287\n",
      "total Loss of epoch  286  is  3.738805055618286\n",
      "Step 0  Loss: 0.31047847867012024\n",
      "Step 1  Loss: 0.35692891478538513\n",
      "Step 2  Loss: 0.33076104521751404\n",
      "Step 3  Loss: 0.3811526596546173\n",
      "Step 4  Loss: 0.37211617827415466\n",
      "Step 5  Loss: 0.40465447306632996\n",
      "Step 6  Loss: 0.4091278314590454\n",
      "Step 7  Loss: 0.31953778862953186\n",
      "Step 8  Loss: 0.3353874087333679\n",
      "Step 9  Loss: 0.4147375226020813\n",
      "Step 10  Loss: 0.48275190591812134\n",
      "total Loss of epoch  287  is  4.117634207010269\n",
      "Step 0  Loss: 0.43485599756240845\n",
      "Step 1  Loss: 0.24998533725738525\n",
      "Step 2  Loss: 0.4308367669582367\n",
      "Step 3  Loss: 0.3192557692527771\n",
      "Step 4  Loss: 0.3658801317214966\n",
      "Step 5  Loss: 0.28522881865501404\n",
      "Step 6  Loss: 0.41769030690193176\n",
      "Step 7  Loss: 0.35924044251441956\n",
      "Step 8  Loss: 0.28350621461868286\n",
      "Step 9  Loss: 0.2911495268344879\n",
      "Step 10  Loss: 0.35331106185913086\n",
      "total Loss of epoch  288  is  3.790940374135971\n",
      "Step 0  Loss: 0.36133384704589844\n",
      "Step 1  Loss: 0.3409373462200165\n",
      "Step 2  Loss: 0.33297309279441833\n",
      "Step 3  Loss: 0.36479586362838745\n",
      "Step 4  Loss: 0.3375033736228943\n",
      "Step 5  Loss: 0.3172127604484558\n",
      "Step 6  Loss: 0.33641573786735535\n",
      "Step 7  Loss: 0.25879040360450745\n",
      "Step 8  Loss: 0.36221805214881897\n",
      "Step 9  Loss: 0.4240543842315674\n",
      "Step 10  Loss: 0.350970596075058\n",
      "total Loss of epoch  289  is  3.787205457687378\n",
      "Step 0  Loss: 0.36072778701782227\n",
      "Step 1  Loss: 0.25924184918403625\n",
      "Step 2  Loss: 0.3536829948425293\n",
      "Step 3  Loss: 0.3319963216781616\n",
      "Step 4  Loss: 0.3057774007320404\n",
      "Step 5  Loss: 0.33025190234184265\n",
      "Step 6  Loss: 0.37124738097190857\n",
      "Step 7  Loss: 0.4325522780418396\n",
      "Step 8  Loss: 0.3077346384525299\n",
      "Step 9  Loss: 0.33174929022789\n",
      "Step 10  Loss: 0.36441826820373535\n",
      "total Loss of epoch  290  is  3.749380111694336\n",
      "Step 0  Loss: 0.4091000556945801\n",
      "Step 1  Loss: 0.4293409287929535\n",
      "Step 2  Loss: 0.4199134111404419\n",
      "Step 3  Loss: 0.3276463747024536\n",
      "Step 4  Loss: 0.3389192819595337\n",
      "Step 5  Loss: 0.3746424913406372\n",
      "Step 6  Loss: 0.3897399306297302\n",
      "Step 7  Loss: 0.2574830651283264\n",
      "Step 8  Loss: 0.2911122143268585\n",
      "Step 9  Loss: 0.27362051606178284\n",
      "Step 10  Loss: 0.41192254424095154\n",
      "total Loss of epoch  291  is  3.9234408140182495\n",
      "Step 0  Loss: 0.4238201379776001\n",
      "Step 1  Loss: 0.33487382531166077\n",
      "Step 2  Loss: 0.3089371919631958\n",
      "Step 3  Loss: 0.40917426347732544\n",
      "Step 4  Loss: 0.3826071619987488\n",
      "Step 5  Loss: 0.34138229489326477\n",
      "Step 6  Loss: 0.36209821701049805\n",
      "Step 7  Loss: 0.47206997871398926\n",
      "Step 8  Loss: 0.3368909955024719\n",
      "Step 9  Loss: 0.4611460268497467\n",
      "Step 10  Loss: 0.5110136866569519\n",
      "total Loss of epoch  292  is  4.3440137803554535\n",
      "Step 0  Loss: 0.3894954323768616\n",
      "Step 1  Loss: 0.5286431312561035\n",
      "Step 2  Loss: 0.32223907113075256\n",
      "Step 3  Loss: 0.3996070623397827\n",
      "Step 4  Loss: 0.2732146680355072\n",
      "Step 5  Loss: 0.26842644810676575\n",
      "Step 6  Loss: 0.4060440957546234\n",
      "Step 7  Loss: 0.3121330440044403\n",
      "Step 8  Loss: 0.437549889087677\n",
      "Step 9  Loss: 0.24517229199409485\n",
      "Step 10  Loss: 0.3160112500190735\n",
      "total Loss of epoch  293  is  3.8985363841056824\n",
      "Step 0  Loss: 0.3482512831687927\n",
      "Step 1  Loss: 0.4137830436229706\n",
      "Step 2  Loss: 0.3016895651817322\n",
      "Step 3  Loss: 0.4152304232120514\n",
      "Step 4  Loss: 0.23372310400009155\n",
      "Step 5  Loss: 0.29437071084976196\n",
      "Step 6  Loss: 0.29392385482788086\n",
      "Step 7  Loss: 0.31391072273254395\n",
      "Step 8  Loss: 0.36249926686286926\n",
      "Step 9  Loss: 0.33845585584640503\n",
      "Step 10  Loss: 0.36429643630981445\n",
      "total Loss of epoch  294  is  3.680134266614914\n",
      "Step 0  Loss: 0.3741159439086914\n",
      "Step 1  Loss: 0.2252195179462433\n",
      "Step 2  Loss: 0.3502601385116577\n",
      "Step 3  Loss: 0.36473187804222107\n",
      "Step 4  Loss: 0.32998916506767273\n",
      "Step 5  Loss: 0.3849351704120636\n",
      "Step 6  Loss: 0.21438708901405334\n",
      "Step 7  Loss: 0.30442678928375244\n",
      "Step 8  Loss: 0.25966840982437134\n",
      "Step 9  Loss: 0.3695567846298218\n",
      "Step 10  Loss: 0.538931131362915\n",
      "total Loss of epoch  295  is  3.7162220180034637\n",
      "Step 0  Loss: 0.33260607719421387\n",
      "Step 1  Loss: 0.31515973806381226\n",
      "Step 2  Loss: 0.4071127772331238\n",
      "Step 3  Loss: 0.2926265001296997\n",
      "Step 4  Loss: 0.4473501741886139\n",
      "Step 5  Loss: 0.3095986247062683\n",
      "Step 6  Loss: 0.36830806732177734\n",
      "Step 7  Loss: 0.28857576847076416\n",
      "Step 8  Loss: 0.37034833431243896\n",
      "Step 9  Loss: 0.28379014134407043\n",
      "Step 10  Loss: 0.35159048438072205\n",
      "total Loss of epoch  296  is  3.7670666873455048\n",
      "Step 0  Loss: 0.25435692071914673\n",
      "Step 1  Loss: 0.3744278848171234\n",
      "Step 2  Loss: 0.3484622836112976\n",
      "Step 3  Loss: 0.3493621051311493\n",
      "Step 4  Loss: 0.42561325430870056\n",
      "Step 5  Loss: 0.30154427886009216\n",
      "Step 6  Loss: 0.4338567554950714\n",
      "Step 7  Loss: 0.31472960114479065\n",
      "Step 8  Loss: 0.3130703866481781\n",
      "Step 9  Loss: 0.4550260603427887\n",
      "Step 10  Loss: 0.32007330656051636\n",
      "total Loss of epoch  297  is  3.890522837638855\n",
      "Step 0  Loss: 0.3432140350341797\n",
      "Step 1  Loss: 0.4173683524131775\n",
      "Step 2  Loss: 0.3482569456100464\n",
      "Step 3  Loss: 0.3289824426174164\n",
      "Step 4  Loss: 0.38751524686813354\n",
      "Step 5  Loss: 0.33034101128578186\n",
      "Step 6  Loss: 0.454059898853302\n",
      "Step 7  Loss: 0.29576146602630615\n",
      "Step 8  Loss: 0.251475989818573\n",
      "Step 9  Loss: 0.34975776076316833\n",
      "Step 10  Loss: 0.256395161151886\n",
      "total Loss of epoch  298  is  3.763128310441971\n",
      "Step 0  Loss: 0.42587709426879883\n",
      "Step 1  Loss: 0.4208197593688965\n",
      "Step 2  Loss: 0.46190953254699707\n",
      "Step 3  Loss: 0.3202051818370819\n",
      "Step 4  Loss: 0.3735617995262146\n",
      "Step 5  Loss: 0.3037278354167938\n",
      "Step 6  Loss: 0.2611786723136902\n",
      "Step 7  Loss: 0.39208918809890747\n",
      "Step 8  Loss: 0.3546378016471863\n",
      "Step 9  Loss: 0.3293427526950836\n",
      "Step 10  Loss: 0.3785524368286133\n",
      "total Loss of epoch  299  is  4.0219020545482635\n",
      "Step 0  Loss: 0.3665941655635834\n",
      "Step 1  Loss: 0.33038026094436646\n",
      "Step 2  Loss: 0.32017049193382263\n",
      "Step 3  Loss: 0.42616477608680725\n",
      "Step 4  Loss: 0.30626899003982544\n",
      "Step 5  Loss: 0.36007586121559143\n",
      "Step 6  Loss: 0.19857746362686157\n",
      "Step 7  Loss: 0.3707684874534607\n",
      "Step 8  Loss: 0.3898402154445648\n",
      "Step 9  Loss: 0.30711695551872253\n",
      "Step 10  Loss: 0.2690422832965851\n",
      "total Loss of epoch  300  is  3.6449999511241913\n",
      "Step 0  Loss: 0.4481593072414398\n",
      "Step 1  Loss: 0.30879056453704834\n",
      "Step 2  Loss: 0.41375747323036194\n",
      "Step 3  Loss: 0.3877260088920593\n",
      "Step 4  Loss: 0.31385594606399536\n",
      "Step 5  Loss: 0.32324159145355225\n",
      "Step 6  Loss: 0.33693045377731323\n",
      "Step 7  Loss: 0.27723294496536255\n",
      "Step 8  Loss: 0.37279489636421204\n",
      "Step 9  Loss: 0.18285509943962097\n",
      "Step 10  Loss: 0.38399389386177063\n",
      "total Loss of epoch  301  is  3.7493381798267365\n",
      "Step 0  Loss: 0.27811843156814575\n",
      "Step 1  Loss: 0.35234975814819336\n",
      "Step 2  Loss: 0.3980562090873718\n",
      "Step 3  Loss: 0.3837261199951172\n",
      "Step 4  Loss: 0.27133119106292725\n",
      "Step 5  Loss: 0.4130121171474457\n",
      "Step 6  Loss: 0.39475128054618835\n",
      "Step 7  Loss: 0.3344995677471161\n",
      "Step 8  Loss: 0.4456595778465271\n",
      "Step 9  Loss: 0.32384324073791504\n",
      "Step 10  Loss: 0.26247042417526245\n",
      "total Loss of epoch  302  is  3.85781791806221\n",
      "Step 0  Loss: 0.3109736144542694\n",
      "Step 1  Loss: 0.34436118602752686\n",
      "Step 2  Loss: 0.43021219968795776\n",
      "Step 3  Loss: 0.2827497720718384\n",
      "Step 4  Loss: 0.3097170293331146\n",
      "Step 5  Loss: 0.40074339509010315\n",
      "Step 6  Loss: 0.33956313133239746\n",
      "Step 7  Loss: 0.31343114376068115\n",
      "Step 8  Loss: 0.2973534166812897\n",
      "Step 9  Loss: 0.30039775371551514\n",
      "Step 10  Loss: 0.23889745771884918\n",
      "total Loss of epoch  303  is  3.568400099873543\n",
      "Step 0  Loss: 0.2901792526245117\n",
      "Step 1  Loss: 0.4321255385875702\n",
      "Step 2  Loss: 0.4111815392971039\n",
      "Step 3  Loss: 0.26802772283554077\n",
      "Step 4  Loss: 0.32674887776374817\n",
      "Step 5  Loss: 0.4175730049610138\n",
      "Step 6  Loss: 0.4113529324531555\n",
      "Step 7  Loss: 0.30226367712020874\n",
      "Step 8  Loss: 0.2818337380886078\n",
      "Step 9  Loss: 0.35584771633148193\n",
      "Step 10  Loss: 0.23680782318115234\n",
      "total Loss of epoch  304  is  3.733941823244095\n",
      "Step 0  Loss: 0.25911349058151245\n",
      "Step 1  Loss: 0.31699857115745544\n",
      "Step 2  Loss: 0.4446103274822235\n",
      "Step 3  Loss: 0.42710795998573303\n",
      "Step 4  Loss: 0.3424682021141052\n",
      "Step 5  Loss: 0.41863468289375305\n",
      "Step 6  Loss: 0.3005690276622772\n",
      "Step 7  Loss: 0.2878047823905945\n",
      "Step 8  Loss: 0.25122153759002686\n",
      "Step 9  Loss: 0.4022704064846039\n",
      "Step 10  Loss: 0.2528587281703949\n",
      "total Loss of epoch  305  is  3.70365771651268\n",
      "Step 0  Loss: 0.3830606937408447\n",
      "Step 1  Loss: 0.29776468873023987\n",
      "Step 2  Loss: 0.4726950526237488\n",
      "Step 3  Loss: 0.34303948283195496\n",
      "Step 4  Loss: 0.286773681640625\n",
      "Step 5  Loss: 0.3306052088737488\n",
      "Step 6  Loss: 0.4630744457244873\n",
      "Step 7  Loss: 0.24860845506191254\n",
      "Step 8  Loss: 0.32477590441703796\n",
      "Step 9  Loss: 0.35164332389831543\n",
      "Step 10  Loss: 0.4862632751464844\n",
      "total Loss of epoch  306  is  3.9883042126893997\n",
      "Step 0  Loss: 0.33661365509033203\n",
      "Step 1  Loss: 0.29665741324424744\n",
      "Step 2  Loss: 0.3018282651901245\n",
      "Step 3  Loss: 0.2847086787223816\n",
      "Step 4  Loss: 0.3212891221046448\n",
      "Step 5  Loss: 0.3929285407066345\n",
      "Step 6  Loss: 0.2894143760204315\n",
      "Step 7  Loss: 0.37945860624313354\n",
      "Step 8  Loss: 0.26524409651756287\n",
      "Step 9  Loss: 0.2559514045715332\n",
      "Step 10  Loss: 0.2946343719959259\n",
      "total Loss of epoch  307  is  3.418728530406952\n",
      "Step 0  Loss: 0.4466933608055115\n",
      "Step 1  Loss: 0.385612428188324\n",
      "Step 2  Loss: 0.42743173241615295\n",
      "Step 3  Loss: 0.3221902549266815\n",
      "Step 4  Loss: 0.45476433634757996\n",
      "Step 5  Loss: 0.2577095627784729\n",
      "Step 6  Loss: 0.4085443317890167\n",
      "Step 7  Loss: 0.39805665612220764\n",
      "Step 8  Loss: 0.3407883644104004\n",
      "Step 9  Loss: 0.35846251249313354\n",
      "Step 10  Loss: 0.48122262954711914\n",
      "total Loss of epoch  308  is  4.2814761698246\n",
      "Step 0  Loss: 0.28799307346343994\n",
      "Step 1  Loss: 0.3672003448009491\n",
      "Step 2  Loss: 0.41286537051200867\n",
      "Step 3  Loss: 0.35044166445732117\n",
      "Step 4  Loss: 0.42228177189826965\n",
      "Step 5  Loss: 0.3643481135368347\n",
      "Step 6  Loss: 0.30963942408561707\n",
      "Step 7  Loss: 0.28075018525123596\n",
      "Step 8  Loss: 0.4264911711215973\n",
      "Step 9  Loss: 0.35293808579444885\n",
      "Step 10  Loss: 0.1454823613166809\n",
      "total Loss of epoch  309  is  3.7204315662384033\n",
      "Step 0  Loss: 0.40379616618156433\n",
      "Step 1  Loss: 0.2619059681892395\n",
      "Step 2  Loss: 0.42927446961402893\n",
      "Step 3  Loss: 0.35255154967308044\n",
      "Step 4  Loss: 0.3161691129207611\n",
      "Step 5  Loss: 0.33396482467651367\n",
      "Step 6  Loss: 0.4304676949977875\n",
      "Step 7  Loss: 0.3371809422969818\n",
      "Step 8  Loss: 0.4831727147102356\n",
      "Step 9  Loss: 0.3870490491390228\n",
      "Step 10  Loss: 0.22968336939811707\n",
      "total Loss of epoch  310  is  3.9652158617973328\n",
      "Step 0  Loss: 0.35333964228630066\n",
      "Step 1  Loss: 0.39483386278152466\n",
      "Step 2  Loss: 0.32980185747146606\n",
      "Step 3  Loss: 0.34030845761299133\n",
      "Step 4  Loss: 0.3421691358089447\n",
      "Step 5  Loss: 0.24894662201404572\n",
      "Step 6  Loss: 0.3993436098098755\n",
      "Step 7  Loss: 0.2890336215496063\n",
      "Step 8  Loss: 0.26752325892448425\n",
      "Step 9  Loss: 0.46110832691192627\n",
      "Step 10  Loss: 0.3047471344470978\n",
      "total Loss of epoch  311  is  3.7311555296182632\n",
      "Step 0  Loss: 0.29833656549453735\n",
      "Step 1  Loss: 0.4385775625705719\n",
      "Step 2  Loss: 0.33889687061309814\n",
      "Step 3  Loss: 0.3350188136100769\n",
      "Step 4  Loss: 0.2865990698337555\n",
      "Step 5  Loss: 0.35353153944015503\n",
      "Step 6  Loss: 0.36415231227874756\n",
      "Step 7  Loss: 0.36746203899383545\n",
      "Step 8  Loss: 0.4935949742794037\n",
      "Step 9  Loss: 0.3180033564567566\n",
      "Step 10  Loss: 0.22554995119571686\n",
      "total Loss of epoch  312  is  3.819723054766655\n",
      "Step 0  Loss: 0.38358911871910095\n",
      "Step 1  Loss: 0.3674820363521576\n",
      "Step 2  Loss: 0.2811821401119232\n",
      "Step 3  Loss: 0.3762986958026886\n",
      "Step 4  Loss: 0.42601004242897034\n",
      "Step 5  Loss: 0.3167668282985687\n",
      "Step 6  Loss: 0.40206432342529297\n",
      "Step 7  Loss: 0.4895722568035126\n",
      "Step 8  Loss: 0.26207345724105835\n",
      "Step 9  Loss: 0.3158212900161743\n",
      "Step 10  Loss: 0.5378330945968628\n",
      "total Loss of epoch  313  is  4.15869328379631\n",
      "Step 0  Loss: 0.349325954914093\n",
      "Step 1  Loss: 0.38765382766723633\n",
      "Step 2  Loss: 0.2968302369117737\n",
      "Step 3  Loss: 0.38574695587158203\n",
      "Step 4  Loss: 0.2988339364528656\n",
      "Step 5  Loss: 0.30939462780952454\n",
      "Step 6  Loss: 0.27624228596687317\n",
      "Step 7  Loss: 0.3680448830127716\n",
      "Step 8  Loss: 0.31296902894973755\n",
      "Step 9  Loss: 0.46731194853782654\n",
      "Step 10  Loss: 0.2974514067173004\n",
      "total Loss of epoch  314  is  3.7498050928115845\n",
      "Step 0  Loss: 0.3940381705760956\n",
      "Step 1  Loss: 0.2644747197628021\n",
      "Step 2  Loss: 0.4253741204738617\n",
      "Step 3  Loss: 0.31788283586502075\n",
      "Step 4  Loss: 0.38074272871017456\n",
      "Step 5  Loss: 0.30119025707244873\n",
      "Step 6  Loss: 0.3658497929573059\n",
      "Step 7  Loss: 0.2796172499656677\n",
      "Step 8  Loss: 0.37048104405403137\n",
      "Step 9  Loss: 0.3523724675178528\n",
      "Step 10  Loss: 0.3171752095222473\n",
      "total Loss of epoch  315  is  3.7691985964775085\n",
      "Step 0  Loss: 0.39640575647354126\n",
      "Step 1  Loss: 0.3551411032676697\n",
      "Step 2  Loss: 0.2961190342903137\n",
      "Step 3  Loss: 0.3337646424770355\n",
      "Step 4  Loss: 0.46092653274536133\n",
      "Step 5  Loss: 0.40975403785705566\n",
      "Step 6  Loss: 0.33571672439575195\n",
      "Step 7  Loss: 0.36930131912231445\n",
      "Step 8  Loss: 0.40280044078826904\n",
      "Step 9  Loss: 0.41455528140068054\n",
      "Step 10  Loss: 0.5772913098335266\n",
      "total Loss of epoch  316  is  4.35177618265152\n",
      "Step 0  Loss: 0.29669103026390076\n",
      "Step 1  Loss: 0.4954475164413452\n",
      "Step 2  Loss: 0.43863445520401\n",
      "Step 3  Loss: 0.3887292444705963\n",
      "Step 4  Loss: 0.3197624385356903\n",
      "Step 5  Loss: 0.30375370383262634\n",
      "Step 6  Loss: 0.32726097106933594\n",
      "Step 7  Loss: 0.308031290769577\n",
      "Step 8  Loss: 0.41137996315956116\n",
      "Step 9  Loss: 0.39355117082595825\n",
      "Step 10  Loss: 0.2638438642024994\n",
      "total Loss of epoch  317  is  3.9470856487751007\n",
      "Step 0  Loss: 0.3098202347755432\n",
      "Step 1  Loss: 0.32839179039001465\n",
      "Step 2  Loss: 0.30130642652511597\n",
      "Step 3  Loss: 0.3773222863674164\n",
      "Step 4  Loss: 0.35159575939178467\n",
      "Step 5  Loss: 0.41550320386886597\n",
      "Step 6  Loss: 0.31249913573265076\n",
      "Step 7  Loss: 0.2941828668117523\n",
      "Step 8  Loss: 0.2185167372226715\n",
      "Step 9  Loss: 0.40072759985923767\n",
      "Step 10  Loss: 0.3052027225494385\n",
      "total Loss of epoch  318  is  3.6150687634944916\n",
      "Step 0  Loss: 0.35112181305885315\n",
      "Step 1  Loss: 0.4528340995311737\n",
      "Step 2  Loss: 0.35665789246559143\n",
      "Step 3  Loss: 0.35026803612709045\n",
      "Step 4  Loss: 0.37564224004745483\n",
      "Step 5  Loss: 0.40711694955825806\n",
      "Step 6  Loss: 0.36896371841430664\n",
      "Step 7  Loss: 0.23054218292236328\n",
      "Step 8  Loss: 0.2578379511833191\n",
      "Step 9  Loss: 0.31631651520729065\n",
      "Step 10  Loss: 0.2662787139415741\n",
      "total Loss of epoch  319  is  3.7335801124572754\n",
      "Step 0  Loss: 0.37050455808639526\n",
      "Step 1  Loss: 0.2160036712884903\n",
      "Step 2  Loss: 0.28133389353752136\n",
      "Step 3  Loss: 0.2822951674461365\n",
      "Step 4  Loss: 0.29306888580322266\n",
      "Step 5  Loss: 0.3518603444099426\n",
      "Step 6  Loss: 0.2753051221370697\n",
      "Step 7  Loss: 0.4075160622596741\n",
      "Step 8  Loss: 0.38442546129226685\n",
      "Step 9  Loss: 0.2747403681278229\n",
      "Step 10  Loss: 0.4316856861114502\n",
      "total Loss of epoch  320  is  3.5687392204999924\n",
      "Step 0  Loss: 0.29175806045532227\n",
      "Step 1  Loss: 0.41091203689575195\n",
      "Step 2  Loss: 0.3648863732814789\n",
      "Step 3  Loss: 0.37356090545654297\n",
      "Step 4  Loss: 0.4592495560646057\n",
      "Step 5  Loss: 0.30245450139045715\n",
      "Step 6  Loss: 0.37987348437309265\n",
      "Step 7  Loss: 0.446308970451355\n",
      "Step 8  Loss: 0.3264126181602478\n",
      "Step 9  Loss: 0.35648995637893677\n",
      "Step 10  Loss: 0.3675096929073334\n",
      "total Loss of epoch  321  is  4.0794161558151245\n",
      "Step 0  Loss: 0.29192817211151123\n",
      "Step 1  Loss: 0.39942026138305664\n",
      "Step 2  Loss: 0.37471726536750793\n",
      "Step 3  Loss: 0.33803948760032654\n",
      "Step 4  Loss: 0.420970618724823\n",
      "Step 5  Loss: 0.35658881068229675\n",
      "Step 6  Loss: 0.3678111135959625\n",
      "Step 7  Loss: 0.28148919343948364\n",
      "Step 8  Loss: 0.45299163460731506\n",
      "Step 9  Loss: 0.3973955810070038\n",
      "Step 10  Loss: 0.4193556606769562\n",
      "total Loss of epoch  322  is  4.100707799196243\n",
      "Step 0  Loss: 0.37252748012542725\n",
      "Step 1  Loss: 0.27875763177871704\n",
      "Step 2  Loss: 0.4156171977519989\n",
      "Step 3  Loss: 0.3120741844177246\n",
      "Step 4  Loss: 0.2651698887348175\n",
      "Step 5  Loss: 0.40939778089523315\n",
      "Step 6  Loss: 0.3651854693889618\n",
      "Step 7  Loss: 0.36719685792922974\n",
      "Step 8  Loss: 0.4323022961616516\n",
      "Step 9  Loss: 0.3886012136936188\n",
      "Step 10  Loss: 0.3992196321487427\n",
      "total Loss of epoch  323  is  4.006049633026123\n",
      "Step 0  Loss: 0.2677813172340393\n",
      "Step 1  Loss: 0.3681550920009613\n",
      "Step 2  Loss: 0.38321414589881897\n",
      "Step 3  Loss: 0.4391822814941406\n",
      "Step 4  Loss: 0.39322060346603394\n",
      "Step 5  Loss: 0.3885937035083771\n",
      "Step 6  Loss: 0.2888454496860504\n",
      "Step 7  Loss: 0.32292914390563965\n",
      "Step 8  Loss: 0.2992228865623474\n",
      "Step 9  Loss: 0.34032633900642395\n",
      "Step 10  Loss: 0.26396721601486206\n",
      "total Loss of epoch  324  is  3.7554381787776947\n",
      "Step 0  Loss: 0.2884134352207184\n",
      "Step 1  Loss: 0.2700878381729126\n",
      "Step 2  Loss: 0.276961088180542\n",
      "Step 3  Loss: 0.31078070402145386\n",
      "Step 4  Loss: 0.28983384370803833\n",
      "Step 5  Loss: 0.4054836630821228\n",
      "Step 6  Loss: 0.42789486050605774\n",
      "Step 7  Loss: 0.3249448835849762\n",
      "Step 8  Loss: 0.3648623526096344\n",
      "Step 9  Loss: 0.3397926092147827\n",
      "Step 10  Loss: 0.37148118019104004\n",
      "total Loss of epoch  325  is  3.670536458492279\n",
      "Step 0  Loss: 0.351521760225296\n",
      "Step 1  Loss: 0.3836474120616913\n",
      "Step 2  Loss: 0.37580615282058716\n",
      "Step 3  Loss: 0.3004992604255676\n",
      "Step 4  Loss: 0.39412200450897217\n",
      "Step 5  Loss: 0.3806490898132324\n",
      "Step 6  Loss: 0.3399052619934082\n",
      "Step 7  Loss: 0.3021031320095062\n",
      "Step 8  Loss: 0.4357838034629822\n",
      "Step 9  Loss: 0.40456607937812805\n",
      "Step 10  Loss: 0.24534432590007782\n",
      "total Loss of epoch  326  is  3.913948282599449\n",
      "Step 0  Loss: 0.33849450945854187\n",
      "Step 1  Loss: 0.2900436222553253\n",
      "Step 2  Loss: 0.3872223496437073\n",
      "Step 3  Loss: 0.2600736916065216\n",
      "Step 4  Loss: 0.3417227864265442\n",
      "Step 5  Loss: 0.3054632246494293\n",
      "Step 6  Loss: 0.47441884875297546\n",
      "Step 7  Loss: 0.3596031069755554\n",
      "Step 8  Loss: 0.40208518505096436\n",
      "Step 9  Loss: 0.2694879472255707\n",
      "Step 10  Loss: 0.4425381124019623\n",
      "total Loss of epoch  327  is  3.871153384447098\n",
      "Step 0  Loss: 0.37785035371780396\n",
      "Step 1  Loss: 0.43200311064720154\n",
      "Step 2  Loss: 0.3583250641822815\n",
      "Step 3  Loss: 0.28071337938308716\n",
      "Step 4  Loss: 0.35529106855392456\n",
      "Step 5  Loss: 0.3506630063056946\n",
      "Step 6  Loss: 0.26446300745010376\n",
      "Step 7  Loss: 0.3098681569099426\n",
      "Step 8  Loss: 0.3463030755519867\n",
      "Step 9  Loss: 0.42440980672836304\n",
      "Step 10  Loss: 0.2247738540172577\n",
      "total Loss of epoch  328  is  3.724663883447647\n",
      "Step 0  Loss: 0.29206109046936035\n",
      "Step 1  Loss: 0.38697537779808044\n",
      "Step 2  Loss: 0.25450968742370605\n",
      "Step 3  Loss: 0.468927800655365\n",
      "Step 4  Loss: 0.32395896315574646\n",
      "Step 5  Loss: 0.3702363073825836\n",
      "Step 6  Loss: 0.4123779237270355\n",
      "Step 7  Loss: 0.3050746023654938\n",
      "Step 8  Loss: 0.3358856439590454\n",
      "Step 9  Loss: 0.3889342248439789\n",
      "Step 10  Loss: 0.28441140055656433\n",
      "total Loss of epoch  329  is  3.82335302233696\n",
      "Step 0  Loss: 0.3350493013858795\n",
      "Step 1  Loss: 0.3884560763835907\n",
      "Step 2  Loss: 0.3402464985847473\n",
      "Step 3  Loss: 0.4007071852684021\n",
      "Step 4  Loss: 0.4161280691623688\n",
      "Step 5  Loss: 0.3826903700828552\n",
      "Step 6  Loss: 0.3502734303474426\n",
      "Step 7  Loss: 0.3537416160106659\n",
      "Step 8  Loss: 0.2548065483570099\n",
      "Step 9  Loss: 0.30033770203590393\n",
      "Step 10  Loss: 0.08694979548454285\n",
      "total Loss of epoch  330  is  3.609386593103409\n",
      "Step 0  Loss: 0.2866937220096588\n",
      "Step 1  Loss: 0.34448468685150146\n",
      "Step 2  Loss: 0.24976415932178497\n",
      "Step 3  Loss: 0.27779316902160645\n",
      "Step 4  Loss: 0.33906590938568115\n",
      "Step 5  Loss: 0.2960195243358612\n",
      "Step 6  Loss: 0.3498742878437042\n",
      "Step 7  Loss: 0.35836493968963623\n",
      "Step 8  Loss: 0.23325896263122559\n",
      "Step 9  Loss: 0.30822497606277466\n",
      "Step 10  Loss: 0.3199070692062378\n",
      "total Loss of epoch  331  is  3.3634514063596725\n",
      "Step 0  Loss: 0.34487611055374146\n",
      "Step 1  Loss: 0.3139632046222687\n",
      "Step 2  Loss: 0.3534228205680847\n",
      "Step 3  Loss: 0.25382375717163086\n",
      "Step 4  Loss: 0.4117162525653839\n",
      "Step 5  Loss: 0.36775556206703186\n",
      "Step 6  Loss: 0.43546923995018005\n",
      "Step 7  Loss: 0.29500237107276917\n",
      "Step 8  Loss: 0.3306713402271271\n",
      "Step 9  Loss: 0.4293826222419739\n",
      "Step 10  Loss: 0.32624325156211853\n",
      "total Loss of epoch  332  is  3.86232653260231\n",
      "Step 0  Loss: 0.3513256907463074\n",
      "Step 1  Loss: 0.378646582365036\n",
      "Step 2  Loss: 0.41607218980789185\n",
      "Step 3  Loss: 0.3286186456680298\n",
      "Step 4  Loss: 0.32755836844444275\n",
      "Step 5  Loss: 0.3415413498878479\n",
      "Step 6  Loss: 0.3788434565067291\n",
      "Step 7  Loss: 0.34851327538490295\n",
      "Step 8  Loss: 0.38060134649276733\n",
      "Step 9  Loss: 0.35932475328445435\n",
      "Step 10  Loss: 0.28773027658462524\n",
      "total Loss of epoch  333  is  3.8987759351730347\n",
      "Step 0  Loss: 0.30855855345726013\n",
      "Step 1  Loss: 0.354001522064209\n",
      "Step 2  Loss: 0.4234544634819031\n",
      "Step 3  Loss: 0.4067954123020172\n",
      "Step 4  Loss: 0.2640465497970581\n",
      "Step 5  Loss: 0.27634528279304504\n",
      "Step 6  Loss: 0.31639742851257324\n",
      "Step 7  Loss: 0.29344433546066284\n",
      "Step 8  Loss: 0.39652541279792786\n",
      "Step 9  Loss: 0.2907555103302002\n",
      "Step 10  Loss: 0.36523231863975525\n",
      "total Loss of epoch  334  is  3.695556789636612\n",
      "Step 0  Loss: 0.43808209896087646\n",
      "Step 1  Loss: 0.38933005928993225\n",
      "Step 2  Loss: 0.42379724979400635\n",
      "Step 3  Loss: 0.44154971837997437\n",
      "Step 4  Loss: 0.28818178176879883\n",
      "Step 5  Loss: 0.2759089469909668\n",
      "Step 6  Loss: 0.3278798460960388\n",
      "Step 7  Loss: 0.40153300762176514\n",
      "Step 8  Loss: 0.3248196542263031\n",
      "Step 9  Loss: 0.24532261490821838\n",
      "Step 10  Loss: 0.39194756746292114\n",
      "total Loss of epoch  335  is  3.9483525454998016\n",
      "Step 0  Loss: 0.3997751474380493\n",
      "Step 1  Loss: 0.4020777940750122\n",
      "Step 2  Loss: 0.3328646421432495\n",
      "Step 3  Loss: 0.43617022037506104\n",
      "Step 4  Loss: 0.3788636028766632\n",
      "Step 5  Loss: 0.38438481092453003\n",
      "Step 6  Loss: 0.30924326181411743\n",
      "Step 7  Loss: 0.31205058097839355\n",
      "Step 8  Loss: 0.35492733120918274\n",
      "Step 9  Loss: 0.36607062816619873\n",
      "Step 10  Loss: 0.46519234776496887\n",
      "total Loss of epoch  336  is  4.141620367765427\n",
      "Step 0  Loss: 0.46368303894996643\n",
      "Step 1  Loss: 0.3411482274532318\n",
      "Step 2  Loss: 0.35850340127944946\n",
      "Step 3  Loss: 0.4193425178527832\n",
      "Step 4  Loss: 0.3045431077480316\n",
      "Step 5  Loss: 0.3755863904953003\n",
      "Step 6  Loss: 0.4603758156299591\n",
      "Step 7  Loss: 0.3665783405303955\n",
      "Step 8  Loss: 0.19797249138355255\n",
      "Step 9  Loss: 0.2549028992652893\n",
      "Step 10  Loss: 0.43378204107284546\n",
      "total Loss of epoch  337  is  3.9764182716608047\n",
      "Step 0  Loss: 0.40119531750679016\n",
      "Step 1  Loss: 0.3670782148838043\n",
      "Step 2  Loss: 0.33008724451065063\n",
      "Step 3  Loss: 0.3188672363758087\n",
      "Step 4  Loss: 0.44035375118255615\n",
      "Step 5  Loss: 0.2988608479499817\n",
      "Step 6  Loss: 0.3571649491786957\n",
      "Step 7  Loss: 0.38174837827682495\n",
      "Step 8  Loss: 0.43856996297836304\n",
      "Step 9  Loss: 0.2812519371509552\n",
      "Step 10  Loss: 0.3726128041744232\n",
      "total Loss of epoch  338  is  3.9877906441688538\n",
      "Step 0  Loss: 0.45419126749038696\n",
      "Step 1  Loss: 0.28041285276412964\n",
      "Step 2  Loss: 0.26719582080841064\n",
      "Step 3  Loss: 0.2948939800262451\n",
      "Step 4  Loss: 0.3345358073711395\n",
      "Step 5  Loss: 0.37405768036842346\n",
      "Step 6  Loss: 0.4527727961540222\n",
      "Step 7  Loss: 0.5337674021720886\n",
      "Step 8  Loss: 0.3876115679740906\n",
      "Step 9  Loss: 0.301643043756485\n",
      "Step 10  Loss: 0.4177146852016449\n",
      "total Loss of epoch  339  is  4.098796904087067\n",
      "Step 0  Loss: 0.2791009843349457\n",
      "Step 1  Loss: 0.3868069350719452\n",
      "Step 2  Loss: 0.36647748947143555\n",
      "Step 3  Loss: 0.347135454416275\n",
      "Step 4  Loss: 0.40670186281204224\n",
      "Step 5  Loss: 0.3096016049385071\n",
      "Step 6  Loss: 0.28599315881729126\n",
      "Step 7  Loss: 0.3530227541923523\n",
      "Step 8  Loss: 0.38325974345207214\n",
      "Step 9  Loss: 0.35769304633140564\n",
      "Step 10  Loss: 0.39650729298591614\n",
      "total Loss of epoch  340  is  3.8723003268241882\n",
      "Step 0  Loss: 0.3385390341281891\n",
      "Step 1  Loss: 0.341187983751297\n",
      "Step 2  Loss: 0.3667113184928894\n",
      "Step 3  Loss: 0.25798413157463074\n",
      "Step 4  Loss: 0.3511609435081482\n",
      "Step 5  Loss: 0.29948219656944275\n",
      "Step 6  Loss: 0.27351680397987366\n",
      "Step 7  Loss: 0.346348375082016\n",
      "Step 8  Loss: 0.366705060005188\n",
      "Step 9  Loss: 0.29424822330474854\n",
      "Step 10  Loss: 0.502665638923645\n",
      "total Loss of epoch  341  is  3.7385497093200684\n",
      "Step 0  Loss: 0.38815838098526\n",
      "Step 1  Loss: 0.3495613932609558\n",
      "Step 2  Loss: 0.33109208941459656\n",
      "Step 3  Loss: 0.30806976556777954\n",
      "Step 4  Loss: 0.2904888093471527\n",
      "Step 5  Loss: 0.2870043218135834\n",
      "Step 6  Loss: 0.4403476119041443\n",
      "Step 7  Loss: 0.34431570768356323\n",
      "Step 8  Loss: 0.40921592712402344\n",
      "Step 9  Loss: 0.35308387875556946\n",
      "Step 10  Loss: 0.3755777180194855\n",
      "total Loss of epoch  342  is  3.876915603876114\n",
      "Step 0  Loss: 0.33009669184684753\n",
      "Step 1  Loss: 0.31848013401031494\n",
      "Step 2  Loss: 0.38679906725883484\n",
      "Step 3  Loss: 0.4035276472568512\n",
      "Step 4  Loss: 0.45061612129211426\n",
      "Step 5  Loss: 0.40454229712486267\n",
      "Step 6  Loss: 0.3705577850341797\n",
      "Step 7  Loss: 0.4734858572483063\n",
      "Step 8  Loss: 0.3611963391304016\n",
      "Step 9  Loss: 0.2814895808696747\n",
      "Step 10  Loss: 0.41399404406547546\n",
      "total Loss of epoch  343  is  4.194785565137863\n",
      "Step 0  Loss: 0.3966616690158844\n",
      "Step 1  Loss: 0.4041943848133087\n",
      "Step 2  Loss: 0.28531980514526367\n",
      "Step 3  Loss: 0.29418253898620605\n",
      "Step 4  Loss: 0.3072831332683563\n",
      "Step 5  Loss: 0.30123594403266907\n",
      "Step 6  Loss: 0.28517866134643555\n",
      "Step 7  Loss: 0.3197919726371765\n",
      "Step 8  Loss: 0.20597514510154724\n",
      "Step 9  Loss: 0.2769123911857605\n",
      "Step 10  Loss: 0.3804486393928528\n",
      "total Loss of epoch  344  is  3.457184284925461\n",
      "Step 0  Loss: 0.48761263489723206\n",
      "Step 1  Loss: 0.32672497630119324\n",
      "Step 2  Loss: 0.3809584677219391\n",
      "Step 3  Loss: 0.3055107295513153\n",
      "Step 4  Loss: 0.4028191864490509\n",
      "Step 5  Loss: 0.32984915375709534\n",
      "Step 6  Loss: 0.3777310848236084\n",
      "Step 7  Loss: 0.3473130762577057\n",
      "Step 8  Loss: 0.25575998425483704\n",
      "Step 9  Loss: 0.2979365885257721\n",
      "Step 10  Loss: 0.35221949219703674\n",
      "total Loss of epoch  345  is  3.864435374736786\n",
      "Step 0  Loss: 0.40903329849243164\n",
      "Step 1  Loss: 0.4029536247253418\n",
      "Step 2  Loss: 0.22296616435050964\n",
      "Step 3  Loss: 0.2425638735294342\n",
      "Step 4  Loss: 0.4242206811904907\n",
      "Step 5  Loss: 0.4417378306388855\n",
      "Step 6  Loss: 0.42717045545578003\n",
      "Step 7  Loss: 0.36671021580696106\n",
      "Step 8  Loss: 0.4000289738178253\n",
      "Step 9  Loss: 0.2362353652715683\n",
      "Step 10  Loss: 0.54815274477005\n",
      "total Loss of epoch  346  is  4.121773228049278\n",
      "Step 0  Loss: 0.36773309111595154\n",
      "Step 1  Loss: 0.3478659391403198\n",
      "Step 2  Loss: 0.3401776850223541\n",
      "Step 3  Loss: 0.32210952043533325\n",
      "Step 4  Loss: 0.31847620010375977\n",
      "Step 5  Loss: 0.2759894132614136\n",
      "Step 6  Loss: 0.3206520080566406\n",
      "Step 7  Loss: 0.43644586205482483\n",
      "Step 8  Loss: 0.3214440643787384\n",
      "Step 9  Loss: 0.28158819675445557\n",
      "Step 10  Loss: 0.3233796954154968\n",
      "total Loss of epoch  347  is  3.6558616757392883\n",
      "Step 0  Loss: 0.40835005044937134\n",
      "Step 1  Loss: 0.37430623173713684\n",
      "Step 2  Loss: 0.4588906168937683\n",
      "Step 3  Loss: 0.2676236629486084\n",
      "Step 4  Loss: 0.33669695258140564\n",
      "Step 5  Loss: 0.30879127979278564\n",
      "Step 6  Loss: 0.4555772840976715\n",
      "Step 7  Loss: 0.3150365352630615\n",
      "Step 8  Loss: 0.38500699400901794\n",
      "Step 9  Loss: 0.3356555998325348\n",
      "Step 10  Loss: 0.2893204092979431\n",
      "total Loss of epoch  348  is  3.935255616903305\n",
      "Step 0  Loss: 0.37401410937309265\n",
      "Step 1  Loss: 0.3152909278869629\n",
      "Step 2  Loss: 0.3499555289745331\n",
      "Step 3  Loss: 0.29103535413742065\n",
      "Step 4  Loss: 0.33724960684776306\n",
      "Step 5  Loss: 0.3778645694255829\n",
      "Step 6  Loss: 0.398525595664978\n",
      "Step 7  Loss: 0.36581215262413025\n",
      "Step 8  Loss: 0.41295021772384644\n",
      "Step 9  Loss: 0.36538970470428467\n",
      "Step 10  Loss: 0.3334865868091583\n",
      "total Loss of epoch  349  is  3.921574354171753\n",
      "Step 0  Loss: 0.2908342778682709\n",
      "Step 1  Loss: 0.1912238597869873\n",
      "Step 2  Loss: 0.30630019307136536\n",
      "Step 3  Loss: 0.3533543348312378\n",
      "Step 4  Loss: 0.36814063787460327\n",
      "Step 5  Loss: 0.4641347825527191\n",
      "Step 6  Loss: 0.40063783526420593\n",
      "Step 7  Loss: 0.37486931681632996\n",
      "Step 8  Loss: 0.3440898060798645\n",
      "Step 9  Loss: 0.41638022661209106\n",
      "Step 10  Loss: 0.4146498143672943\n",
      "total Loss of epoch  350  is  3.9246150851249695\n",
      "Step 0  Loss: 0.30841439962387085\n",
      "Step 1  Loss: 0.4187961518764496\n",
      "Step 2  Loss: 0.41475728154182434\n",
      "Step 3  Loss: 0.44809961318969727\n",
      "Step 4  Loss: 0.2815473973751068\n",
      "Step 5  Loss: 0.2826334834098816\n",
      "Step 6  Loss: 0.3396702706813812\n",
      "Step 7  Loss: 0.2581327557563782\n",
      "Step 8  Loss: 0.35524773597717285\n",
      "Step 9  Loss: 0.3864261209964752\n",
      "Step 10  Loss: 0.2751510739326477\n",
      "total Loss of epoch  351  is  3.7688762843608856\n",
      "Step 0  Loss: 0.3806391656398773\n",
      "Step 1  Loss: 0.38191625475883484\n",
      "Step 2  Loss: 0.3187949061393738\n",
      "Step 3  Loss: 0.5158092975616455\n",
      "Step 4  Loss: 0.31203585863113403\n",
      "Step 5  Loss: 0.3015267252922058\n",
      "Step 6  Loss: 0.36110812425613403\n",
      "Step 7  Loss: 0.4569299519062042\n",
      "Step 8  Loss: 0.3907789885997772\n",
      "Step 9  Loss: 0.39893990755081177\n",
      "Step 10  Loss: 0.3958125114440918\n",
      "total Loss of epoch  352  is  4.21429169178009\n",
      "Step 0  Loss: 0.2784600555896759\n",
      "Step 1  Loss: 0.2478684037923813\n",
      "Step 2  Loss: 0.39453309774398804\n",
      "Step 3  Loss: 0.3517349362373352\n",
      "Step 4  Loss: 0.3657013773918152\n",
      "Step 5  Loss: 0.3081713318824768\n",
      "Step 6  Loss: 0.2564486563205719\n",
      "Step 7  Loss: 0.45266467332839966\n",
      "Step 8  Loss: 0.43158769607543945\n",
      "Step 9  Loss: 0.3283074200153351\n",
      "Step 10  Loss: 0.5335912108421326\n",
      "total Loss of epoch  353  is  3.949068859219551\n",
      "Step 0  Loss: 0.3728485405445099\n",
      "Step 1  Loss: 0.3881981074810028\n",
      "Step 2  Loss: 0.4180584251880646\n",
      "Step 3  Loss: 0.3129196763038635\n",
      "Step 4  Loss: 0.3592776954174042\n",
      "Step 5  Loss: 0.4114646017551422\n",
      "Step 6  Loss: 0.37785404920578003\n",
      "Step 7  Loss: 0.29820528626441956\n",
      "Step 8  Loss: 0.2757250666618347\n",
      "Step 9  Loss: 0.2702291011810303\n",
      "Step 10  Loss: 0.28208014369010925\n",
      "total Loss of epoch  354  is  3.766860693693161\n",
      "Step 0  Loss: 0.3343920409679413\n",
      "Step 1  Loss: 0.317484587430954\n",
      "Step 2  Loss: 0.43358245491981506\n",
      "Step 3  Loss: 0.34388768672943115\n",
      "Step 4  Loss: 0.35601261258125305\n",
      "Step 5  Loss: 0.3173230290412903\n",
      "Step 6  Loss: 0.3219512104988098\n",
      "Step 7  Loss: 0.30946388840675354\n",
      "Step 8  Loss: 0.2842367887496948\n",
      "Step 9  Loss: 0.4476260244846344\n",
      "Step 10  Loss: 0.5352863669395447\n",
      "total Loss of epoch  355  is  4.001246690750122\n",
      "Step 0  Loss: 0.42239752411842346\n",
      "Step 1  Loss: 0.44008904695510864\n",
      "Step 2  Loss: 0.30835962295532227\n",
      "Step 3  Loss: 0.4061836004257202\n",
      "Step 4  Loss: 0.29877060651779175\n",
      "Step 5  Loss: 0.3377474248409271\n",
      "Step 6  Loss: 0.27481040358543396\n",
      "Step 7  Loss: 0.4064613878726959\n",
      "Step 8  Loss: 0.360026478767395\n",
      "Step 9  Loss: 0.31221723556518555\n",
      "Step 10  Loss: 0.2515914738178253\n",
      "total Loss of epoch  356  is  3.8186548054218292\n",
      "Step 0  Loss: 0.3486502766609192\n",
      "Step 1  Loss: 0.3248125910758972\n",
      "Step 2  Loss: 0.3999517261981964\n",
      "Step 3  Loss: 0.3724225163459778\n",
      "Step 4  Loss: 0.3271868824958801\n",
      "Step 5  Loss: 0.4530007243156433\n",
      "Step 6  Loss: 0.3421371281147003\n",
      "Step 7  Loss: 0.24184732139110565\n",
      "Step 8  Loss: 0.33272436261177063\n",
      "Step 9  Loss: 0.2996211647987366\n",
      "Step 10  Loss: 0.4315311908721924\n",
      "total Loss of epoch  357  is  3.8738858848810196\n",
      "Step 0  Loss: 0.3205801546573639\n",
      "Step 1  Loss: 0.2740395963191986\n",
      "Step 2  Loss: 0.38089144229888916\n",
      "Step 3  Loss: 0.27409690618515015\n",
      "Step 4  Loss: 0.31754758954048157\n",
      "Step 5  Loss: 0.3777233064174652\n",
      "Step 6  Loss: 0.3790932893753052\n",
      "Step 7  Loss: 0.3205920159816742\n",
      "Step 8  Loss: 0.2826675772666931\n",
      "Step 9  Loss: 0.3160417377948761\n",
      "Step 10  Loss: 0.5790560841560364\n",
      "total Loss of epoch  358  is  3.8223296999931335\n",
      "Step 0  Loss: 0.4361032545566559\n",
      "Step 1  Loss: 0.3705279231071472\n",
      "Step 2  Loss: 0.25820156931877136\n",
      "Step 3  Loss: 0.37212634086608887\n",
      "Step 4  Loss: 0.3334847092628479\n",
      "Step 5  Loss: 0.33307328820228577\n",
      "Step 6  Loss: 0.3576425015926361\n",
      "Step 7  Loss: 0.2741541564464569\n",
      "Step 8  Loss: 0.41632816195487976\n",
      "Step 9  Loss: 0.26256829500198364\n",
      "Step 10  Loss: 0.36293110251426697\n",
      "total Loss of epoch  359  is  3.7771413028240204\n",
      "Step 0  Loss: 0.48568102717399597\n",
      "Step 1  Loss: 0.3422672748565674\n",
      "Step 2  Loss: 0.3872477412223816\n",
      "Step 3  Loss: 0.28753334283828735\n",
      "Step 4  Loss: 0.3997704088687897\n",
      "Step 5  Loss: 0.3115196228027344\n",
      "Step 6  Loss: 0.3680749237537384\n",
      "Step 7  Loss: 0.4156416952610016\n",
      "Step 8  Loss: 0.34117135405540466\n",
      "Step 9  Loss: 0.31270620226860046\n",
      "Step 10  Loss: 0.35820865631103516\n",
      "total Loss of epoch  360  is  4.009822249412537\n",
      "Step 0  Loss: 0.39773550629615784\n",
      "Step 1  Loss: 0.3557887077331543\n",
      "Step 2  Loss: 0.36932119727134705\n",
      "Step 3  Loss: 0.44249239563941956\n",
      "Step 4  Loss: 0.3233259320259094\n",
      "Step 5  Loss: 0.36272329092025757\n",
      "Step 6  Loss: 0.36014387011528015\n",
      "Step 7  Loss: 0.4304948151111603\n",
      "Step 8  Loss: 0.3074771761894226\n",
      "Step 9  Loss: 0.29646340012550354\n",
      "Step 10  Loss: 0.49278128147125244\n",
      "total Loss of epoch  361  is  4.138747572898865\n",
      "Step 0  Loss: 0.454330712556839\n",
      "Step 1  Loss: 0.3514528274536133\n",
      "Step 2  Loss: 0.41565752029418945\n",
      "Step 3  Loss: 0.42081111669540405\n",
      "Step 4  Loss: 0.3393774628639221\n",
      "Step 5  Loss: 0.31505945324897766\n",
      "Step 6  Loss: 0.3852311670780182\n",
      "Step 7  Loss: 0.28446218371391296\n",
      "Step 8  Loss: 0.3047427535057068\n",
      "Step 9  Loss: 0.2974260151386261\n",
      "Step 10  Loss: 0.40229007601737976\n",
      "total Loss of epoch  362  is  3.9708412885665894\n",
      "Step 0  Loss: 0.46815675497055054\n",
      "Step 1  Loss: 0.3314318060874939\n",
      "Step 2  Loss: 0.2791486978530884\n",
      "Step 3  Loss: 0.2825773060321808\n",
      "Step 4  Loss: 0.39532387256622314\n",
      "Step 5  Loss: 0.32565072178840637\n",
      "Step 6  Loss: 0.3834895193576813\n",
      "Step 7  Loss: 0.3244437575340271\n",
      "Step 8  Loss: 0.4021269977092743\n",
      "Step 9  Loss: 0.30601778626441956\n",
      "Step 10  Loss: 0.38063743710517883\n",
      "total Loss of epoch  363  is  3.879004657268524\n",
      "Step 0  Loss: 0.2455277293920517\n",
      "Step 1  Loss: 0.286521852016449\n",
      "Step 2  Loss: 0.3195893168449402\n",
      "Step 3  Loss: 0.3958280086517334\n",
      "Step 4  Loss: 0.294080525636673\n",
      "Step 5  Loss: 0.509393036365509\n",
      "Step 6  Loss: 0.2807231843471527\n",
      "Step 7  Loss: 0.33962109684944153\n",
      "Step 8  Loss: 0.302879273891449\n",
      "Step 9  Loss: 0.4118119776248932\n",
      "Step 10  Loss: 0.37174728512763977\n",
      "total Loss of epoch  364  is  3.7577232867479324\n",
      "Step 0  Loss: 0.24214836955070496\n",
      "Step 1  Loss: 0.32849839329719543\n",
      "Step 2  Loss: 0.3766815960407257\n",
      "Step 3  Loss: 0.3376968204975128\n",
      "Step 4  Loss: 0.2685450613498688\n",
      "Step 5  Loss: 0.2044147104024887\n",
      "Step 6  Loss: 0.4336068332195282\n",
      "Step 7  Loss: 0.3141273856163025\n",
      "Step 8  Loss: 0.3968260884284973\n",
      "Step 9  Loss: 0.25255087018013\n",
      "Step 10  Loss: 0.30529090762138367\n",
      "total Loss of epoch  365  is  3.460387036204338\n",
      "Step 0  Loss: 0.37386831641197205\n",
      "Step 1  Loss: 0.2234395295381546\n",
      "Step 2  Loss: 0.31745705008506775\n",
      "Step 3  Loss: 0.3725729286670685\n",
      "Step 4  Loss: 0.36745986342430115\n",
      "Step 5  Loss: 0.4328368306159973\n",
      "Step 6  Loss: 0.3234862983226776\n",
      "Step 7  Loss: 0.28526899218559265\n",
      "Step 8  Loss: 0.30519068241119385\n",
      "Step 9  Loss: 0.37027400732040405\n",
      "Step 10  Loss: 0.4672892391681671\n",
      "total Loss of epoch  366  is  3.8391437381505966\n",
      "Step 0  Loss: 0.42150965332984924\n",
      "Step 1  Loss: 0.2720465362071991\n",
      "Step 2  Loss: 0.34378311038017273\n",
      "Step 3  Loss: 0.3365866541862488\n",
      "Step 4  Loss: 0.23311427235603333\n",
      "Step 5  Loss: 0.33083492517471313\n",
      "Step 6  Loss: 0.3615640103816986\n",
      "Step 7  Loss: 0.2945243716239929\n",
      "Step 8  Loss: 0.29090040922164917\n",
      "Step 9  Loss: 0.3859626054763794\n",
      "Step 10  Loss: 0.27836135029792786\n",
      "total Loss of epoch  367  is  3.5491878986358643\n",
      "Step 0  Loss: 0.3331184387207031\n",
      "Step 1  Loss: 0.3820814788341522\n",
      "Step 2  Loss: 0.47021231055259705\n",
      "Step 3  Loss: 0.4298178255558014\n",
      "Step 4  Loss: 0.44006454944610596\n",
      "Step 5  Loss: 0.4909806251525879\n",
      "Step 6  Loss: 0.3511236906051636\n",
      "Step 7  Loss: 0.20515409111976624\n",
      "Step 8  Loss: 0.2991442382335663\n",
      "Step 9  Loss: 0.4084528684616089\n",
      "Step 10  Loss: 0.4057644307613373\n",
      "total Loss of epoch  368  is  4.21591454744339\n",
      "Step 0  Loss: 0.336020290851593\n",
      "Step 1  Loss: 0.3373403251171112\n",
      "Step 2  Loss: 0.3380255401134491\n",
      "Step 3  Loss: 0.396271675825119\n",
      "Step 4  Loss: 0.4126107394695282\n",
      "Step 5  Loss: 0.3068925440311432\n",
      "Step 6  Loss: 0.26527488231658936\n",
      "Step 7  Loss: 0.356748104095459\n",
      "Step 8  Loss: 0.2982969284057617\n",
      "Step 9  Loss: 0.46873658895492554\n",
      "Step 10  Loss: 0.49709147214889526\n",
      "total Loss of epoch  369  is  4.013309091329575\n",
      "Step 0  Loss: 0.4053488075733185\n",
      "Step 1  Loss: 0.4275132715702057\n",
      "Step 2  Loss: 0.29761138558387756\n",
      "Step 3  Loss: 0.3475116193294525\n",
      "Step 4  Loss: 0.2920372784137726\n",
      "Step 5  Loss: 0.3237285912036896\n",
      "Step 6  Loss: 0.39410147070884705\n",
      "Step 7  Loss: 0.3966432809829712\n",
      "Step 8  Loss: 0.5248339176177979\n",
      "Step 9  Loss: 0.3806058466434479\n",
      "Step 10  Loss: 0.3612978160381317\n",
      "total Loss of epoch  370  is  4.151233285665512\n",
      "Step 0  Loss: 0.3941398561000824\n",
      "Step 1  Loss: 0.32031911611557007\n",
      "Step 2  Loss: 0.4214140772819519\n",
      "Step 3  Loss: 0.35311004519462585\n",
      "Step 4  Loss: 0.3998119831085205\n",
      "Step 5  Loss: 0.29310905933380127\n",
      "Step 6  Loss: 0.32258370518684387\n",
      "Step 7  Loss: 0.3989153504371643\n",
      "Step 8  Loss: 0.4417276680469513\n",
      "Step 9  Loss: 0.4350369870662689\n",
      "Step 10  Loss: 0.4032166302204132\n",
      "total Loss of epoch  371  is  4.183384478092194\n",
      "Step 0  Loss: 0.3566940128803253\n",
      "Step 1  Loss: 0.3128187954425812\n",
      "Step 2  Loss: 0.2854662239551544\n",
      "Step 3  Loss: 0.43544358015060425\n",
      "Step 4  Loss: 0.29659929871559143\n",
      "Step 5  Loss: 0.21610702574253082\n",
      "Step 6  Loss: 0.3849865794181824\n",
      "Step 7  Loss: 0.29400643706321716\n",
      "Step 8  Loss: 0.37671032547950745\n",
      "Step 9  Loss: 0.2526807487010956\n",
      "Step 10  Loss: 0.49896305799484253\n",
      "total Loss of epoch  372  is  3.7104760855436325\n",
      "Step 0  Loss: 0.3157311975955963\n",
      "Step 1  Loss: 0.24857211112976074\n",
      "Step 2  Loss: 0.4386342167854309\n",
      "Step 3  Loss: 0.28831446170806885\n",
      "Step 4  Loss: 0.3420107364654541\n",
      "Step 5  Loss: 0.34517234563827515\n",
      "Step 6  Loss: 0.3659467399120331\n",
      "Step 7  Loss: 0.3340018093585968\n",
      "Step 8  Loss: 0.33049532771110535\n",
      "Step 9  Loss: 0.4110179543495178\n",
      "Step 10  Loss: 0.4034203588962555\n",
      "total Loss of epoch  373  is  3.8233172595500946\n",
      "Step 0  Loss: 0.3717515170574188\n",
      "Step 1  Loss: 0.3714470863342285\n",
      "Step 2  Loss: 0.3514682352542877\n",
      "Step 3  Loss: 0.18408167362213135\n",
      "Step 4  Loss: 0.3738236427307129\n",
      "Step 5  Loss: 0.3674136996269226\n",
      "Step 6  Loss: 0.2801336646080017\n",
      "Step 7  Loss: 0.3655408024787903\n",
      "Step 8  Loss: 0.22098268568515778\n",
      "Step 9  Loss: 0.4611745774745941\n",
      "Step 10  Loss: 0.3802487850189209\n",
      "total Loss of epoch  374  is  3.7280663698911667\n",
      "Step 0  Loss: 0.28755801916122437\n",
      "Step 1  Loss: 0.31148481369018555\n",
      "Step 2  Loss: 0.383251428604126\n",
      "Step 3  Loss: 0.402182936668396\n",
      "Step 4  Loss: 0.41366153955459595\n",
      "Step 5  Loss: 0.20955190062522888\n",
      "Step 6  Loss: 0.30312368273735046\n",
      "Step 7  Loss: 0.2912617325782776\n",
      "Step 8  Loss: 0.37935197353363037\n",
      "Step 9  Loss: 0.2874719202518463\n",
      "Step 10  Loss: 0.35267066955566406\n",
      "total Loss of epoch  375  is  3.6215706169605255\n",
      "Step 0  Loss: 0.26360076665878296\n",
      "Step 1  Loss: 0.40417829155921936\n",
      "Step 2  Loss: 0.37582412362098694\n",
      "Step 3  Loss: 0.4041980504989624\n",
      "Step 4  Loss: 0.37955695390701294\n",
      "Step 5  Loss: 0.3932307958602905\n",
      "Step 6  Loss: 0.47686833143234253\n",
      "Step 7  Loss: 0.3330477178096771\n",
      "Step 8  Loss: 0.42771369218826294\n",
      "Step 9  Loss: 0.2386365532875061\n",
      "Step 10  Loss: 0.22866809368133545\n",
      "total Loss of epoch  376  is  3.9255233705043793\n",
      "Step 0  Loss: 0.3920179009437561\n",
      "Step 1  Loss: 0.3313691020011902\n",
      "Step 2  Loss: 0.38691967725753784\n",
      "Step 3  Loss: 0.3752608299255371\n",
      "Step 4  Loss: 0.25055545568466187\n",
      "Step 5  Loss: 0.223249152302742\n",
      "Step 6  Loss: 0.3240374028682709\n",
      "Step 7  Loss: 0.30936399102211\n",
      "Step 8  Loss: 0.29216858744621277\n",
      "Step 9  Loss: 0.3894887864589691\n",
      "Step 10  Loss: 0.32120615243911743\n",
      "total Loss of epoch  377  is  3.5956370383501053\n",
      "Step 0  Loss: 0.3311167061328888\n",
      "Step 1  Loss: 0.373298317193985\n",
      "Step 2  Loss: 0.411058634519577\n",
      "Step 3  Loss: 0.3626628816127777\n",
      "Step 4  Loss: 0.3103606700897217\n",
      "Step 5  Loss: 0.3989478051662445\n",
      "Step 6  Loss: 0.3781617283821106\n",
      "Step 7  Loss: 0.33015957474708557\n",
      "Step 8  Loss: 0.27356934547424316\n",
      "Step 9  Loss: 0.34506142139434814\n",
      "Step 10  Loss: 0.25816604495048523\n",
      "total Loss of epoch  378  is  3.7725631296634674\n",
      "Step 0  Loss: 0.32389023900032043\n",
      "Step 1  Loss: 0.3587726354598999\n",
      "Step 2  Loss: 0.3025917708873749\n",
      "Step 3  Loss: 0.36670804023742676\n",
      "Step 4  Loss: 0.372334361076355\n",
      "Step 5  Loss: 0.4402865767478943\n",
      "Step 6  Loss: 0.37356477975845337\n",
      "Step 7  Loss: 0.47577810287475586\n",
      "Step 8  Loss: 0.2806263566017151\n",
      "Step 9  Loss: 0.3480870723724365\n",
      "Step 10  Loss: 0.5460817217826843\n",
      "total Loss of epoch  379  is  4.188721656799316\n",
      "Step 0  Loss: 0.350239098072052\n",
      "Step 1  Loss: 0.34537938237190247\n",
      "Step 2  Loss: 0.29045626521110535\n",
      "Step 3  Loss: 0.31715455651283264\n",
      "Step 4  Loss: 0.347867488861084\n",
      "Step 5  Loss: 0.29018712043762207\n",
      "Step 6  Loss: 0.3094615340232849\n",
      "Step 7  Loss: 0.28180405497550964\n",
      "Step 8  Loss: 0.46790575981140137\n",
      "Step 9  Loss: 0.3802587687969208\n",
      "Step 10  Loss: 0.30238115787506104\n",
      "total Loss of epoch  380  is  3.6830951869487762\n",
      "Step 0  Loss: 0.27118903398513794\n",
      "Step 1  Loss: 0.3294086158275604\n",
      "Step 2  Loss: 0.43194204568862915\n",
      "Step 3  Loss: 0.29674163460731506\n",
      "Step 4  Loss: 0.31722185015678406\n",
      "Step 5  Loss: 0.3174799382686615\n",
      "Step 6  Loss: 0.2741795480251312\n",
      "Step 7  Loss: 0.3736906945705414\n",
      "Step 8  Loss: 0.4292091131210327\n",
      "Step 9  Loss: 0.3018108606338501\n",
      "Step 10  Loss: 0.4388381540775299\n",
      "total Loss of epoch  381  is  3.7817114889621735\n",
      "Step 0  Loss: 0.3747905492782593\n",
      "Step 1  Loss: 0.24680642783641815\n",
      "Step 2  Loss: 0.47379323840141296\n",
      "Step 3  Loss: 0.3261314332485199\n",
      "Step 4  Loss: 0.305155873298645\n",
      "Step 5  Loss: 0.42026323080062866\n",
      "Step 6  Loss: 0.39441341161727905\n",
      "Step 7  Loss: 0.3722679018974304\n",
      "Step 8  Loss: 0.27986016869544983\n",
      "Step 9  Loss: 0.4277275800704956\n",
      "Step 10  Loss: 0.28033822774887085\n",
      "total Loss of epoch  382  is  3.9015480428934097\n",
      "Step 0  Loss: 0.2541726231575012\n",
      "Step 1  Loss: 0.23047156631946564\n",
      "Step 2  Loss: 0.315082848072052\n",
      "Step 3  Loss: 0.3868200182914734\n",
      "Step 4  Loss: 0.39435067772865295\n",
      "Step 5  Loss: 0.3173324167728424\n",
      "Step 6  Loss: 0.3290470540523529\n",
      "Step 7  Loss: 0.24943280220031738\n",
      "Step 8  Loss: 0.31090861558914185\n",
      "Step 9  Loss: 0.4381901025772095\n",
      "Step 10  Loss: 0.32551056146621704\n",
      "total Loss of epoch  383  is  3.5513192862272263\n",
      "Step 0  Loss: 0.32091665267944336\n",
      "Step 1  Loss: 0.29014143347740173\n",
      "Step 2  Loss: 0.33403557538986206\n",
      "Step 3  Loss: 0.360077828168869\n",
      "Step 4  Loss: 0.3953340947628021\n",
      "Step 5  Loss: 0.3640916645526886\n",
      "Step 6  Loss: 0.33358868956565857\n",
      "Step 7  Loss: 0.26144924759864807\n",
      "Step 8  Loss: 0.36903777718544006\n",
      "Step 9  Loss: 0.45084989070892334\n",
      "Step 10  Loss: 0.5196235179901123\n",
      "total Loss of epoch  384  is  3.9991463720798492\n",
      "Step 0  Loss: 0.3504880666732788\n",
      "Step 1  Loss: 0.3197503387928009\n",
      "Step 2  Loss: 0.3958355188369751\n",
      "Step 3  Loss: 0.37400248646736145\n",
      "Step 4  Loss: 0.3158915042877197\n",
      "Step 5  Loss: 0.26869919896125793\n",
      "Step 6  Loss: 0.3444717526435852\n",
      "Step 7  Loss: 0.3462991714477539\n",
      "Step 8  Loss: 0.3425987958908081\n",
      "Step 9  Loss: 0.3597085773944855\n",
      "Step 10  Loss: 0.2198619246482849\n",
      "total Loss of epoch  385  is  3.6376073360443115\n",
      "Step 0  Loss: 0.3691820204257965\n",
      "Step 1  Loss: 0.2866915464401245\n",
      "Step 2  Loss: 0.41627663373947144\n",
      "Step 3  Loss: 0.3262583017349243\n",
      "Step 4  Loss: 0.375559538602829\n",
      "Step 5  Loss: 0.3485104441642761\n",
      "Step 6  Loss: 0.41292524337768555\n",
      "Step 7  Loss: 0.4417928159236908\n",
      "Step 8  Loss: 0.36867955327033997\n",
      "Step 9  Loss: 0.48022976517677307\n",
      "Step 10  Loss: 0.3160235583782196\n",
      "total Loss of epoch  386  is  4.142129421234131\n",
      "Step 0  Loss: 0.37358254194259644\n",
      "Step 1  Loss: 0.3056703805923462\n",
      "Step 2  Loss: 0.4249037504196167\n",
      "Step 3  Loss: 0.3733659088611603\n",
      "Step 4  Loss: 0.2939736247062683\n",
      "Step 5  Loss: 0.31783151626586914\n",
      "Step 6  Loss: 0.39905276894569397\n",
      "Step 7  Loss: 0.47476261854171753\n",
      "Step 8  Loss: 0.3848288357257843\n",
      "Step 9  Loss: 0.30028337240219116\n",
      "Step 10  Loss: 0.25570744276046753\n",
      "total Loss of epoch  387  is  3.9039627611637115\n",
      "Step 0  Loss: 0.3561484217643738\n",
      "Step 1  Loss: 0.3520009517669678\n",
      "Step 2  Loss: 0.37566834688186646\n",
      "Step 3  Loss: 0.2442069947719574\n",
      "Step 4  Loss: 0.4748084545135498\n",
      "Step 5  Loss: 0.30037012696266174\n",
      "Step 6  Loss: 0.3442716598510742\n",
      "Step 7  Loss: 0.38546663522720337\n",
      "Step 8  Loss: 0.39949139952659607\n",
      "Step 9  Loss: 0.3871532678604126\n",
      "Step 10  Loss: 0.36191436648368835\n",
      "total Loss of epoch  388  is  3.9815006256103516\n",
      "Step 0  Loss: 0.34666359424591064\n",
      "Step 1  Loss: 0.40766972303390503\n",
      "Step 2  Loss: 0.26470163464546204\n",
      "Step 3  Loss: 0.37073731422424316\n",
      "Step 4  Loss: 0.32857558131217957\n",
      "Step 5  Loss: 0.3981233835220337\n",
      "Step 6  Loss: 0.315976083278656\n",
      "Step 7  Loss: 0.38811206817626953\n",
      "Step 8  Loss: 0.3522402346134186\n",
      "Step 9  Loss: 0.28880661725997925\n",
      "Step 10  Loss: 0.37054920196533203\n",
      "total Loss of epoch  389  is  3.8321554362773895\n",
      "Step 0  Loss: 0.3929966986179352\n",
      "Step 1  Loss: 0.4082556664943695\n",
      "Step 2  Loss: 0.4050038158893585\n",
      "Step 3  Loss: 0.42137640714645386\n",
      "Step 4  Loss: 0.24726705253124237\n",
      "Step 5  Loss: 0.2932444214820862\n",
      "Step 6  Loss: 0.2853785455226898\n",
      "Step 7  Loss: 0.36544084548950195\n",
      "Step 8  Loss: 0.34999242424964905\n",
      "Step 9  Loss: 0.3680470883846283\n",
      "Step 10  Loss: 0.4403294026851654\n",
      "total Loss of epoch  390  is  3.97733236849308\n",
      "Step 0  Loss: 0.34513959288597107\n",
      "Step 1  Loss: 0.28917044401168823\n",
      "Step 2  Loss: 0.3361552357673645\n",
      "Step 3  Loss: 0.43205586075782776\n",
      "Step 4  Loss: 0.29072102904319763\n",
      "Step 5  Loss: 0.4365573525428772\n",
      "Step 6  Loss: 0.42704153060913086\n",
      "Step 7  Loss: 0.25357484817504883\n",
      "Step 8  Loss: 0.3824481964111328\n",
      "Step 9  Loss: 0.2735134959220886\n",
      "Step 10  Loss: 0.2486257702112198\n",
      "total Loss of epoch  391  is  3.7150033563375473\n",
      "Step 0  Loss: 0.32802391052246094\n",
      "Step 1  Loss: 0.3854006230831146\n",
      "Step 2  Loss: 0.42043063044548035\n",
      "Step 3  Loss: 0.27983224391937256\n",
      "Step 4  Loss: 0.38879871368408203\n",
      "Step 5  Loss: 0.36052805185317993\n",
      "Step 6  Loss: 0.3801257610321045\n",
      "Step 7  Loss: 0.28514134883880615\n",
      "Step 8  Loss: 0.33370620012283325\n",
      "Step 9  Loss: 0.2868955135345459\n",
      "Step 10  Loss: 0.49003806710243225\n",
      "total Loss of epoch  392  is  3.9389210641384125\n",
      "Step 0  Loss: 0.259718120098114\n",
      "Step 1  Loss: 0.361682265996933\n",
      "Step 2  Loss: 0.39799755811691284\n",
      "Step 3  Loss: 0.2774442136287689\n",
      "Step 4  Loss: 0.28425392508506775\n",
      "Step 5  Loss: 0.4880139231681824\n",
      "Step 6  Loss: 0.4366263747215271\n",
      "Step 7  Loss: 0.2725611925125122\n",
      "Step 8  Loss: 0.26783403754234314\n",
      "Step 9  Loss: 0.31817230582237244\n",
      "Step 10  Loss: 0.28752508759498596\n",
      "total Loss of epoch  393  is  3.6518290042877197\n",
      "Step 0  Loss: 0.36118966341018677\n",
      "Step 1  Loss: 0.36883044242858887\n",
      "Step 2  Loss: 0.3999655246734619\n",
      "Step 3  Loss: 0.40917643904685974\n",
      "Step 4  Loss: 0.31786128878593445\n",
      "Step 5  Loss: 0.4025578498840332\n",
      "Step 6  Loss: 0.3489688038825989\n",
      "Step 7  Loss: 0.32650226354599\n",
      "Step 8  Loss: 0.38090282678604126\n",
      "Step 9  Loss: 0.27941691875457764\n",
      "Step 10  Loss: 0.262285441160202\n",
      "total Loss of epoch  394  is  3.8576574623584747\n",
      "Step 0  Loss: 0.3623524606227875\n",
      "Step 1  Loss: 0.3362681269645691\n",
      "Step 2  Loss: 0.2800033688545227\n",
      "Step 3  Loss: 0.3697308301925659\n",
      "Step 4  Loss: 0.37283411622047424\n",
      "Step 5  Loss: 0.342792809009552\n",
      "Step 6  Loss: 0.38993558287620544\n",
      "Step 7  Loss: 0.24226005375385284\n",
      "Step 8  Loss: 0.34539395570755005\n",
      "Step 9  Loss: 0.3062836527824402\n",
      "Step 10  Loss: 0.23124709725379944\n",
      "total Loss of epoch  395  is  3.5791020542383194\n",
      "Step 0  Loss: 0.4093134105205536\n",
      "Step 1  Loss: 0.35789093375205994\n",
      "Step 2  Loss: 0.33824777603149414\n",
      "Step 3  Loss: 0.2616272270679474\n",
      "Step 4  Loss: 0.3660379946231842\n",
      "Step 5  Loss: 0.34877684712409973\n",
      "Step 6  Loss: 0.326551228761673\n",
      "Step 7  Loss: 0.40787309408187866\n",
      "Step 8  Loss: 0.31704846024513245\n",
      "Step 9  Loss: 0.30981874465942383\n",
      "Step 10  Loss: 0.48180270195007324\n",
      "total Loss of epoch  396  is  3.92498841881752\n",
      "Step 0  Loss: 0.41430947184562683\n",
      "Step 1  Loss: 0.38453707098960876\n",
      "Step 2  Loss: 0.3198757767677307\n",
      "Step 3  Loss: 0.32675671577453613\n",
      "Step 4  Loss: 0.29937729239463806\n",
      "Step 5  Loss: 0.31621429324150085\n",
      "Step 6  Loss: 0.49005502462387085\n",
      "Step 7  Loss: 0.391579270362854\n",
      "Step 8  Loss: 0.36679497361183167\n",
      "Step 9  Loss: 0.2777171730995178\n",
      "Step 10  Loss: 0.5124950408935547\n",
      "total Loss of epoch  397  is  4.09971210360527\n",
      "Step 0  Loss: 0.28137555718421936\n",
      "Step 1  Loss: 0.35814082622528076\n",
      "Step 2  Loss: 0.3431987166404724\n",
      "Step 3  Loss: 0.20226579904556274\n",
      "Step 4  Loss: 0.2570740282535553\n",
      "Step 5  Loss: 0.32389184832572937\n",
      "Step 6  Loss: 0.23100504279136658\n",
      "Step 7  Loss: 0.35010939836502075\n",
      "Step 8  Loss: 0.43210768699645996\n",
      "Step 9  Loss: 0.3611677289009094\n",
      "Step 10  Loss: 0.4985872507095337\n",
      "total Loss of epoch  398  is  3.6389238834381104\n",
      "Step 0  Loss: 0.3793700039386749\n",
      "Step 1  Loss: 0.4642026126384735\n",
      "Step 2  Loss: 0.339046835899353\n",
      "Step 3  Loss: 0.5244853496551514\n",
      "Step 4  Loss: 0.3481825888156891\n",
      "Step 5  Loss: 0.2987467646598816\n",
      "Step 6  Loss: 0.32542771100997925\n",
      "Step 7  Loss: 0.3418523967266083\n",
      "Step 8  Loss: 0.35913875699043274\n",
      "Step 9  Loss: 0.2832300662994385\n",
      "Step 10  Loss: 0.47018784284591675\n",
      "total Loss of epoch  399  is  4.133870929479599\n",
      "Step 0  Loss: 0.2415609061717987\n",
      "Step 1  Loss: 0.32970863580703735\n",
      "Step 2  Loss: 0.4674893915653229\n",
      "Step 3  Loss: 0.3361720144748688\n",
      "Step 4  Loss: 0.41484159231185913\n",
      "Step 5  Loss: 0.2421582192182541\n",
      "Step 6  Loss: 0.2796681821346283\n",
      "Step 7  Loss: 0.3118574321269989\n",
      "Step 8  Loss: 0.33290761709213257\n",
      "Step 9  Loss: 0.3418618142604828\n",
      "Step 10  Loss: 0.3540974259376526\n",
      "total Loss of epoch  400  is  3.652323231101036\n",
      "Step 0  Loss: 0.24696092307567596\n",
      "Step 1  Loss: 0.28143739700317383\n",
      "Step 2  Loss: 0.3284221589565277\n",
      "Step 3  Loss: 0.2764114737510681\n",
      "Step 4  Loss: 0.46102601289749146\n",
      "Step 5  Loss: 0.23083962500095367\n",
      "Step 6  Loss: 0.35620811581611633\n",
      "Step 7  Loss: 0.44468334317207336\n",
      "Step 8  Loss: 0.3978396952152252\n",
      "Step 9  Loss: 0.33547908067703247\n",
      "Step 10  Loss: 0.2622004747390747\n",
      "total Loss of epoch  401  is  3.621508300304413\n",
      "Step 0  Loss: 0.2683495879173279\n",
      "Step 1  Loss: 0.2992119789123535\n",
      "Step 2  Loss: 0.4921414256095886\n",
      "Step 3  Loss: 0.4530161917209625\n",
      "Step 4  Loss: 0.406375914812088\n",
      "Step 5  Loss: 0.5307401418685913\n",
      "Step 6  Loss: 0.3573496639728546\n",
      "Step 7  Loss: 0.32404568791389465\n",
      "Step 8  Loss: 0.41560718417167664\n",
      "Step 9  Loss: 0.3472079634666443\n",
      "Step 10  Loss: 0.32404571771621704\n",
      "total Loss of epoch  402  is  4.218091458082199\n",
      "Step 0  Loss: 0.3920125365257263\n",
      "Step 1  Loss: 0.3387160301208496\n",
      "Step 2  Loss: 0.3556465804576874\n",
      "Step 3  Loss: 0.40016260743141174\n",
      "Step 4  Loss: 0.34269899129867554\n",
      "Step 5  Loss: 0.25479063391685486\n",
      "Step 6  Loss: 0.3559156060218811\n",
      "Step 7  Loss: 0.35734307765960693\n",
      "Step 8  Loss: 0.33514201641082764\n",
      "Step 9  Loss: 0.31674376130104065\n",
      "Step 10  Loss: 0.3109666109085083\n",
      "total Loss of epoch  403  is  3.76013845205307\n",
      "Step 0  Loss: 0.44216567277908325\n",
      "Step 1  Loss: 0.28712141513824463\n",
      "Step 2  Loss: 0.3069741129875183\n",
      "Step 3  Loss: 0.39600107073783875\n",
      "Step 4  Loss: 0.3175508379936218\n",
      "Step 5  Loss: 0.4197161793708801\n",
      "Step 6  Loss: 0.3036397397518158\n",
      "Step 7  Loss: 0.3323940932750702\n",
      "Step 8  Loss: 0.31004413962364197\n",
      "Step 9  Loss: 0.31257352232933044\n",
      "Step 10  Loss: 0.2811095714569092\n",
      "total Loss of epoch  404  is  3.7092903554439545\n",
      "Step 0  Loss: 0.28694987297058105\n",
      "Step 1  Loss: 0.39335140585899353\n",
      "Step 2  Loss: 0.33908605575561523\n",
      "Step 3  Loss: 0.32439717650413513\n",
      "Step 4  Loss: 0.3354540765285492\n",
      "Step 5  Loss: 0.4072955250740051\n",
      "Step 6  Loss: 0.36040666699409485\n",
      "Step 7  Loss: 0.2943515479564667\n",
      "Step 8  Loss: 0.4226044714450836\n",
      "Step 9  Loss: 0.3426201045513153\n",
      "Step 10  Loss: 0.2996116876602173\n",
      "total Loss of epoch  405  is  3.806128591299057\n",
      "Step 0  Loss: 0.30609527230262756\n",
      "Step 1  Loss: 0.34987735748291016\n",
      "Step 2  Loss: 0.3696083128452301\n",
      "Step 3  Loss: 0.36339011788368225\n",
      "Step 4  Loss: 0.3496686518192291\n",
      "Step 5  Loss: 0.23058460652828217\n",
      "Step 6  Loss: 0.4370916783809662\n",
      "Step 7  Loss: 0.4226975440979004\n",
      "Step 8  Loss: 0.2685946226119995\n",
      "Step 9  Loss: 0.4252374470233917\n",
      "Step 10  Loss: 0.3243970274925232\n",
      "total Loss of epoch  406  is  3.8472426384687424\n",
      "Step 0  Loss: 0.25533875823020935\n",
      "Step 1  Loss: 0.3543868958950043\n",
      "Step 2  Loss: 0.3090599775314331\n",
      "Step 3  Loss: 0.44051551818847656\n",
      "Step 4  Loss: 0.2749228775501251\n",
      "Step 5  Loss: 0.31194785237312317\n",
      "Step 6  Loss: 0.3565312922000885\n",
      "Step 7  Loss: 0.35077714920043945\n",
      "Step 8  Loss: 0.3092080354690552\n",
      "Step 9  Loss: 0.28215304017066956\n",
      "Step 10  Loss: 0.3227839767932892\n",
      "total Loss of epoch  407  is  3.5676253736019135\n",
      "Step 0  Loss: 0.3715554475784302\n",
      "Step 1  Loss: 0.21178613603115082\n",
      "Step 2  Loss: 0.35981646180152893\n",
      "Step 3  Loss: 0.3788790702819824\n",
      "Step 4  Loss: 0.3713776469230652\n",
      "Step 5  Loss: 0.2820826470851898\n",
      "Step 6  Loss: 0.35310089588165283\n",
      "Step 7  Loss: 0.38590002059936523\n",
      "Step 8  Loss: 0.287390798330307\n",
      "Step 9  Loss: 0.3612726926803589\n",
      "Step 10  Loss: 0.5469761490821838\n",
      "total Loss of epoch  408  is  3.910137966275215\n",
      "Step 0  Loss: 0.4244726002216339\n",
      "Step 1  Loss: 0.31741321086883545\n",
      "Step 2  Loss: 0.2368396520614624\n",
      "Step 3  Loss: 0.2673572897911072\n",
      "Step 4  Loss: 0.2584640681743622\n",
      "Step 5  Loss: 0.4258156716823578\n",
      "Step 6  Loss: 0.3165537416934967\n",
      "Step 7  Loss: 0.3298666775226593\n",
      "Step 8  Loss: 0.3486907184123993\n",
      "Step 9  Loss: 0.3252543807029724\n",
      "Step 10  Loss: 0.333229124546051\n",
      "total Loss of epoch  409  is  3.5839571356773376\n",
      "Step 0  Loss: 0.3490796983242035\n",
      "Step 1  Loss: 0.22974057495594025\n",
      "Step 2  Loss: 0.3828957974910736\n",
      "Step 3  Loss: 0.27134060859680176\n",
      "Step 4  Loss: 0.24968108534812927\n",
      "Step 5  Loss: 0.45017462968826294\n",
      "Step 6  Loss: 0.3978081941604614\n",
      "Step 7  Loss: 0.2519831657409668\n",
      "Step 8  Loss: 0.39642953872680664\n",
      "Step 9  Loss: 0.2893344759941101\n",
      "Step 10  Loss: 0.4267142713069916\n",
      "total Loss of epoch  410  is  3.695182040333748\n",
      "Step 0  Loss: 0.2667112946510315\n",
      "Step 1  Loss: 0.33804357051849365\n",
      "Step 2  Loss: 0.2720794081687927\n",
      "Step 3  Loss: 0.358177125453949\n",
      "Step 4  Loss: 0.30569180846214294\n",
      "Step 5  Loss: 0.2818523645401001\n",
      "Step 6  Loss: 0.33207085728645325\n",
      "Step 7  Loss: 0.26671209931373596\n",
      "Step 8  Loss: 0.3422595262527466\n",
      "Step 9  Loss: 0.2836519777774811\n",
      "Step 10  Loss: 0.24411648511886597\n",
      "total Loss of epoch  411  is  3.2913665175437927\n",
      "Step 0  Loss: 0.38315948843955994\n",
      "Step 1  Loss: 0.38486331701278687\n",
      "Step 2  Loss: 0.4409477710723877\n",
      "Step 3  Loss: 0.2926812767982483\n",
      "Step 4  Loss: 0.3221794366836548\n",
      "Step 5  Loss: 0.42822688817977905\n",
      "Step 6  Loss: 0.3195839524269104\n",
      "Step 7  Loss: 0.30286258459091187\n",
      "Step 8  Loss: 0.27490535378456116\n",
      "Step 9  Loss: 0.3154557943344116\n",
      "Step 10  Loss: 0.4307214319705963\n",
      "total Loss of epoch  412  is  3.895587295293808\n",
      "Step 0  Loss: 0.4069605767726898\n",
      "Step 1  Loss: 0.32632288336753845\n",
      "Step 2  Loss: 0.42781707644462585\n",
      "Step 3  Loss: 0.3219280540943146\n",
      "Step 4  Loss: 0.3289584219455719\n",
      "Step 5  Loss: 0.36740657687187195\n",
      "Step 6  Loss: 0.3200265169143677\n",
      "Step 7  Loss: 0.3623813986778259\n",
      "Step 8  Loss: 0.3793553411960602\n",
      "Step 9  Loss: 0.4654541611671448\n",
      "Step 10  Loss: 0.4719952940940857\n",
      "total Loss of epoch  413  is  4.178606301546097\n",
      "Step 0  Loss: 0.39309176802635193\n",
      "Step 1  Loss: 0.39718425273895264\n",
      "Step 2  Loss: 0.48571422696113586\n",
      "Step 3  Loss: 0.34241780638694763\n",
      "Step 4  Loss: 0.31629282236099243\n",
      "Step 5  Loss: 0.3811595141887665\n",
      "Step 6  Loss: 0.2758257985115051\n",
      "Step 7  Loss: 0.44303274154663086\n",
      "Step 8  Loss: 0.3056557774543762\n",
      "Step 9  Loss: 0.3613210618495941\n",
      "Step 10  Loss: 0.282056599855423\n",
      "total Loss of epoch  414  is  3.9837523698806763\n",
      "Step 0  Loss: 0.3955326974391937\n",
      "Step 1  Loss: 0.41174429655075073\n",
      "Step 2  Loss: 0.2751294672489166\n",
      "Step 3  Loss: 0.36058828234672546\n",
      "Step 4  Loss: 0.2704162895679474\n",
      "Step 5  Loss: 0.33844709396362305\n",
      "Step 6  Loss: 0.4320313632488251\n",
      "Step 7  Loss: 0.41181814670562744\n",
      "Step 8  Loss: 0.44054412841796875\n",
      "Step 9  Loss: 0.4511401057243347\n",
      "Step 10  Loss: 0.4496772289276123\n",
      "total Loss of epoch  415  is  4.237069100141525\n",
      "Step 0  Loss: 0.29892781376838684\n",
      "Step 1  Loss: 0.2626887261867523\n",
      "Step 2  Loss: 0.3015753924846649\n",
      "Step 3  Loss: 0.4044594466686249\n",
      "Step 4  Loss: 0.44695597887039185\n",
      "Step 5  Loss: 0.4268761873245239\n",
      "Step 6  Loss: 0.3472558557987213\n",
      "Step 7  Loss: 0.424549400806427\n",
      "Step 8  Loss: 0.25044330954551697\n",
      "Step 9  Loss: 0.3842766284942627\n",
      "Step 10  Loss: 0.2938559353351593\n",
      "total Loss of epoch  416  is  3.841864675283432\n",
      "Step 0  Loss: 0.4437647759914398\n",
      "Step 1  Loss: 0.40666496753692627\n",
      "Step 2  Loss: 0.29718348383903503\n",
      "Step 3  Loss: 0.357977956533432\n",
      "Step 4  Loss: 0.2834708094596863\n",
      "Step 5  Loss: 0.4284155070781708\n",
      "Step 6  Loss: 0.23478956520557404\n",
      "Step 7  Loss: 0.30386123061180115\n",
      "Step 8  Loss: 0.34441182017326355\n",
      "Step 9  Loss: 0.38953131437301636\n",
      "Step 10  Loss: 0.2333071231842041\n",
      "total Loss of epoch  417  is  3.7233785539865494\n",
      "Step 0  Loss: 0.37079909443855286\n",
      "Step 1  Loss: 0.3037082850933075\n",
      "Step 2  Loss: 0.3977367877960205\n",
      "Step 3  Loss: 0.3924354016780853\n",
      "Step 4  Loss: 0.3114306926727295\n",
      "Step 5  Loss: 0.44369083642959595\n",
      "Step 6  Loss: 0.3651794195175171\n",
      "Step 7  Loss: 0.3076794445514679\n",
      "Step 8  Loss: 0.3417786955833435\n",
      "Step 9  Loss: 0.31041327118873596\n",
      "Step 10  Loss: 0.2937384247779846\n",
      "total Loss of epoch  418  is  3.8385903537273407\n",
      "Step 0  Loss: 0.33197805285453796\n",
      "Step 1  Loss: 0.3320498466491699\n",
      "Step 2  Loss: 0.3641141355037689\n",
      "Step 3  Loss: 0.362951397895813\n",
      "Step 4  Loss: 0.3469313681125641\n",
      "Step 5  Loss: 0.2564217746257782\n",
      "Step 6  Loss: 0.31656670570373535\n",
      "Step 7  Loss: 0.3529633581638336\n",
      "Step 8  Loss: 0.4188081920146942\n",
      "Step 9  Loss: 0.39363449811935425\n",
      "Step 10  Loss: 0.12300237268209457\n",
      "total Loss of epoch  419  is  3.599421702325344\n",
      "Step 0  Loss: 0.49400582909584045\n",
      "Step 1  Loss: 0.4729853570461273\n",
      "Step 2  Loss: 0.36631155014038086\n",
      "Step 3  Loss: 0.4288736581802368\n",
      "Step 4  Loss: 0.428210973739624\n",
      "Step 5  Loss: 0.2659309506416321\n",
      "Step 6  Loss: 0.34261733293533325\n",
      "Step 7  Loss: 0.3471281826496124\n",
      "Step 8  Loss: 0.23274189233779907\n",
      "Step 9  Loss: 0.26848599314689636\n",
      "Step 10  Loss: 0.4785062074661255\n",
      "total Loss of epoch  420  is  4.125797927379608\n",
      "Step 0  Loss: 0.3618943989276886\n",
      "Step 1  Loss: 0.3764326870441437\n",
      "Step 2  Loss: 0.3762179911136627\n",
      "Step 3  Loss: 0.3844843804836273\n",
      "Step 4  Loss: 0.4053487479686737\n",
      "Step 5  Loss: 0.4390155076980591\n",
      "Step 6  Loss: 0.3562379479408264\n",
      "Step 7  Loss: 0.31123682856559753\n",
      "Step 8  Loss: 0.23415686190128326\n",
      "Step 9  Loss: 0.35233479738235474\n",
      "Step 10  Loss: 0.3874330222606659\n",
      "total Loss of epoch  421  is  3.984793171286583\n",
      "Step 0  Loss: 0.42543017864227295\n",
      "Step 1  Loss: 0.39240822196006775\n",
      "Step 2  Loss: 0.33262738585472107\n",
      "Step 3  Loss: 0.4228545129299164\n",
      "Step 4  Loss: 0.3735957145690918\n",
      "Step 5  Loss: 0.37875398993492126\n",
      "Step 6  Loss: 0.2525273561477661\n",
      "Step 7  Loss: 0.3888343572616577\n",
      "Step 8  Loss: 0.37648439407348633\n",
      "Step 9  Loss: 0.36221903562545776\n",
      "Step 10  Loss: 0.4522227942943573\n",
      "total Loss of epoch  422  is  4.157957941293716\n",
      "Step 0  Loss: 0.21835917234420776\n",
      "Step 1  Loss: 0.38527458906173706\n",
      "Step 2  Loss: 0.28335392475128174\n",
      "Step 3  Loss: 0.36436164379119873\n",
      "Step 4  Loss: 0.3350573480129242\n",
      "Step 5  Loss: 0.2915879786014557\n",
      "Step 6  Loss: 0.42586126923561096\n",
      "Step 7  Loss: 0.2883670926094055\n",
      "Step 8  Loss: 0.32116180658340454\n",
      "Step 9  Loss: 0.33563685417175293\n",
      "Step 10  Loss: 0.1761486679315567\n",
      "total Loss of epoch  423  is  3.425170347094536\n",
      "Step 0  Loss: 0.4351588189601898\n",
      "Step 1  Loss: 0.3625207245349884\n",
      "Step 2  Loss: 0.43964117765426636\n",
      "Step 3  Loss: 0.3991232216358185\n",
      "Step 4  Loss: 0.3056996762752533\n",
      "Step 5  Loss: 0.431389182806015\n",
      "Step 6  Loss: 0.1930694431066513\n",
      "Step 7  Loss: 0.3370877802371979\n",
      "Step 8  Loss: 0.46394264698028564\n",
      "Step 9  Loss: 0.3830402195453644\n",
      "Step 10  Loss: 0.41298720240592957\n",
      "total Loss of epoch  424  is  4.16366009414196\n",
      "Step 0  Loss: 0.3224869668483734\n",
      "Step 1  Loss: 0.41840726137161255\n",
      "Step 2  Loss: 0.32034268975257874\n",
      "Step 3  Loss: 0.3328876197338104\n",
      "Step 4  Loss: 0.27734923362731934\n",
      "Step 5  Loss: 0.37808358669281006\n",
      "Step 6  Loss: 0.34260493516921997\n",
      "Step 7  Loss: 0.4003872871398926\n",
      "Step 8  Loss: 0.2724631726741791\n",
      "Step 9  Loss: 0.4015257656574249\n",
      "Step 10  Loss: 0.24958132207393646\n",
      "total Loss of epoch  425  is  3.7161198407411575\n",
      "Step 0  Loss: 0.3031046390533447\n",
      "Step 1  Loss: 0.37988749146461487\n",
      "Step 2  Loss: 0.34117379784584045\n",
      "Step 3  Loss: 0.2973170280456543\n",
      "Step 4  Loss: 0.40911558270454407\n",
      "Step 5  Loss: 0.34954649209976196\n",
      "Step 6  Loss: 0.38922467827796936\n",
      "Step 7  Loss: 0.3836929202079773\n",
      "Step 8  Loss: 0.19301427900791168\n",
      "Step 9  Loss: 0.27040690183639526\n",
      "Step 10  Loss: 0.23557844758033752\n",
      "total Loss of epoch  426  is  3.5520622581243515\n",
      "Step 0  Loss: 0.3668799102306366\n",
      "Step 1  Loss: 0.3365272581577301\n",
      "Step 2  Loss: 0.3761046528816223\n",
      "Step 3  Loss: 0.2970028817653656\n",
      "Step 4  Loss: 0.3009665310382843\n",
      "Step 5  Loss: 0.3151416778564453\n",
      "Step 6  Loss: 0.3245910406112671\n",
      "Step 7  Loss: 0.24363626539707184\n",
      "Step 8  Loss: 0.41744205355644226\n",
      "Step 9  Loss: 0.3245293200016022\n",
      "Step 10  Loss: 0.3089614510536194\n",
      "total Loss of epoch  427  is  3.611783042550087\n",
      "Step 0  Loss: 0.28735849261283875\n",
      "Step 1  Loss: 0.23882216215133667\n",
      "Step 2  Loss: 0.3079480230808258\n",
      "Step 3  Loss: 0.2858986556529999\n",
      "Step 4  Loss: 0.34842655062675476\n",
      "Step 5  Loss: 0.27940893173217773\n",
      "Step 6  Loss: 0.36108624935150146\n",
      "Step 7  Loss: 0.4317913055419922\n",
      "Step 8  Loss: 0.36729416251182556\n",
      "Step 9  Loss: 0.3913983702659607\n",
      "Step 10  Loss: 0.30721908807754517\n",
      "total Loss of epoch  428  is  3.6066519916057587\n",
      "Step 0  Loss: 0.35440605878829956\n",
      "Step 1  Loss: 0.26031064987182617\n",
      "Step 2  Loss: 0.43277615308761597\n",
      "Step 3  Loss: 0.35640108585357666\n",
      "Step 4  Loss: 0.3695519268512726\n",
      "Step 5  Loss: 0.2675713300704956\n",
      "Step 6  Loss: 0.2861999571323395\n",
      "Step 7  Loss: 0.3646877408027649\n",
      "Step 8  Loss: 0.3957962393760681\n",
      "Step 9  Loss: 0.39145734906196594\n",
      "Step 10  Loss: 0.3533876836299896\n",
      "total Loss of epoch  429  is  3.8325461745262146\n",
      "Step 0  Loss: 0.4022636115550995\n",
      "Step 1  Loss: 0.25894391536712646\n",
      "Step 2  Loss: 0.3390387296676636\n",
      "Step 3  Loss: 0.2647671699523926\n",
      "Step 4  Loss: 0.3130638003349304\n",
      "Step 5  Loss: 0.3486413359642029\n",
      "Step 6  Loss: 0.20364709198474884\n",
      "Step 7  Loss: 0.3689012825489044\n",
      "Step 8  Loss: 0.2849912643432617\n",
      "Step 9  Loss: 0.2757954001426697\n",
      "Step 10  Loss: 0.49104562401771545\n",
      "total Loss of epoch  430  is  3.5510992258787155\n",
      "Step 0  Loss: 0.4477689564228058\n",
      "Step 1  Loss: 0.42230024933815\n",
      "Step 2  Loss: 0.3595444858074188\n",
      "Step 3  Loss: 0.3579895496368408\n",
      "Step 4  Loss: 0.42748722434043884\n",
      "Step 5  Loss: 0.3434644341468811\n",
      "Step 6  Loss: 0.28939974308013916\n",
      "Step 7  Loss: 0.3777647018432617\n",
      "Step 8  Loss: 0.3732970356941223\n",
      "Step 9  Loss: 0.23289160430431366\n",
      "Step 10  Loss: 0.3355964422225952\n",
      "total Loss of epoch  431  is  3.9675044268369675\n",
      "Step 0  Loss: 0.36741092801094055\n",
      "Step 1  Loss: 0.30891600251197815\n",
      "Step 2  Loss: 0.25938576459884644\n",
      "Step 3  Loss: 0.35872361063957214\n",
      "Step 4  Loss: 0.2685767114162445\n",
      "Step 5  Loss: 0.3308982253074646\n",
      "Step 6  Loss: 0.2395179122686386\n",
      "Step 7  Loss: 0.3478884696960449\n",
      "Step 8  Loss: 0.31943491101264954\n",
      "Step 9  Loss: 0.16947712004184723\n",
      "Step 10  Loss: 0.3915303349494934\n",
      "total Loss of epoch  432  is  3.36175999045372\n",
      "Step 0  Loss: 0.39441266655921936\n",
      "Step 1  Loss: 0.4045065939426422\n",
      "Step 2  Loss: 0.3405154347419739\n",
      "Step 3  Loss: 0.2765074670314789\n",
      "Step 4  Loss: 0.373654842376709\n",
      "Step 5  Loss: 0.45481088757514954\n",
      "Step 6  Loss: 0.29795804619789124\n",
      "Step 7  Loss: 0.40797582268714905\n",
      "Step 8  Loss: 0.5225362181663513\n",
      "Step 9  Loss: 0.37106645107269287\n",
      "Step 10  Loss: 0.46922242641448975\n",
      "total Loss of epoch  433  is  4.313166856765747\n",
      "Step 0  Loss: 0.3507712781429291\n",
      "Step 1  Loss: 0.31080055236816406\n",
      "Step 2  Loss: 0.28948071599006653\n",
      "Step 3  Loss: 0.3714853823184967\n",
      "Step 4  Loss: 0.5248478651046753\n",
      "Step 5  Loss: 0.31643539667129517\n",
      "Step 6  Loss: 0.4005375802516937\n",
      "Step 7  Loss: 0.30092811584472656\n",
      "Step 8  Loss: 0.3522931933403015\n",
      "Step 9  Loss: 0.3797553777694702\n",
      "Step 10  Loss: 0.4673404395580292\n",
      "total Loss of epoch  434  is  4.064675897359848\n",
      "Step 0  Loss: 0.2297438234090805\n",
      "Step 1  Loss: 0.2754640579223633\n",
      "Step 2  Loss: 0.36519303917884827\n",
      "Step 3  Loss: 0.3427981734275818\n",
      "Step 4  Loss: 0.5017145276069641\n",
      "Step 5  Loss: 0.32515475153923035\n",
      "Step 6  Loss: 0.39197760820388794\n",
      "Step 7  Loss: 0.24771340191364288\n",
      "Step 8  Loss: 0.27475622296333313\n",
      "Step 9  Loss: 0.29799023270606995\n",
      "Step 10  Loss: 0.19674766063690186\n",
      "total Loss of epoch  435  is  3.449253499507904\n",
      "Step 0  Loss: 0.40482670068740845\n",
      "Step 1  Loss: 0.28563886880874634\n",
      "Step 2  Loss: 0.4142783582210541\n",
      "Step 3  Loss: 0.3874121308326721\n",
      "Step 4  Loss: 0.28742411732673645\n",
      "Step 5  Loss: 0.33223047852516174\n",
      "Step 6  Loss: 0.3488754332065582\n",
      "Step 7  Loss: 0.21146169304847717\n",
      "Step 8  Loss: 0.28085610270500183\n",
      "Step 9  Loss: 0.33870118856430054\n",
      "Step 10  Loss: 0.49964797496795654\n",
      "total Loss of epoch  436  is  3.7913530468940735\n",
      "Step 0  Loss: 0.3382985293865204\n",
      "Step 1  Loss: 0.434976726770401\n",
      "Step 2  Loss: 0.48325100541114807\n",
      "Step 3  Loss: 0.30559736490249634\n",
      "Step 4  Loss: 0.3610040843486786\n",
      "Step 5  Loss: 0.38535842299461365\n",
      "Step 6  Loss: 0.22539488971233368\n",
      "Step 7  Loss: 0.4692429304122925\n",
      "Step 8  Loss: 0.2906965911388397\n",
      "Step 9  Loss: 0.3228440284729004\n",
      "Step 10  Loss: 0.4224843978881836\n",
      "total Loss of epoch  437  is  4.039148971438408\n",
      "Step 0  Loss: 0.38743484020233154\n",
      "Step 1  Loss: 0.3019639849662781\n",
      "Step 2  Loss: 0.3253431022167206\n",
      "Step 3  Loss: 0.3245012164115906\n",
      "Step 4  Loss: 0.23555074632167816\n",
      "Step 5  Loss: 0.2909030020236969\n",
      "Step 6  Loss: 0.36086133122444153\n",
      "Step 7  Loss: 0.41263750195503235\n",
      "Step 8  Loss: 0.3370562791824341\n",
      "Step 9  Loss: 0.3946095407009125\n",
      "Step 10  Loss: 0.2231741100549698\n",
      "total Loss of epoch  438  is  3.594035655260086\n",
      "Step 0  Loss: 0.327052503824234\n",
      "Step 1  Loss: 0.3358263075351715\n",
      "Step 2  Loss: 0.29337990283966064\n",
      "Step 3  Loss: 0.2901086211204529\n",
      "Step 4  Loss: 0.3247855603694916\n",
      "Step 5  Loss: 0.43843770027160645\n",
      "Step 6  Loss: 0.3477305769920349\n",
      "Step 7  Loss: 0.3122707009315491\n",
      "Step 8  Loss: 0.41799816489219666\n",
      "Step 9  Loss: 0.23103272914886475\n",
      "Step 10  Loss: 0.282787948846817\n",
      "total Loss of epoch  439  is  3.6014107167720795\n",
      "Step 0  Loss: 0.2400217056274414\n",
      "Step 1  Loss: 0.32902979850769043\n",
      "Step 2  Loss: 0.40248382091522217\n",
      "Step 3  Loss: 0.36946749687194824\n",
      "Step 4  Loss: 0.5358210206031799\n",
      "Step 5  Loss: 0.3034456670284271\n",
      "Step 6  Loss: 0.3963080048561096\n",
      "Step 7  Loss: 0.337263822555542\n",
      "Step 8  Loss: 0.2957395911216736\n",
      "Step 9  Loss: 0.2910153269767761\n",
      "Step 10  Loss: 0.26395678520202637\n",
      "total Loss of epoch  440  is  3.764553040266037\n",
      "Step 0  Loss: 0.3778904378414154\n",
      "Step 1  Loss: 0.23982524871826172\n",
      "Step 2  Loss: 0.31728869676589966\n",
      "Step 3  Loss: 0.3208297789096832\n",
      "Step 4  Loss: 0.4264870584011078\n",
      "Step 5  Loss: 0.4829025864601135\n",
      "Step 6  Loss: 0.26095324754714966\n",
      "Step 7  Loss: 0.23031435906887054\n",
      "Step 8  Loss: 0.3497558534145355\n",
      "Step 9  Loss: 0.2763173282146454\n",
      "Step 10  Loss: 0.27720028162002563\n",
      "total Loss of epoch  441  is  3.559764876961708\n",
      "Step 0  Loss: 0.42050546407699585\n",
      "Step 1  Loss: 0.3275080621242523\n",
      "Step 2  Loss: 0.32583436369895935\n",
      "Step 3  Loss: 0.3522520065307617\n",
      "Step 4  Loss: 0.3353157639503479\n",
      "Step 5  Loss: 0.2858334183692932\n",
      "Step 6  Loss: 0.3573083281517029\n",
      "Step 7  Loss: 0.2751598060131073\n",
      "Step 8  Loss: 0.395540714263916\n",
      "Step 9  Loss: 0.41728314757347107\n",
      "Step 10  Loss: 0.44832178950309753\n",
      "total Loss of epoch  442  is  3.940862864255905\n",
      "Step 0  Loss: 0.33741432428359985\n",
      "Step 1  Loss: 0.2370026558637619\n",
      "Step 2  Loss: 0.4268612265586853\n",
      "Step 3  Loss: 0.39007139205932617\n",
      "Step 4  Loss: 0.4095194935798645\n",
      "Step 5  Loss: 0.3181093633174896\n",
      "Step 6  Loss: 0.3755231499671936\n",
      "Step 7  Loss: 0.35642173886299133\n",
      "Step 8  Loss: 0.32896333932876587\n",
      "Step 9  Loss: 0.42082855105400085\n",
      "Step 10  Loss: 0.28419995307922363\n",
      "total Loss of epoch  443  is  3.8849151879549026\n",
      "Step 0  Loss: 0.23608073592185974\n",
      "Step 1  Loss: 0.3300747871398926\n",
      "Step 2  Loss: 0.3246332108974457\n",
      "Step 3  Loss: 0.32310160994529724\n",
      "Step 4  Loss: 0.38069555163383484\n",
      "Step 5  Loss: 0.47851938009262085\n",
      "Step 6  Loss: 0.36927440762519836\n",
      "Step 7  Loss: 0.26470711827278137\n",
      "Step 8  Loss: 0.2906256318092346\n",
      "Step 9  Loss: 0.28321292996406555\n",
      "Step 10  Loss: 0.3328629434108734\n",
      "total Loss of epoch  444  is  3.6137883067131042\n",
      "Step 0  Loss: 0.3077351748943329\n",
      "Step 1  Loss: 0.34450605511665344\n",
      "Step 2  Loss: 0.3598378896713257\n",
      "Step 3  Loss: 0.24600230157375336\n",
      "Step 4  Loss: 0.34889230132102966\n",
      "Step 5  Loss: 0.39211079478263855\n",
      "Step 6  Loss: 0.24560263752937317\n",
      "Step 7  Loss: 0.293942928314209\n",
      "Step 8  Loss: 0.30558472871780396\n",
      "Step 9  Loss: 0.37902596592903137\n",
      "Step 10  Loss: 0.4637303650379181\n",
      "total Loss of epoch  445  is  3.686971142888069\n",
      "Step 0  Loss: 0.30572277307510376\n",
      "Step 1  Loss: 0.37207892537117004\n",
      "Step 2  Loss: 0.3642455041408539\n",
      "Step 3  Loss: 0.43207940459251404\n",
      "Step 4  Loss: 0.2592310905456543\n",
      "Step 5  Loss: 0.2753533720970154\n",
      "Step 6  Loss: 0.3352573812007904\n",
      "Step 7  Loss: 0.3956149220466614\n",
      "Step 8  Loss: 0.3698139190673828\n",
      "Step 9  Loss: 0.3636394739151001\n",
      "Step 10  Loss: 0.2281440645456314\n",
      "total Loss of epoch  446  is  3.7011808305978775\n",
      "Step 0  Loss: 0.3464862108230591\n",
      "Step 1  Loss: 0.32686713337898254\n",
      "Step 2  Loss: 0.45072782039642334\n",
      "Step 3  Loss: 0.2753600478172302\n",
      "Step 4  Loss: 0.3405396640300751\n",
      "Step 5  Loss: 0.3297140896320343\n",
      "Step 6  Loss: 0.3872639536857605\n",
      "Step 7  Loss: 0.46990635991096497\n",
      "Step 8  Loss: 0.3348678648471832\n",
      "Step 9  Loss: 0.43107038736343384\n",
      "Step 10  Loss: 0.33034127950668335\n",
      "total Loss of epoch  447  is  4.0231448113918304\n",
      "Step 0  Loss: 0.35531964898109436\n",
      "Step 1  Loss: 0.32447296380996704\n",
      "Step 2  Loss: 0.2496788650751114\n",
      "Step 3  Loss: 0.44520920515060425\n",
      "Step 4  Loss: 0.45307132601737976\n",
      "Step 5  Loss: 0.327625572681427\n",
      "Step 6  Loss: 0.353963702917099\n",
      "Step 7  Loss: 0.4056464731693268\n",
      "Step 8  Loss: 0.2934509813785553\n",
      "Step 9  Loss: 0.3089236319065094\n",
      "Step 10  Loss: 0.4164723753929138\n",
      "total Loss of epoch  448  is  3.933834746479988\n",
      "Step 0  Loss: 0.37383389472961426\n",
      "Step 1  Loss: 0.3741086721420288\n",
      "Step 2  Loss: 0.44853225350379944\n",
      "Step 3  Loss: 0.3417139947414398\n",
      "Step 4  Loss: 0.3716224431991577\n",
      "Step 5  Loss: 0.2993517518043518\n",
      "Step 6  Loss: 0.2674136161804199\n",
      "Step 7  Loss: 0.37909606099128723\n",
      "Step 8  Loss: 0.3803858757019043\n",
      "Step 9  Loss: 0.3489203155040741\n",
      "Step 10  Loss: 0.3596629500389099\n",
      "total Loss of epoch  449  is  3.9446418285369873\n",
      "Step 0  Loss: 0.3405629098415375\n",
      "Step 1  Loss: 0.38347527384757996\n",
      "Step 2  Loss: 0.35447847843170166\n",
      "Step 3  Loss: 0.3800060451030731\n",
      "Step 4  Loss: 0.3870212137699127\n",
      "Step 5  Loss: 0.34720224142074585\n",
      "Step 6  Loss: 0.32421982288360596\n",
      "Step 7  Loss: 0.3599502742290497\n",
      "Step 8  Loss: 0.2850589156150818\n",
      "Step 9  Loss: 0.39276596903800964\n",
      "Step 10  Loss: 0.32920464873313904\n",
      "total Loss of epoch  450  is  3.883945792913437\n",
      "Step 0  Loss: 0.32123127579689026\n",
      "Step 1  Loss: 0.40401551127433777\n",
      "Step 2  Loss: 0.2803194522857666\n",
      "Step 3  Loss: 0.24955716729164124\n",
      "Step 4  Loss: 0.37293025851249695\n",
      "Step 5  Loss: 0.3082890808582306\n",
      "Step 6  Loss: 0.35153502225875854\n",
      "Step 7  Loss: 0.28921231627464294\n",
      "Step 8  Loss: 0.35144972801208496\n",
      "Step 9  Loss: 0.3090212941169739\n",
      "Step 10  Loss: 0.1663375049829483\n",
      "total Loss of epoch  451  is  3.403898611664772\n",
      "Step 0  Loss: 0.3270685076713562\n",
      "Step 1  Loss: 0.3185237646102905\n",
      "Step 2  Loss: 0.3665741980075836\n",
      "Step 3  Loss: 0.34763869643211365\n",
      "Step 4  Loss: 0.4256342947483063\n",
      "Step 5  Loss: 0.3263416290283203\n",
      "Step 6  Loss: 0.3077472448348999\n",
      "Step 7  Loss: 0.43564894795417786\n",
      "Step 8  Loss: 0.33034393191337585\n",
      "Step 9  Loss: 0.2510285973548889\n",
      "Step 10  Loss: 0.39753496646881104\n",
      "total Loss of epoch  452  is  3.834084779024124\n",
      "Step 0  Loss: 0.48758840560913086\n",
      "Step 1  Loss: 0.3880271911621094\n",
      "Step 2  Loss: 0.3544653654098511\n",
      "Step 3  Loss: 0.3540523648262024\n",
      "Step 4  Loss: 0.28567132353782654\n",
      "Step 5  Loss: 0.3375154733657837\n",
      "Step 6  Loss: 0.4036300778388977\n",
      "Step 7  Loss: 0.2558545172214508\n",
      "Step 8  Loss: 0.3678267002105713\n",
      "Step 9  Loss: 0.29987406730651855\n",
      "Step 10  Loss: 0.42707112431526184\n",
      "total Loss of epoch  453  is  3.961576610803604\n",
      "Step 0  Loss: 0.2589513957500458\n",
      "Step 1  Loss: 0.28029632568359375\n",
      "Step 2  Loss: 0.3437311351299286\n",
      "Step 3  Loss: 0.47403138875961304\n",
      "Step 4  Loss: 0.2527078688144684\n",
      "Step 5  Loss: 0.28602170944213867\n",
      "Step 6  Loss: 0.2851932644844055\n",
      "Step 7  Loss: 0.33005625009536743\n",
      "Step 8  Loss: 0.3151366114616394\n",
      "Step 9  Loss: 0.2832089364528656\n",
      "Step 10  Loss: 0.43568551540374756\n",
      "total Loss of epoch  454  is  3.5450204014778137\n",
      "Step 0  Loss: 0.44878217577934265\n",
      "Step 1  Loss: 0.45304644107818604\n",
      "Step 2  Loss: 0.3113364279270172\n",
      "Step 3  Loss: 0.37862396240234375\n",
      "Step 4  Loss: 0.44953763484954834\n",
      "Step 5  Loss: 0.3206923007965088\n",
      "Step 6  Loss: 0.42243680357933044\n",
      "Step 7  Loss: 0.3200465142726898\n",
      "Step 8  Loss: 0.40629205107688904\n",
      "Step 9  Loss: 0.4479300081729889\n",
      "Step 10  Loss: 0.3219945430755615\n",
      "total Loss of epoch  455  is  4.2807188630104065\n",
      "Step 0  Loss: 0.23108574748039246\n",
      "Step 1  Loss: 0.27759695053100586\n",
      "Step 2  Loss: 0.35952767729759216\n",
      "Step 3  Loss: 0.38912466168403625\n",
      "Step 4  Loss: 0.4713408052921295\n",
      "Step 5  Loss: 0.3546813130378723\n",
      "Step 6  Loss: 0.3734956681728363\n",
      "Step 7  Loss: 0.3035048544406891\n",
      "Step 8  Loss: 0.36127471923828125\n",
      "Step 9  Loss: 0.4481523931026459\n",
      "Step 10  Loss: 0.3780246675014496\n",
      "total Loss of epoch  456  is  3.9478094577789307\n",
      "Step 0  Loss: 0.30953532457351685\n",
      "Step 1  Loss: 0.2748412489891052\n",
      "Step 2  Loss: 0.31348717212677\n",
      "Step 3  Loss: 0.49228575825691223\n",
      "Step 4  Loss: 0.2547098994255066\n",
      "Step 5  Loss: 0.29268762469291687\n",
      "Step 6  Loss: 0.35094207525253296\n",
      "Step 7  Loss: 0.48879823088645935\n",
      "Step 8  Loss: 0.3808433413505554\n",
      "Step 9  Loss: 0.3281918466091156\n",
      "Step 10  Loss: 0.4323035180568695\n",
      "total Loss of epoch  457  is  3.9186260402202606\n",
      "Step 0  Loss: 0.38692373037338257\n",
      "Step 1  Loss: 0.27899524569511414\n",
      "Step 2  Loss: 0.34924647212028503\n",
      "Step 3  Loss: 0.29207178950309753\n",
      "Step 4  Loss: 0.2509813904762268\n",
      "Step 5  Loss: 0.35605642199516296\n",
      "Step 6  Loss: 0.3684002459049225\n",
      "Step 7  Loss: 0.4266158938407898\n",
      "Step 8  Loss: 0.39918699860572815\n",
      "Step 9  Loss: 0.36155635118484497\n",
      "Step 10  Loss: 0.42459234595298767\n",
      "total Loss of epoch  458  is  3.894626885652542\n",
      "Step 0  Loss: 0.44883620738983154\n",
      "Step 1  Loss: 0.4175552725791931\n",
      "Step 2  Loss: 0.21516777575016022\n",
      "Step 3  Loss: 0.3707730174064636\n",
      "Step 4  Loss: 0.32670778036117554\n",
      "Step 5  Loss: 0.30918025970458984\n",
      "Step 6  Loss: 0.3822944462299347\n",
      "Step 7  Loss: 0.377440869808197\n",
      "Step 8  Loss: 0.3563920557498932\n",
      "Step 9  Loss: 0.4453430771827698\n",
      "Step 10  Loss: 0.32762807607650757\n",
      "total Loss of epoch  459  is  3.977318838238716\n",
      "Step 0  Loss: 0.30498450994491577\n",
      "Step 1  Loss: 0.33948561549186707\n",
      "Step 2  Loss: 0.41181033849716187\n",
      "Step 3  Loss: 0.34189528226852417\n",
      "Step 4  Loss: 0.26211655139923096\n",
      "Step 5  Loss: 0.4904994070529938\n",
      "Step 6  Loss: 0.30194634199142456\n",
      "Step 7  Loss: 0.5193811655044556\n",
      "Step 8  Loss: 0.26022329926490784\n",
      "Step 9  Loss: 0.3908979892730713\n",
      "Step 10  Loss: 0.27997928857803345\n",
      "total Loss of epoch  460  is  3.9032197892665863\n",
      "Step 0  Loss: 0.26320046186447144\n",
      "Step 1  Loss: 0.30953219532966614\n",
      "Step 2  Loss: 0.31232887506484985\n",
      "Step 3  Loss: 0.23454567790031433\n",
      "Step 4  Loss: 0.36702629923820496\n",
      "Step 5  Loss: 0.3279232382774353\n",
      "Step 6  Loss: 0.30897438526153564\n",
      "Step 7  Loss: 0.29858824610710144\n",
      "Step 8  Loss: 0.5000688433647156\n",
      "Step 9  Loss: 0.3541243076324463\n",
      "Step 10  Loss: 0.3407151699066162\n",
      "total Loss of epoch  461  is  3.617027699947357\n",
      "Step 0  Loss: 0.3884839713573456\n",
      "Step 1  Loss: 0.2540975511074066\n",
      "Step 2  Loss: 0.3579554557800293\n",
      "Step 3  Loss: 0.3355922996997833\n",
      "Step 4  Loss: 0.378925621509552\n",
      "Step 5  Loss: 0.24705851078033447\n",
      "Step 6  Loss: 0.31121233105659485\n",
      "Step 7  Loss: 0.3017001748085022\n",
      "Step 8  Loss: 0.3710848391056061\n",
      "Step 9  Loss: 0.2661094069480896\n",
      "Step 10  Loss: 0.406421959400177\n",
      "total Loss of epoch  462  is  3.618642121553421\n",
      "Step 0  Loss: 0.31517258286476135\n",
      "Step 1  Loss: 0.33191633224487305\n",
      "Step 2  Loss: 0.34006810188293457\n",
      "Step 3  Loss: 0.2271919995546341\n",
      "Step 4  Loss: 0.24738629162311554\n",
      "Step 5  Loss: 0.41401612758636475\n",
      "Step 6  Loss: 0.22345700860023499\n",
      "Step 7  Loss: 0.3283955454826355\n",
      "Step 8  Loss: 0.3006000220775604\n",
      "Step 9  Loss: 0.38929039239883423\n",
      "Step 10  Loss: 0.21885912120342255\n",
      "total Loss of epoch  463  is  3.336353525519371\n",
      "Step 0  Loss: 0.23187611997127533\n",
      "Step 1  Loss: 0.24388282001018524\n",
      "Step 2  Loss: 0.3279731869697571\n",
      "Step 3  Loss: 0.3662799000740051\n",
      "Step 4  Loss: 0.32179364562034607\n",
      "Step 5  Loss: 0.392455130815506\n",
      "Step 6  Loss: 0.37165841460227966\n",
      "Step 7  Loss: 0.37927043437957764\n",
      "Step 8  Loss: 0.33018261194229126\n",
      "Step 9  Loss: 0.3870682716369629\n",
      "Step 10  Loss: 0.1953958421945572\n",
      "total Loss of epoch  464  is  3.5478363782167435\n",
      "Step 0  Loss: 0.42397764325141907\n",
      "Step 1  Loss: 0.3595231771469116\n",
      "Step 2  Loss: 0.3185276985168457\n",
      "Step 3  Loss: 0.336433470249176\n",
      "Step 4  Loss: 0.3235562741756439\n",
      "Step 5  Loss: 0.43100157380104065\n",
      "Step 6  Loss: 0.3707093298435211\n",
      "Step 7  Loss: 0.43415769934654236\n",
      "Step 8  Loss: 0.3858620822429657\n",
      "Step 9  Loss: 0.38052135705947876\n",
      "Step 10  Loss: 0.43128064274787903\n",
      "total Loss of epoch  465  is  4.195550948381424\n",
      "Step 0  Loss: 0.24687416851520538\n",
      "Step 1  Loss: 0.4261893332004547\n",
      "Step 2  Loss: 0.22406570613384247\n",
      "Step 3  Loss: 0.34539738297462463\n",
      "Step 4  Loss: 0.2440633475780487\n",
      "Step 5  Loss: 0.3472268879413605\n",
      "Step 6  Loss: 0.3001876473426819\n",
      "Step 7  Loss: 0.37199175357818604\n",
      "Step 8  Loss: 0.27979978919029236\n",
      "Step 9  Loss: 0.4035325050354004\n",
      "Step 10  Loss: 0.4378061294555664\n",
      "total Loss of epoch  466  is  3.6271346509456635\n",
      "Step 0  Loss: 0.310025691986084\n",
      "Step 1  Loss: 0.3308241367340088\n",
      "Step 2  Loss: 0.43065473437309265\n",
      "Step 3  Loss: 0.3786625266075134\n",
      "Step 4  Loss: 0.24860155582427979\n",
      "Step 5  Loss: 0.3643622100353241\n",
      "Step 6  Loss: 0.4794137179851532\n",
      "Step 7  Loss: 0.3242235481739044\n",
      "Step 8  Loss: 0.301318496465683\n",
      "Step 9  Loss: 0.30563703179359436\n",
      "Step 10  Loss: 0.22595800459384918\n",
      "total Loss of epoch  467  is  3.699681654572487\n",
      "Step 0  Loss: 0.43776583671569824\n",
      "Step 1  Loss: 0.3898860812187195\n",
      "Step 2  Loss: 0.29781851172447205\n",
      "Step 3  Loss: 0.38203203678131104\n",
      "Step 4  Loss: 0.38965746760368347\n",
      "Step 5  Loss: 0.3674509525299072\n",
      "Step 6  Loss: 0.36971089243888855\n",
      "Step 7  Loss: 0.3165125846862793\n",
      "Step 8  Loss: 0.28672322630882263\n",
      "Step 9  Loss: 0.36046260595321655\n",
      "Step 10  Loss: 0.4147135317325592\n",
      "total Loss of epoch  468  is  4.012733727693558\n",
      "Step 0  Loss: 0.2312740683555603\n",
      "Step 1  Loss: 0.40865907073020935\n",
      "Step 2  Loss: 0.36243510246276855\n",
      "Step 3  Loss: 0.26871365308761597\n",
      "Step 4  Loss: 0.2807273864746094\n",
      "Step 5  Loss: 0.3542530834674835\n",
      "Step 6  Loss: 0.26098567247390747\n",
      "Step 7  Loss: 0.42628076672554016\n",
      "Step 8  Loss: 0.35347622632980347\n",
      "Step 9  Loss: 0.35260534286499023\n",
      "Step 10  Loss: 0.223203644156456\n",
      "total Loss of epoch  469  is  3.5226140171289444\n",
      "Step 0  Loss: 0.2864322066307068\n",
      "Step 1  Loss: 0.4414321780204773\n",
      "Step 2  Loss: 0.3377685546875\n",
      "Step 3  Loss: 0.1901084929704666\n",
      "Step 4  Loss: 0.3891426920890808\n",
      "Step 5  Loss: 0.309337854385376\n",
      "Step 6  Loss: 0.36765772104263306\n",
      "Step 7  Loss: 0.398642361164093\n",
      "Step 8  Loss: 0.328502893447876\n",
      "Step 9  Loss: 0.3841327428817749\n",
      "Step 10  Loss: 0.17009992897510529\n",
      "total Loss of epoch  470  is  3.6032576262950897\n",
      "Step 0  Loss: 0.4292154312133789\n",
      "Step 1  Loss: 0.3393069803714752\n",
      "Step 2  Loss: 0.21927332878112793\n",
      "Step 3  Loss: 0.39876359701156616\n",
      "Step 4  Loss: 0.29986265301704407\n",
      "Step 5  Loss: 0.32488900423049927\n",
      "Step 6  Loss: 0.456732839345932\n",
      "Step 7  Loss: 0.42836445569992065\n",
      "Step 8  Loss: 0.228847473859787\n",
      "Step 9  Loss: 0.3606034219264984\n",
      "Step 10  Loss: 0.33877456188201904\n",
      "total Loss of epoch  471  is  3.8246337473392487\n",
      "Step 0  Loss: 0.2884116768836975\n",
      "Step 1  Loss: 0.23279334604740143\n",
      "Step 2  Loss: 0.514295220375061\n",
      "Step 3  Loss: 0.3559257686138153\n",
      "Step 4  Loss: 0.29954102635383606\n",
      "Step 5  Loss: 0.3509994149208069\n",
      "Step 6  Loss: 0.4055815637111664\n",
      "Step 7  Loss: 0.5041508674621582\n",
      "Step 8  Loss: 0.34554538130760193\n",
      "Step 9  Loss: 0.41875791549682617\n",
      "Step 10  Loss: 0.38214996457099915\n",
      "total Loss of epoch  472  is  4.09815214574337\n",
      "Step 0  Loss: 0.39616766571998596\n",
      "Step 1  Loss: 0.32651668787002563\n",
      "Step 2  Loss: 0.4303548336029053\n",
      "Step 3  Loss: 0.2977932393550873\n",
      "Step 4  Loss: 0.2586507499217987\n",
      "Step 5  Loss: 0.24978037178516388\n",
      "Step 6  Loss: 0.3279286324977875\n",
      "Step 7  Loss: 0.30561646819114685\n",
      "Step 8  Loss: 0.40014779567718506\n",
      "Step 9  Loss: 0.2444656491279602\n",
      "Step 10  Loss: 0.2103235423564911\n",
      "total Loss of epoch  473  is  3.4477456361055374\n",
      "Step 0  Loss: 0.4595261514186859\n",
      "Step 1  Loss: 0.356388121843338\n",
      "Step 2  Loss: 0.4320757985115051\n",
      "Step 3  Loss: 0.3217315971851349\n",
      "Step 4  Loss: 0.3408910930156708\n",
      "Step 5  Loss: 0.2965622544288635\n",
      "Step 6  Loss: 0.3725097179412842\n",
      "Step 7  Loss: 0.29870450496673584\n",
      "Step 8  Loss: 0.33644217252731323\n",
      "Step 9  Loss: 0.28440505266189575\n",
      "Step 10  Loss: 0.3614529073238373\n",
      "total Loss of epoch  474  is  3.8606893718242645\n",
      "Step 0  Loss: 0.41836312413215637\n",
      "Step 1  Loss: 0.3597015142440796\n",
      "Step 2  Loss: 0.3058633506298065\n",
      "Step 3  Loss: 0.29280760884284973\n",
      "Step 4  Loss: 0.33624428510665894\n",
      "Step 5  Loss: 0.21085333824157715\n",
      "Step 6  Loss: 0.37050458788871765\n",
      "Step 7  Loss: 0.2655143141746521\n",
      "Step 8  Loss: 0.31670624017715454\n",
      "Step 9  Loss: 0.3541121184825897\n",
      "Step 10  Loss: 0.36407268047332764\n",
      "total Loss of epoch  475  is  3.59474316239357\n",
      "Step 0  Loss: 0.30336275696754456\n",
      "Step 1  Loss: 0.39026153087615967\n",
      "Step 2  Loss: 0.35152530670166016\n",
      "Step 3  Loss: 0.505549430847168\n",
      "Step 4  Loss: 0.2402803599834442\n",
      "Step 5  Loss: 0.27197781205177307\n",
      "Step 6  Loss: 0.40486258268356323\n",
      "Step 7  Loss: 0.349398136138916\n",
      "Step 8  Loss: 0.3370162546634674\n",
      "Step 9  Loss: 0.3291420042514801\n",
      "Step 10  Loss: 0.20384371280670166\n",
      "total Loss of epoch  476  is  3.687219887971878\n",
      "Step 0  Loss: 0.30278506875038147\n",
      "Step 1  Loss: 0.26264381408691406\n",
      "Step 2  Loss: 0.4537051022052765\n",
      "Step 3  Loss: 0.30466705560684204\n",
      "Step 4  Loss: 0.26837339997291565\n",
      "Step 5  Loss: 0.2091703712940216\n",
      "Step 6  Loss: 0.262712687253952\n",
      "Step 7  Loss: 0.32064199447631836\n",
      "Step 8  Loss: 0.3055177330970764\n",
      "Step 9  Loss: 0.283241331577301\n",
      "Step 10  Loss: 0.31150856614112854\n",
      "total Loss of epoch  477  is  3.2849671244621277\n",
      "Step 0  Loss: 0.43939661979675293\n",
      "Step 1  Loss: 0.38019102811813354\n",
      "Step 2  Loss: 0.377914160490036\n",
      "Step 3  Loss: 0.24676848948001862\n",
      "Step 4  Loss: 0.41097941994667053\n",
      "Step 5  Loss: 0.43150028586387634\n",
      "Step 6  Loss: 0.4453660547733307\n",
      "Step 7  Loss: 0.410926878452301\n",
      "Step 8  Loss: 0.2466842234134674\n",
      "Step 9  Loss: 0.32348984479904175\n",
      "Step 10  Loss: 0.42519035935401917\n",
      "total Loss of epoch  478  is  4.138407364487648\n",
      "Step 0  Loss: 0.32456138730049133\n",
      "Step 1  Loss: 0.4383431077003479\n",
      "Step 2  Loss: 0.3278631269931793\n",
      "Step 3  Loss: 0.3548501133918762\n",
      "Step 4  Loss: 0.37001240253448486\n",
      "Step 5  Loss: 0.2967156767845154\n",
      "Step 6  Loss: 0.3522496223449707\n",
      "Step 7  Loss: 0.34983453154563904\n",
      "Step 8  Loss: 0.4103432297706604\n",
      "Step 9  Loss: 0.387680321931839\n",
      "Step 10  Loss: 0.5062783360481262\n",
      "total Loss of epoch  479  is  4.11873185634613\n",
      "Step 0  Loss: 0.4315233528614044\n",
      "Step 1  Loss: 0.340717077255249\n",
      "Step 2  Loss: 0.3377060294151306\n",
      "Step 3  Loss: 0.40194571018218994\n",
      "Step 4  Loss: 0.36472365260124207\n",
      "Step 5  Loss: 0.3417212963104248\n",
      "Step 6  Loss: 0.33096572756767273\n",
      "Step 7  Loss: 0.2841013967990875\n",
      "Step 8  Loss: 0.23862013220787048\n",
      "Step 9  Loss: 0.3523366153240204\n",
      "Step 10  Loss: 0.44631898403167725\n",
      "total Loss of epoch  480  is  3.8706799745559692\n",
      "Step 0  Loss: 0.30880841612815857\n",
      "Step 1  Loss: 0.3472289443016052\n",
      "Step 2  Loss: 0.39858874678611755\n",
      "Step 3  Loss: 0.3002690076828003\n",
      "Step 4  Loss: 0.27168652415275574\n",
      "Step 5  Loss: 0.34686803817749023\n",
      "Step 6  Loss: 0.28843677043914795\n",
      "Step 7  Loss: 0.31486234068870544\n",
      "Step 8  Loss: 0.35903438925743103\n",
      "Step 9  Loss: 0.5139466524124146\n",
      "Step 10  Loss: 0.22393661737442017\n",
      "total Loss of epoch  481  is  3.6736664474010468\n",
      "Step 0  Loss: 0.35108163952827454\n",
      "Step 1  Loss: 0.3261950612068176\n",
      "Step 2  Loss: 0.2825842797756195\n",
      "Step 3  Loss: 0.4302786886692047\n",
      "Step 4  Loss: 0.266382098197937\n",
      "Step 5  Loss: 0.3308675289154053\n",
      "Step 6  Loss: 0.3103272616863251\n",
      "Step 7  Loss: 0.3852868974208832\n",
      "Step 8  Loss: 0.29469701647758484\n",
      "Step 9  Loss: 0.4298047721385956\n",
      "Step 10  Loss: 0.5263213515281677\n",
      "total Loss of epoch  482  is  3.933826595544815\n",
      "Step 0  Loss: 0.268576443195343\n",
      "Step 1  Loss: 0.3838730454444885\n",
      "Step 2  Loss: 0.3793726861476898\n",
      "Step 3  Loss: 0.37391337752342224\n",
      "Step 4  Loss: 0.4171673655509949\n",
      "Step 5  Loss: 0.2948096990585327\n",
      "Step 6  Loss: 0.3453996479511261\n",
      "Step 7  Loss: 0.23667322099208832\n",
      "Step 8  Loss: 0.35126858949661255\n",
      "Step 9  Loss: 0.25822365283966064\n",
      "Step 10  Loss: 0.16870057582855225\n",
      "total Loss of epoch  483  is  3.477978304028511\n",
      "Step 0  Loss: 0.2780734896659851\n",
      "Step 1  Loss: 0.3083057403564453\n",
      "Step 2  Loss: 0.27986210584640503\n",
      "Step 3  Loss: 0.4170311987400055\n",
      "Step 4  Loss: 0.2686882019042969\n",
      "Step 5  Loss: 0.4549793601036072\n",
      "Step 6  Loss: 0.36513376235961914\n",
      "Step 7  Loss: 0.2769019603729248\n",
      "Step 8  Loss: 0.3066355586051941\n",
      "Step 9  Loss: 0.35369253158569336\n",
      "Step 10  Loss: 0.21595898270606995\n",
      "total Loss of epoch  484  is  3.5252628922462463\n",
      "Step 0  Loss: 0.3181970417499542\n",
      "Step 1  Loss: 0.31029370427131653\n",
      "Step 2  Loss: 0.38309210538864136\n",
      "Step 3  Loss: 0.4222102165222168\n",
      "Step 4  Loss: 0.2985328733921051\n",
      "Step 5  Loss: 0.4630982279777527\n",
      "Step 6  Loss: 0.37514728307724\n",
      "Step 7  Loss: 0.29412978887557983\n",
      "Step 8  Loss: 0.2944258153438568\n",
      "Step 9  Loss: 0.2678501605987549\n",
      "Step 10  Loss: 0.29075396060943604\n",
      "total Loss of epoch  485  is  3.7177311778068542\n",
      "Step 0  Loss: 0.37530142068862915\n",
      "Step 1  Loss: 0.24197624623775482\n",
      "Step 2  Loss: 0.41285207867622375\n",
      "Step 3  Loss: 0.3859454393386841\n",
      "Step 4  Loss: 0.3585352599620819\n",
      "Step 5  Loss: 0.4217440187931061\n",
      "Step 6  Loss: 0.3906458020210266\n",
      "Step 7  Loss: 0.3533124029636383\n",
      "Step 8  Loss: 0.3232731819152832\n",
      "Step 9  Loss: 0.334526389837265\n",
      "Step 10  Loss: 0.3896137475967407\n",
      "total Loss of epoch  486  is  3.9877259880304337\n",
      "Step 0  Loss: 0.3423619866371155\n",
      "Step 1  Loss: 0.3365885019302368\n",
      "Step 2  Loss: 0.37834563851356506\n",
      "Step 3  Loss: 0.2846573293209076\n",
      "Step 4  Loss: 0.3418722450733185\n",
      "Step 5  Loss: 0.36246466636657715\n",
      "Step 6  Loss: 0.31749293208122253\n",
      "Step 7  Loss: 0.26593703031539917\n",
      "Step 8  Loss: 0.3861243426799774\n",
      "Step 9  Loss: 0.2959668040275574\n",
      "Step 10  Loss: 0.17600251734256744\n",
      "total Loss of epoch  487  is  3.4878139942884445\n",
      "Step 0  Loss: 0.3419264554977417\n",
      "Step 1  Loss: 0.33554738759994507\n",
      "Step 2  Loss: 0.5169601440429688\n",
      "Step 3  Loss: 0.30645307898521423\n",
      "Step 4  Loss: 0.36446133255958557\n",
      "Step 5  Loss: 0.33466780185699463\n",
      "Step 6  Loss: 0.31352531909942627\n",
      "Step 7  Loss: 0.44402679800987244\n",
      "Step 8  Loss: 0.4328850507736206\n",
      "Step 9  Loss: 0.3231474757194519\n",
      "Step 10  Loss: 0.4886849522590637\n",
      "total Loss of epoch  488  is  4.202285796403885\n",
      "Step 0  Loss: 0.3692432641983032\n",
      "Step 1  Loss: 0.38253289461135864\n",
      "Step 2  Loss: 0.31775081157684326\n",
      "Step 3  Loss: 0.3178085684776306\n",
      "Step 4  Loss: 0.29240715503692627\n",
      "Step 5  Loss: 0.4836120009422302\n",
      "Step 6  Loss: 0.34396690130233765\n",
      "Step 7  Loss: 0.29065999388694763\n",
      "Step 8  Loss: 0.3722473978996277\n",
      "Step 9  Loss: 0.41540786623954773\n",
      "Step 10  Loss: 0.41626253724098206\n",
      "total Loss of epoch  489  is  4.001899391412735\n",
      "Step 0  Loss: 0.35082197189331055\n",
      "Step 1  Loss: 0.2639729678630829\n",
      "Step 2  Loss: 0.26297348737716675\n",
      "Step 3  Loss: 0.2723390460014343\n",
      "Step 4  Loss: 0.31244024634361267\n",
      "Step 5  Loss: 0.24969376623630524\n",
      "Step 6  Loss: 0.34649649262428284\n",
      "Step 7  Loss: 0.4053725302219391\n",
      "Step 8  Loss: 0.3709775507450104\n",
      "Step 9  Loss: 0.274551659822464\n",
      "Step 10  Loss: 0.36571863293647766\n",
      "total Loss of epoch  490  is  3.4753583520650864\n",
      "Step 0  Loss: 0.21328137814998627\n",
      "Step 1  Loss: 0.32913047075271606\n",
      "Step 2  Loss: 0.437181681394577\n",
      "Step 3  Loss: 0.3053765594959259\n",
      "Step 4  Loss: 0.2969697415828705\n",
      "Step 5  Loss: 0.35098570585250854\n",
      "Step 6  Loss: 0.31453922390937805\n",
      "Step 7  Loss: 0.32842206954956055\n",
      "Step 8  Loss: 0.4346037805080414\n",
      "Step 9  Loss: 0.3469027280807495\n",
      "Step 10  Loss: 0.32771825790405273\n",
      "total Loss of epoch  491  is  3.6851115971803665\n",
      "Step 0  Loss: 0.4023738503456116\n",
      "Step 1  Loss: 0.31454816460609436\n",
      "Step 2  Loss: 0.39165353775024414\n",
      "Step 3  Loss: 0.3990303874015808\n",
      "Step 4  Loss: 0.3366136848926544\n",
      "Step 5  Loss: 0.34169745445251465\n",
      "Step 6  Loss: 0.2815517783164978\n",
      "Step 7  Loss: 0.2803126573562622\n",
      "Step 8  Loss: 0.2595774531364441\n",
      "Step 9  Loss: 0.43423399329185486\n",
      "Step 10  Loss: 0.21992690861225128\n",
      "total Loss of epoch  492  is  3.66151987016201\n",
      "Step 0  Loss: 0.20996040105819702\n",
      "Step 1  Loss: 0.36280080676078796\n",
      "Step 2  Loss: 0.3377772271633148\n",
      "Step 3  Loss: 0.2695773243904114\n",
      "Step 4  Loss: 0.35955366492271423\n",
      "Step 5  Loss: 0.35220903158187866\n",
      "Step 6  Loss: 0.4860979914665222\n",
      "Step 7  Loss: 0.34653937816619873\n",
      "Step 8  Loss: 0.41780075430870056\n",
      "Step 9  Loss: 0.3593784272670746\n",
      "Step 10  Loss: 0.2902444303035736\n",
      "total Loss of epoch  493  is  3.791939437389374\n",
      "Step 0  Loss: 0.30121076107025146\n",
      "Step 1  Loss: 0.3795779347419739\n",
      "Step 2  Loss: 0.3693462014198303\n",
      "Step 3  Loss: 0.34148332476615906\n",
      "Step 4  Loss: 0.4072306156158447\n",
      "Step 5  Loss: 0.39438503980636597\n",
      "Step 6  Loss: 0.41323748230934143\n",
      "Step 7  Loss: 0.40220731496810913\n",
      "Step 8  Loss: 0.35843518376350403\n",
      "Step 9  Loss: 0.3681766390800476\n",
      "Step 10  Loss: 0.16607147455215454\n",
      "total Loss of epoch  494  is  3.901361972093582\n",
      "Step 0  Loss: 0.4556502401828766\n",
      "Step 1  Loss: 0.39436841011047363\n",
      "Step 2  Loss: 0.4043255150318146\n",
      "Step 3  Loss: 0.3887576758861542\n",
      "Step 4  Loss: 0.2832953929901123\n",
      "Step 5  Loss: 0.31527945399284363\n",
      "Step 6  Loss: 0.4667431712150574\n",
      "Step 7  Loss: 0.37782952189445496\n",
      "Step 8  Loss: 0.2552926242351532\n",
      "Step 9  Loss: 0.2799307107925415\n",
      "Step 10  Loss: 0.3609920144081116\n",
      "total Loss of epoch  495  is  3.9824647307395935\n",
      "Step 0  Loss: 0.3691807985305786\n",
      "Step 1  Loss: 0.3557356894016266\n",
      "Step 2  Loss: 0.44931742548942566\n",
      "Step 3  Loss: 0.31571510434150696\n",
      "Step 4  Loss: 0.3486262261867523\n",
      "Step 5  Loss: 0.44289615750312805\n",
      "Step 6  Loss: 0.33005082607269287\n",
      "Step 7  Loss: 0.30864188075065613\n",
      "Step 8  Loss: 0.2656821608543396\n",
      "Step 9  Loss: 0.3326801061630249\n",
      "Step 10  Loss: 0.2568884789943695\n",
      "total Loss of epoch  496  is  3.775414854288101\n",
      "Step 0  Loss: 0.2874770164489746\n",
      "Step 1  Loss: 0.370571106672287\n",
      "Step 2  Loss: 0.3636833727359772\n",
      "Step 3  Loss: 0.2662474513053894\n",
      "Step 4  Loss: 0.466450572013855\n",
      "Step 5  Loss: 0.29369235038757324\n",
      "Step 6  Loss: 0.3318404257297516\n",
      "Step 7  Loss: 0.41011741757392883\n",
      "Step 8  Loss: 0.4180482029914856\n",
      "Step 9  Loss: 0.39109867811203003\n",
      "Step 10  Loss: 0.4864908754825592\n",
      "total Loss of epoch  497  is  4.085717469453812\n",
      "Step 0  Loss: 0.38198718428611755\n",
      "Step 1  Loss: 0.3110477924346924\n",
      "Step 2  Loss: 0.26602673530578613\n",
      "Step 3  Loss: 0.32149970531463623\n",
      "Step 4  Loss: 0.31714928150177\n",
      "Step 5  Loss: 0.3322109878063202\n",
      "Step 6  Loss: 0.322716623544693\n",
      "Step 7  Loss: 0.36486560106277466\n",
      "Step 8  Loss: 0.2983742952346802\n",
      "Step 9  Loss: 0.2859300673007965\n",
      "Step 10  Loss: 0.21046200394630432\n",
      "total Loss of epoch  498  is  3.412270277738571\n",
      "Step 0  Loss: 0.3019324243068695\n",
      "Step 1  Loss: 0.3392370939254761\n",
      "Step 2  Loss: 0.25244829058647156\n",
      "Step 3  Loss: 0.3937174677848816\n",
      "Step 4  Loss: 0.3743448853492737\n",
      "Step 5  Loss: 0.35356685519218445\n",
      "Step 6  Loss: 0.30776479840278625\n",
      "Step 7  Loss: 0.37539878487586975\n",
      "Step 8  Loss: 0.32646340131759644\n",
      "Step 9  Loss: 0.42536842823028564\n",
      "Step 10  Loss: 0.2936668395996094\n",
      "total Loss of epoch  499  is  3.7439092695713043\n",
      "Step 0  Loss: 0.2941666543483734\n",
      "Step 1  Loss: 0.3182159960269928\n",
      "Step 2  Loss: 0.36625936627388\n",
      "Step 3  Loss: 0.37336450815200806\n",
      "Step 4  Loss: 0.24581976234912872\n",
      "Step 5  Loss: 0.38571739196777344\n",
      "Step 6  Loss: 0.2939937114715576\n",
      "Step 7  Loss: 0.3785688579082489\n",
      "Step 8  Loss: 0.3565000593662262\n",
      "Step 9  Loss: 0.3732580840587616\n",
      "Step 10  Loss: 0.425606906414032\n",
      "total Loss of epoch  500  is  3.8114712983369827\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(n_epochs)):\n",
    "    spike_gen_obj = get_batches(real_train_trial_spikes_stand,batch_size)\n",
    "    emg_gen_obj = get_batches(real_train_trial_vel_tide,batch_size)\n",
    "    for ii in range(n_batches):\n",
    "        optimizer.zero_grad()\n",
    "        spike_batch = next(spike_gen_obj)\n",
    "        emg_batch = next(emg_gen_obj)\n",
    "\n",
    "        spike_batch = Variable(torch.from_numpy(spike_batch)).float()\n",
    "        emg_batch = Variable(torch.from_numpy(emg_batch)).float()\n",
    "\n",
    "        # Loss\n",
    "        batch_loss = get_loss(model, spike_batch, emg_batch)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_total_loss = get_loss(model, spike_val, emg_val)\n",
    "        vae_loss_list.append(val_total_loss.item())\n",
    "\n",
    "        _, _, train_latents, _ = model(spike_train, train_flag = False)\n",
    "\n",
    "        if val_total_loss < pre_total_loss_: \n",
    "            pre_total_loss_ = val_total_loss\n",
    "            torch.save(model.state_dict(),'source_vae_model')\n",
    "\n",
    "            np.save(\"train_latents.npy\",train_latents)\n",
    "\n",
    "        \n",
    "    train_latents = np.expand_dims(train_latents,1).astype(np.float32)\n",
    "    train_spike_data = train_latents.transpose(0,1,3,2)\n",
    "\n",
    "    dataloader = DataLoader(train_spike_data, batch_size=global_batch_size)\n",
    "\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        dm_optimizer.zero_grad()\n",
    "\n",
    "        batch_size = batch.shape[0]\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # generate diffusion timesteps (noise scale) for each batch from range\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "        loss = p_losses(dm_model, batch, t)\n",
    "\n",
    "        print(\"Step\", step, \" Loss:\", loss.item())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        dm_optimizer.step()\n",
    "\n",
    "    print(\"total Loss of epoch \", epoch+1, \" is \", total_loss)\n",
    "    diff_loss_list.append(total_loss)\n",
    "\n",
    "    if total_loss < pre_loss:\n",
    "        pre_loss = total_loss\n",
    "        torch.save(dm_model.state_dict(), 'source_diffusion_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target domain: Maximum Likelihood Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the data and preparing (same as in source domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_trial, num_neurons_s, num_neurons_t = 37, 187, 172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/Neural_Target.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)['data']\n",
    "test_trial_spikes, test_trial_vel, test_trial_dir = test_data['firing_rates'], test_data['velocity'], np.squeeze(test_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for test neural signal: (184, 37, 172)\n",
      "Shape for test velocity: (184, 37, 2)\n"
     ]
    }
   ],
   "source": [
    "test_trial_spikes_tide = np.array([spike[:len_trial, :num_neurons_t] for spike in test_trial_spikes])\n",
    "print(\"Shape for test neural signal:\", np.shape(test_trial_spikes_tide))\n",
    "\n",
    "test_trial_vel_tide = np.array([spike[:len_trial, :] for spike in test_trial_vel])\n",
    "print(\"Shape for test velocity:\",np.shape(test_trial_vel_tide))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trial_spikes_smoothed = np.apply_along_axis(filt, 1, test_trial_spikes_tide)\n",
    "test_trial_spikes_stand = (test_trial_spikes_smoothed)\n",
    "spike_test = Variable(torch.from_numpy(test_trial_spikes_stand)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trial_spikes_stand_half_len = len(test_trial_spikes_stand) // 2\n",
    "\n",
    "spike_day_0 = Variable(torch.from_numpy(real_train_trial_spikes_stand)).float()\n",
    "spike_day_k = Variable(torch.from_numpy(test_trial_spikes_stand[:test_trial_spikes_stand_half_len])).float()\n",
    "\n",
    "num_x, num_y, num_y_test = spike_day_0.shape[0], spike_day_k.shape[0], test_trial_spikes_stand.shape[0]\n",
    "\n",
    "p = Variable(torch.from_numpy(np.full((num_x, 1), 1 / num_x))).float()\n",
    "q = Variable(torch.from_numpy(np.full((num_y, 1), 1 / num_y))).float()\n",
    "q_test = Variable(torch.from_numpy(np.full((num_y_test, 1), 1 / num_y_test))).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "epoches = 300\n",
    "batch_size = 64\n",
    "timesteps = 50\n",
    "eps = 1 / timesteps\n",
    "channels = 1\n",
    "input_dim = 1\n",
    "pre_total_loss_ = 1e18\n",
    "l_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger_performance(model):\n",
    "    re_sp_test, vel_hat_test, _, _, _, _,_ = model(spike_train, spike_test, p, q_test, train_flag=False)\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    # print(test_trial_vel_tide.reshape((-1,2)))\n",
    "    # print(vel_hat_test.reshape((-1,2)))\n",
    "    key_metric = 100 * r2_score(test_trial_vel_tide.reshape((-1,2)),vel_hat_test.reshape((-1,2)), multioutput='uniform_average')\n",
    "    return  key_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_model = diff_STBlock(input_dim)\n",
    "diff_model_dict = torch.load('source_diffusion_model')\n",
    "diff_model.load_state_dict(diff_model_dict)\n",
    "for k,v in diff_model.named_parameters():\n",
    "    v.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(21)\n",
    "# vanilla_model_dict = torch.load('../model_checkpoints_old/source_vae_model')\n",
    "vanilla_model_dict = torch.load('source_vae_model')\n",
    "MLA_model = VAE_MLA_Model()\n",
    "MLA_dict_keys = MLA_model.state_dict().keys()\n",
    "vanilla_model_dict_keys = vanilla_model_dict.keys()\n",
    "\n",
    "MLA_dict_new = MLA_model.state_dict().copy()\n",
    "\n",
    "for key in vanilla_model_dict_keys:\n",
    "    MLA_dict_new[key] = vanilla_model_dict[key]\n",
    "\n",
    "MLA_model.load_state_dict(MLA_dict_new)\n",
    "\n",
    "optimizer_mla = torch.optim.Adam(MLA_model.parameters(), lr=l_rate)\n",
    "criterion = nn.MSELoss()\n",
    "poisson_criterion = nn.PoissonNLLLoss(log_input=False)\n",
    "\n",
    "for param in MLA_model.vde_rnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in MLA_model.sde_rnn.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in MLA_model.encoder_rnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "MLA_model.low_d_readin_s.weight.requires_grad = False\n",
    "MLA_model.low_d_readin_s.bias.requires_grad = False\n",
    "MLA_model.fc_mu_1.weight.requires_grad = False\n",
    "MLA_model.fc_mu_1.bias.requires_grad = False\n",
    "MLA_model.fc_log_var_1.weight.requires_grad = False\n",
    "MLA_model.fc_log_var_1.bias.requires_grad = False\n",
    "MLA_model.sde_fc1.weight.requires_grad = False\n",
    "MLA_model.sde_fc1.bias.requires_grad = False\n",
    "MLA_model.sde_fc2.weight.requires_grad = False\n",
    "MLA_model.sde_fc2.bias.requires_grad = False\n",
    "MLA_model.vde_fc_minus_0.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Maximum Likelihood Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_metric = -1000\n",
    "appro_alpha = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_mla.zero_grad()\n",
    "re_sp, _, distri_0, distri_k, latents_k, output_sh_loss, log_var = MLA_model(spike_day_0, spike_day_k, p, q, train_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4189572334289551"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sh_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([92, 1, 8, 37])\n"
     ]
    }
   ],
   "source": [
    "latents_k = latents_k[:, None, :, :]\n",
    "latents_k = torch.transpose(latents_k,3,2)\n",
    "print(latents_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = latents_k.shape[0]\n",
    "t = torch.randint(0, timesteps, (batch_size,), device=\"cpu\").long()\n",
    "noise = torch.randn_like(latents_k)\n",
    "\n",
    "z_noisy = q_sample(x_start=latents_k, t=t, noise=noise)\n",
    "predicted_noise = diff_model(z_noisy, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_loss = F.smooth_l1_loss(noise, predicted_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1560746729373932"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_div = skilling_divergence(z_noisy,latents_k,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006736140348948538"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_div.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = output_sh_loss + appro_alpha * noise_loss + skill_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6069204211235046"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss += 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update\n",
      "==================================================\n",
      "-->name low_d_readin_s.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[-0.0039, -0.2162, -0.1601,  ...,  0.0373, -0.0459,  0.0731],\n",
      "        [ 0.0006,  0.1748, -0.2171,  ..., -0.0218,  0.0857, -0.0485],\n",
      "        [ 0.0568, -0.3316, -0.0619,  ...,  0.0599,  0.1428, -0.0464],\n",
      "        ...,\n",
      "        [-0.0119,  0.2882,  0.1063,  ...,  0.0553,  0.1437,  0.0525],\n",
      "        [ 0.0071, -0.1596, -0.0646,  ..., -0.0429, -0.0746,  0.0046],\n",
      "        [-0.0015, -0.2137,  0.3009,  ...,  0.0314, -0.0358, -0.0514]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name low_d_readin_s.bias\n",
      "-->para Parameter containing:\n",
      "tensor([-0.0516, -0.0206,  0.0231, -0.0717,  0.0721, -0.0335, -0.0278,  0.0027,\n",
      "        -0.0643,  0.0031,  0.0318,  0.0148, -0.0155, -0.0365, -0.0099,  0.0119,\n",
      "         0.0318,  0.0140, -0.0493, -0.0264,  0.0728,  0.0486,  0.0071, -0.0132,\n",
      "         0.0428, -0.0333,  0.0057, -0.0414, -0.0076, -0.0301, -0.0214,  0.0436,\n",
      "        -0.0710,  0.0352, -0.0286, -0.0365,  0.0070, -0.0453, -0.0547,  0.0072,\n",
      "         0.0577, -0.0495,  0.0156,  0.0723,  0.0216,  0.0031,  0.0538,  0.0111,\n",
      "        -0.0642,  0.0234, -0.0615,  0.0485,  0.0648,  0.0640,  0.0107,  0.0463,\n",
      "         0.0467, -0.0033,  0.0160, -0.0331, -0.0302,  0.0499, -0.0191, -0.0308])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name align_layer.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], requires_grad=True)\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name low_d_readin_t.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[ 0.0572,  0.0277, -0.0072,  ..., -0.0522, -0.0595, -0.0531],\n",
      "        [ 0.0758, -0.0700, -0.0206,  ...,  0.0547, -0.0258,  0.0046],\n",
      "        [ 0.0495, -0.0705,  0.0025,  ..., -0.0707,  0.0159, -0.0170],\n",
      "        ...,\n",
      "        [ 0.0079, -0.0747,  0.0415,  ..., -0.0498,  0.0529, -0.0508],\n",
      "        [-0.0671,  0.0756, -0.0479,  ...,  0.0631,  0.0288,  0.0395],\n",
      "        [ 0.0576, -0.0663, -0.0248,  ...,  0.0749,  0.0282,  0.0652]],\n",
      "       requires_grad=True)\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name low_d_readin_t.bias\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.0072, -0.0294, -0.0444,  0.0418,  0.0102, -0.0367, -0.0279, -0.0673,\n",
      "         0.0431,  0.0401, -0.0487,  0.0475, -0.0473,  0.0206, -0.0616, -0.0234,\n",
      "        -0.0537, -0.0529,  0.0143, -0.0406,  0.0313, -0.0406,  0.0117,  0.0760,\n",
      "         0.0152,  0.0681,  0.0435, -0.0715, -0.0154, -0.0249, -0.0100,  0.0315,\n",
      "        -0.0368,  0.0015,  0.0755,  0.0669, -0.0294,  0.0227,  0.0243,  0.0542,\n",
      "        -0.0525,  0.0156, -0.0708,  0.0119,  0.0173, -0.0659,  0.0005,  0.0085,\n",
      "        -0.0225,  0.0724,  0.0352, -0.0423, -0.0528,  0.0708, -0.0544, -0.0147,\n",
      "         0.0129,  0.0622,  0.0376, -0.0236,  0.0517,  0.0610, -0.0739,  0.0268],\n",
      "       requires_grad=True)\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_ih_l0\n",
      "-->para Parameter containing:\n",
      "tensor([[-0.0134,  0.0916, -0.0020,  ..., -0.0212,  0.0595, -0.0080],\n",
      "        [-0.0157, -0.0360,  0.0248,  ..., -0.0099, -0.0531, -0.0884],\n",
      "        [ 0.0238,  0.0006,  0.0255,  ..., -0.0272,  0.0133, -0.0089],\n",
      "        ...,\n",
      "        [ 0.0975,  0.0034, -0.0399,  ..., -0.0926,  0.0113, -0.1034],\n",
      "        [ 0.1064,  0.0181, -0.0073,  ..., -0.0056, -0.0293, -0.0595],\n",
      "        [-0.0489, -0.0498, -0.0287,  ...,  0.0220,  0.0114,  0.0056]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_hh_l0\n",
      "-->para Parameter containing:\n",
      "tensor([[ 2.0702e-01,  1.0262e-02, -1.3230e-02,  ...,  6.5496e-02,\n",
      "         -2.2155e-04,  4.5340e-02],\n",
      "        [ 2.1887e-02,  5.4503e-01,  9.3500e-02,  ...,  8.5970e-02,\n",
      "          7.5593e-02,  1.0993e-02],\n",
      "        [-6.9786e-02,  3.4432e-02,  4.9471e-01,  ...,  8.8240e-02,\n",
      "         -3.4073e-02,  1.3374e-01],\n",
      "        ...,\n",
      "        [-3.4031e-02,  1.1862e-01,  1.9329e-02,  ...,  5.1210e-01,\n",
      "         -7.0697e-02,  6.9892e-03],\n",
      "        [-1.1759e-01,  1.6307e-01,  9.5114e-02,  ...,  1.9165e-02,\n",
      "          5.6912e-01, -9.4375e-02],\n",
      "        [ 7.9353e-03, -3.1120e-02,  6.8015e-02,  ..., -8.0276e-03,\n",
      "         -8.2205e-02,  5.5421e-01]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_ih_l0\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.1826, -0.0020,  0.1948,  0.0986,  0.1886,  0.0736, -0.2202, -0.1615,\n",
      "         0.0605,  0.0897,  0.0416, -0.2581,  0.1895, -0.0255,  0.3332,  0.1566,\n",
      "        -0.0662, -0.1178,  0.0648,  0.2126, -0.0052, -0.1391,  0.2091, -0.1398,\n",
      "        -0.1463,  0.1287,  0.0445, -0.1856,  0.0220,  0.1410, -0.1914, -0.2153,\n",
      "         0.0344,  0.2187,  0.2534,  0.1944,  0.0611, -0.0892, -0.1848,  0.0528,\n",
      "        -0.0373, -0.0665, -0.0272,  0.0405,  0.1544, -0.1542,  0.1831,  0.2572,\n",
      "         0.0622,  0.0458, -0.1099, -0.0444, -0.1746,  0.0602,  0.0583, -0.0206,\n",
      "        -0.1856,  0.2040,  0.0081, -0.1424,  0.1757,  0.1526, -0.1112,  0.0664])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_hh_l0\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.0101,  0.0824,  0.1661,  0.1164,  0.2088,  0.0537, -0.1763, -0.2018,\n",
      "         0.1448,  0.2027,  0.0753, -0.1525,  0.1345, -0.0907,  0.1885, -0.0379,\n",
      "        -0.0030, -0.0318,  0.0576,  0.2008,  0.1643, -0.1544,  0.0533, -0.2433,\n",
      "        -0.0939, -0.0087, -0.1608, -0.2798,  0.0352,  0.1271, -0.1217,  0.0092,\n",
      "         0.0347,  0.0739,  0.1232,  0.1678,  0.0008, -0.0712, -0.2167,  0.0655,\n",
      "        -0.1766,  0.0847, -0.0701,  0.0753,  0.1675, -0.1489,  0.1931,  0.1463,\n",
      "         0.1913,  0.0286, -0.0413, -0.0416, -0.2230, -0.0573, -0.0077,  0.1611,\n",
      "        -0.0453,  0.1113,  0.1714, -0.2734,  0.2299,  0.2924, -0.0778,  0.1264])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_ih_l1\n",
      "-->para Parameter containing:\n",
      "tensor([[-0.0194,  0.0066,  0.0159,  ...,  0.0824, -0.0518,  0.0827],\n",
      "        [-0.0028,  0.0356, -0.0115,  ..., -0.0231, -0.0738, -0.1340],\n",
      "        [ 0.0249,  0.0464,  0.0084,  ..., -0.1216,  0.0035,  0.0173],\n",
      "        ...,\n",
      "        [ 0.0530,  0.0325,  0.0937,  ...,  0.0477,  0.0308, -0.0314],\n",
      "        [ 0.0475,  0.0387, -0.0347,  ...,  0.0799,  0.1040,  0.1302],\n",
      "        [-0.0337, -0.0103,  0.0085,  ...,  0.1230,  0.0039, -0.0111]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_hh_l1\n",
      "-->para Parameter containing:\n",
      "tensor([[ 0.5449,  0.0167, -0.1349,  ..., -0.0543, -0.0132,  0.1134],\n",
      "        [-0.0363,  0.1814,  0.0310,  ...,  0.0070, -0.0406, -0.0200],\n",
      "        [-0.1859, -0.0154,  0.1994,  ...,  0.0339,  0.0263, -0.1102],\n",
      "        ...,\n",
      "        [-0.0200, -0.0025,  0.0487,  ...,  0.2135,  0.0293, -0.0653],\n",
      "        [ 0.0554, -0.0557, -0.0086,  ..., -0.0078,  0.1781,  0.0370],\n",
      "        [ 0.1932,  0.0175, -0.1525,  ..., -0.0507, -0.0365,  0.2704]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_ih_l1\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.1178, -0.1524, -0.0340, -0.1432, -0.1152,  0.0387, -0.0913, -0.0327,\n",
      "         0.0622,  0.0997,  0.0427,  0.0558,  0.0583, -0.0033, -0.1456, -0.0501,\n",
      "        -0.0570,  0.0341,  0.1359, -0.1527,  0.0421, -0.1892, -0.1121,  0.0282,\n",
      "        -0.0401,  0.0156,  0.0531,  0.0375,  0.0811, -0.1211, -0.0330, -0.0319,\n",
      "         0.1584,  0.1148,  0.2184,  0.0803,  0.1343,  0.0129, -0.1474,  0.0564,\n",
      "         0.0711,  0.0644, -0.0649,  0.0945, -0.0402,  0.0132, -0.1285, -0.1222,\n",
      "         0.0778, -0.0380,  0.1158,  0.0726,  0.2041, -0.0075, -0.0830, -0.0382,\n",
      "         0.0537, -0.1821, -0.1764,  0.1886,  0.0038,  0.0352,  0.0564,  0.1388])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_hh_l1\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.1430, -0.1829, -0.0920, -0.0933, -0.1664, -0.0195,  0.0833, -0.0793,\n",
      "        -0.0836,  0.1906,  0.1553,  0.1248, -0.1091, -0.0719, -0.1413,  0.0540,\n",
      "         0.0249, -0.0169, -0.1039, -0.1445, -0.1209, -0.1928, -0.1818,  0.0251,\n",
      "         0.1202,  0.0861, -0.0894,  0.1827,  0.0457,  0.0922,  0.1629, -0.0143,\n",
      "        -0.0650, -0.0105,  0.2442,  0.1377, -0.0031, -0.0351, -0.0641, -0.0976,\n",
      "         0.0354, -0.0431, -0.1244,  0.0995, -0.1623,  0.0791, -0.0555, -0.0609,\n",
      "         0.1042,  0.1225,  0.0760,  0.1336,  0.1004,  0.1091,  0.0627,  0.0164,\n",
      "         0.0154, -0.1914, -0.2259,  0.0618, -0.0465,  0.0086,  0.2031,  0.0565])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_mu_1.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[ 1.4383e-02,  7.3843e-02,  3.5763e-02,  5.4814e-02,  1.5630e-01,\n",
      "          3.4752e-02,  1.0347e-01, -1.0470e-01, -1.4871e-01,  2.1275e-02,\n",
      "         -2.4680e-01,  1.9023e-01,  1.7840e-01, -2.2164e-01, -2.5779e-01,\n",
      "          1.7139e-01, -1.6139e-02, -2.0405e-02, -3.4354e-02, -2.3186e-01,\n",
      "          5.1328e-02, -7.0813e-02,  2.4151e-01,  1.5660e-02, -1.1229e-01,\n",
      "         -7.7352e-02, -4.6482e-02,  8.9058e-02,  7.3820e-03, -1.8741e-01,\n",
      "         -5.4002e-03,  2.5471e-02,  9.1554e-02, -1.7124e-01, -1.8833e-01,\n",
      "          1.8278e-01, -2.0753e-01, -9.6388e-02,  1.3156e-01, -1.1293e-01,\n",
      "         -7.4746e-02, -1.3814e-01,  1.1201e-01,  1.5717e-01,  1.4389e-01,\n",
      "         -3.3948e-01,  1.1297e-01, -3.2882e-03,  1.2112e-01,  1.9435e-01,\n",
      "          3.4827e-02, -2.8408e-02, -9.1491e-02,  5.0382e-02, -2.3494e-01,\n",
      "          1.2807e-01, -1.6551e-01, -1.8462e-01,  1.0617e-01,  1.1131e-01,\n",
      "         -2.2944e-02, -1.5188e-01, -1.7614e-01, -6.4726e-02],\n",
      "        [ 3.7072e-02,  2.0673e-03, -8.3480e-02, -8.0442e-02,  1.1285e-01,\n",
      "          5.7430e-02, -8.5355e-02,  1.1049e-02,  1.7979e-01,  2.8120e-01,\n",
      "          1.9047e-01,  3.3229e-02, -1.1638e-01, -7.2826e-02,  1.5401e-01,\n",
      "         -3.3200e-02,  2.3199e-02,  3.3437e-02,  7.7822e-03,  7.3328e-02,\n",
      "         -2.0148e-01,  9.2268e-02,  2.1775e-02,  8.2851e-02,  1.9979e-01,\n",
      "         -1.6750e-01,  2.0064e-01,  2.0752e-01,  2.1509e-01,  1.8114e-01,\n",
      "         -1.2825e-01,  2.0216e-01,  6.6793e-03,  2.4219e-02,  9.3954e-02,\n",
      "         -4.9525e-02,  9.8099e-02, -6.6221e-02,  1.2363e-01, -3.5384e-02,\n",
      "         -3.6751e-02, -1.3780e-01,  9.2094e-02,  7.3905e-02, -7.7715e-02,\n",
      "          3.1684e-01, -1.6821e-01, -1.3650e-01,  1.6238e-01,  9.7136e-02,\n",
      "         -2.6477e-01, -2.1953e-01,  1.3093e-01,  9.5456e-02, -2.8139e-01,\n",
      "          2.1141e-01,  5.0936e-02, -7.0080e-02, -1.5203e-01, -7.8332e-03,\n",
      "          1.9997e-01, -1.0319e-01, -4.7765e-02,  1.7961e-01],\n",
      "        [-1.8853e-01,  1.0716e-01,  1.3169e-01, -1.3531e-02, -1.5628e-02,\n",
      "         -7.0479e-02,  2.1436e-01,  4.7802e-02,  2.2263e-04, -4.4937e-02,\n",
      "         -1.7364e-01,  1.6989e-01, -8.3082e-02, -8.0286e-02,  3.9125e-02,\n",
      "         -4.5807e-02,  2.8287e-02,  5.0013e-02, -4.1894e-02,  1.1127e-01,\n",
      "          2.0224e-01, -1.8652e-01,  9.3622e-02,  1.5387e-03, -2.1512e-01,\n",
      "         -9.9341e-03, -5.8556e-02, -2.0377e-01, -1.5604e-01, -2.7550e-01,\n",
      "          5.1825e-02, -1.4242e-01,  8.0027e-02, -2.0279e-01, -2.8239e-01,\n",
      "         -5.5727e-02, -3.0128e-01,  3.7381e-02, -3.8855e-02,  1.6683e-01,\n",
      "          8.8834e-02,  9.6590e-02, -3.7178e-03, -5.1841e-02,  4.4343e-02,\n",
      "         -2.4545e-01,  1.1815e-01, -5.6714e-02,  1.1284e-01,  5.8026e-02,\n",
      "          1.4482e-01,  2.3499e-01, -5.8360e-02,  5.3842e-02, -3.7024e-02,\n",
      "          1.2755e-01,  1.3118e-01,  1.2458e-01,  1.4486e-01, -8.1009e-02,\n",
      "          1.4658e-03,  1.1878e-01,  1.5571e-01, -9.1345e-02],\n",
      "        [-9.4854e-02, -1.2726e-01,  1.5428e-01, -1.6567e-01, -1.1530e-01,\n",
      "         -9.6336e-02,  1.3361e-01,  9.0662e-02,  1.9141e-02, -7.6064e-02,\n",
      "          1.1166e-01, -2.3265e-01, -3.3292e-02,  1.2201e-01,  7.6913e-02,\n",
      "         -1.0923e-01,  1.2410e-01, -8.8911e-02, -2.2833e-01,  4.1773e-02,\n",
      "         -4.4938e-02, -2.4472e-01,  2.1089e-01,  1.4772e-01, -1.0218e-01,\n",
      "          1.3315e-02, -1.5557e-01, -3.8289e-02,  1.1428e-01,  1.2336e-01,\n",
      "         -8.2945e-02, -9.9705e-02,  2.0119e-01,  8.0248e-02, -9.1229e-02,\n",
      "         -2.3137e-01,  3.5025e-01,  1.2068e-01, -1.9260e-01, -1.3204e-01,\n",
      "         -4.7197e-02,  7.4521e-02, -1.1855e-01,  8.7130e-02,  3.8169e-02,\n",
      "          1.2426e-01, -8.4692e-02, -1.9794e-01,  7.6988e-02, -1.1778e-01,\n",
      "          7.6971e-02,  1.4536e-01,  2.0978e-01, -1.4067e-01, -3.1030e-01,\n",
      "          1.0438e-01,  1.2527e-01,  9.0310e-02,  1.1681e-01, -1.1047e-01,\n",
      "         -2.1121e-01,  4.8378e-02,  2.2418e-01,  2.1204e-02],\n",
      "        [ 9.6192e-02, -2.4272e-01,  2.2104e-02, -2.3822e-01, -1.3276e-01,\n",
      "         -1.0916e-01, -5.0414e-02,  1.2263e-01, -4.1792e-02, -6.5740e-02,\n",
      "          1.3900e-01, -1.0332e-01, -2.9130e-01,  2.1438e-01,  1.7671e-01,\n",
      "         -1.2917e-01, -1.5008e-02,  2.1225e-01,  8.3367e-02,  3.7372e-03,\n",
      "         -2.9717e-02,  7.4396e-02,  1.2698e-01,  1.4943e-01, -3.8415e-02,\n",
      "          1.4578e-01,  9.6193e-02, -1.7463e-02,  3.6257e-02,  1.0048e-01,\n",
      "         -1.8038e-01, -7.2648e-02, -1.0084e-01,  2.8213e-01, -8.6371e-02,\n",
      "         -2.1435e-01, -2.1634e-01,  8.3870e-02, -2.3294e-01,  1.2360e-01,\n",
      "          3.5266e-02,  7.5112e-02, -6.7133e-02, -2.4414e-01, -1.4477e-01,\n",
      "          2.6856e-01, -1.2059e-01, -9.7584e-02, -1.7097e-01,  3.3508e-01,\n",
      "         -1.8872e-01,  8.8079e-02,  1.1793e-01, -4.6746e-02, -5.1353e-02,\n",
      "          1.4230e-01,  8.8008e-02,  1.4422e-01, -9.9760e-02,  8.1628e-03,\n",
      "          8.2248e-02,  5.3424e-02,  1.2083e-01, -1.0269e-01],\n",
      "        [-5.0992e-02,  1.0234e-01, -1.8385e-01,  3.6975e-02,  8.4621e-02,\n",
      "          5.0921e-02, -2.0989e-01, -2.9804e-02, -1.5689e-01,  7.6183e-02,\n",
      "         -2.0064e-01,  1.6997e-01,  6.5024e-02, -4.7257e-02, -1.5618e-01,\n",
      "          1.6102e-01, -1.2322e-01, -1.3815e-01,  2.0333e-01, -2.4752e-01,\n",
      "          2.7542e-02,  2.0908e-01,  8.3258e-02, -8.4342e-02,  6.3944e-02,\n",
      "          6.4098e-02, -3.0981e-02,  5.7254e-02, -5.2018e-02, -1.7993e-01,\n",
      "          2.4012e-01,  5.6740e-02, -1.1184e-01,  1.9302e-02,  4.6995e-02,\n",
      "          1.5888e-01, -7.9851e-02, -8.5146e-02,  2.0793e-01, -6.6299e-02,\n",
      "         -1.0685e-01,  3.5600e-02,  2.3099e-01,  8.6826e-02, -4.4762e-02,\n",
      "         -5.5938e-02,  1.0909e-01, -1.9869e-01, -7.9554e-02,  1.5814e-01,\n",
      "         -6.9276e-02, -9.7946e-02, -1.9657e-01, -8.1138e-02,  9.8952e-02,\n",
      "         -1.9626e-01, -6.5524e-02, -1.6850e-01, -1.2305e-01,  6.4880e-02,\n",
      "          6.8394e-02, -7.9476e-02, -1.7751e-01,  1.5270e-01],\n",
      "        [-1.2624e-01,  1.1871e-01, -1.3396e-01, -2.6787e-03,  2.5826e-02,\n",
      "          1.4838e-01, -1.6528e-02,  9.1500e-03, -1.0688e-01,  6.4031e-02,\n",
      "         -1.8758e-01,  7.5174e-02, -7.0455e-02, -2.1359e-02, -1.5750e-01,\n",
      "          4.8619e-02, -1.7994e-01, -5.0798e-02,  1.3047e-01, -9.5772e-02,\n",
      "          7.9567e-03,  5.9883e-03, -1.9111e-02, -1.6696e-01, -5.5817e-03,\n",
      "          8.1942e-02,  1.2626e-01,  1.1747e-01, -1.0394e-01, -1.6147e-02,\n",
      "          2.1860e-01,  1.3674e-01, -5.0470e-02, -1.5566e-01, -2.2948e-04,\n",
      "          1.9370e-01, -2.5687e-01,  7.2086e-03,  1.7846e-01, -4.9765e-03,\n",
      "          4.8297e-02, -8.0186e-02,  1.6741e-02,  1.4166e-02,  3.6468e-02,\n",
      "         -1.4055e-01,  6.1800e-02, -6.5679e-02,  4.1435e-03,  8.8866e-02,\n",
      "         -1.5669e-01, -1.0226e-01, -1.7936e-01, -1.2801e-02,  7.7564e-02,\n",
      "         -1.3577e-01, -1.0390e-01, -6.7991e-03, -1.7363e-01,  9.2510e-02,\n",
      "          1.3537e-01, -1.0641e-01, -2.0253e-01,  9.6186e-02],\n",
      "        [ 1.4977e-01, -3.1285e-02, -1.1814e-02, -7.5946e-02,  9.4403e-02,\n",
      "          1.4184e-01, -1.3011e-01, -1.7541e-01,  9.7388e-02,  1.9041e-01,\n",
      "         -3.8680e-03,  3.6089e-02,  1.2689e-01, -4.5252e-02, -6.4394e-02,\n",
      "         -5.9621e-02, -1.0425e-02, -1.0457e-01,  4.8174e-02, -1.0904e-01,\n",
      "          6.3616e-03,  1.9583e-01, -1.3480e-01, -1.2110e-01,  1.3848e-01,\n",
      "          8.7900e-02,  1.4578e-01,  1.0261e-01,  1.3827e-01,  9.6402e-02,\n",
      "          9.3585e-02,  1.2046e-01, -2.0966e-01, -2.3103e-02,  1.6061e-01,\n",
      "          3.8850e-02,  1.5220e-01,  2.8359e-02,  1.8348e-01, -1.8667e-02,\n",
      "          4.2208e-02, -7.8978e-02,  1.1972e-01, -6.3099e-02, -7.1189e-02,\n",
      "          3.4930e-02,  4.9986e-03, -2.2596e-02, -3.2745e-02, -5.5431e-02,\n",
      "         -2.2157e-01, -1.6128e-01, -1.1593e-01,  1.3318e-01,  1.7823e-01,\n",
      "          6.6487e-02, -9.3453e-02, -1.5424e-01, -2.1837e-01,  4.8935e-02,\n",
      "          1.7260e-01, -2.9564e-02, -1.5850e-01, -1.7622e-02]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_mu_1.bias\n",
      "-->para Parameter containing:\n",
      "tensor([-0.0993,  0.0643, -0.1790, -0.0940, -0.0250,  0.0677, -0.0372,  0.2079])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_log_var_1.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[-2.1889e-02,  6.5526e-02, -6.7496e-02,  5.1004e-02,  5.7137e-02,\n",
      "          7.8995e-03, -2.6408e-02, -9.8571e-02,  5.7952e-02, -2.5767e-01,\n",
      "         -2.2683e-01, -1.2542e-01, -9.7974e-02,  3.4474e-02,  6.3912e-03,\n",
      "         -6.9930e-02, -1.1061e-01,  1.1824e-01,  8.4891e-02,  1.1272e-01,\n",
      "          1.0860e-01,  3.8887e-01,  2.0258e-01, -5.8043e-02,  1.1786e-01,\n",
      "         -2.7012e-02, -1.3892e-02,  1.7183e-02, -2.3746e-02, -1.3027e-01,\n",
      "         -1.1817e-01, -6.1155e-02, -1.2663e-02,  4.4414e-02, -6.5794e-01,\n",
      "         -1.8065e-01,  1.1506e-02, -1.1826e-01,  9.9914e-02, -2.5625e-02,\n",
      "          1.5820e-01,  3.8648e-02,  4.9823e-02,  2.0179e-02,  2.7372e-02,\n",
      "          2.9354e-02,  1.7989e-02,  7.9149e-02, -7.9550e-02, -6.0977e-02,\n",
      "         -1.3180e-01, -5.4277e-02, -4.4429e-01,  6.5881e-02, -1.0513e-01,\n",
      "         -9.4774e-02,  2.7022e-02,  1.0803e-01,  3.3333e-01, -6.5666e-02,\n",
      "          7.9879e-03, -2.7819e-02, -2.5419e-01, -7.2296e-02],\n",
      "        [-8.0703e-02,  1.1499e-01, -9.5218e-02,  2.0174e-02, -1.2677e-02,\n",
      "          5.9809e-02, -2.4018e-04, -5.6879e-02,  5.8054e-02, -2.9015e-01,\n",
      "         -2.6544e-01, -1.9979e-01, -2.1173e-02, -4.8662e-02,  2.3284e-02,\n",
      "         -6.1203e-02, -9.6695e-03,  1.3502e-01,  1.0083e-01, -1.4377e-02,\n",
      "          1.1985e-01,  3.2346e-01,  2.3950e-01,  3.0185e-02,  2.4331e-02,\n",
      "          4.4137e-02, -1.3584e-02, -1.1561e-01, -7.1365e-03, -1.8632e-01,\n",
      "         -8.3497e-02,  3.7877e-02, -1.4937e-02,  1.6760e-01, -6.0343e-01,\n",
      "         -4.1599e-01, -5.0210e-02, -5.7146e-02, -2.3558e-02, -2.4875e-02,\n",
      "         -4.5528e-02, -4.8381e-02,  1.2978e-01, -5.7238e-02,  2.7570e-02,\n",
      "         -5.5554e-02, -6.0960e-02,  8.1079e-02, -2.0206e-02, -1.2931e-01,\n",
      "         -2.0810e-01, -5.5267e-02, -3.6427e-01, -6.9486e-03, -1.1085e-01,\n",
      "         -1.3174e-01, -8.3730e-02,  1.4214e-01,  3.1545e-01, -1.2326e-01,\n",
      "          3.4228e-02, -5.3376e-02, -2.2161e-01, -1.0036e-01],\n",
      "        [ 8.4794e-02,  1.8901e-01, -4.4248e-02,  1.8807e-02,  2.9787e-02,\n",
      "         -1.4093e-02,  6.2843e-02, -5.8046e-02, -1.1471e-03, -2.0345e-01,\n",
      "         -1.8721e-01, -8.7048e-02, -1.1276e-01,  1.9120e-02,  5.2346e-02,\n",
      "         -2.2033e-02, -6.3626e-02,  2.8792e-02,  7.7280e-02, -2.1021e-02,\n",
      "         -1.7365e-03,  4.5317e-01,  2.4269e-01, -1.1516e-02, -1.2920e-02,\n",
      "         -5.8135e-02,  2.8658e-02, -4.5133e-02, -6.3443e-02, -1.0524e-01,\n",
      "          3.6304e-02,  1.2460e-02,  3.0938e-02,  9.2773e-02, -5.3931e-01,\n",
      "         -2.0253e-01, -2.5852e-02, -1.2472e-01, -2.5163e-02,  5.3877e-02,\n",
      "          4.0749e-02, -1.1857e-01,  1.5007e-01, -5.2166e-02,  5.1164e-02,\n",
      "         -3.2030e-02, -2.4830e-02, -5.4518e-02, -7.5856e-02, -5.2941e-02,\n",
      "         -2.1550e-01,  1.7785e-02, -2.2126e-01, -8.9403e-02, -7.6762e-02,\n",
      "         -5.6657e-02,  5.3913e-02,  1.4793e-01,  3.9829e-01, -3.7165e-02,\n",
      "          7.5341e-02,  8.7210e-02, -2.8700e-01, -3.4295e-02],\n",
      "        [-9.7345e-02,  5.1278e-02, -6.5870e-02,  4.0517e-02,  1.0268e-02,\n",
      "         -3.8134e-02,  6.1839e-02, -1.1268e-02, -2.6152e-02, -1.3793e-01,\n",
      "         -1.5115e-01, -1.4179e-01, -3.2958e-02,  2.0393e-02,  6.8119e-02,\n",
      "         -8.2290e-02, -3.3857e-03,  5.1771e-02,  1.6456e-01,  6.4247e-02,\n",
      "         -2.7844e-03,  3.7398e-01,  1.1891e-01, -6.8267e-02, -4.9471e-02,\n",
      "         -5.3014e-02, -3.2188e-02,  7.9218e-02, -1.6380e-02, -1.2919e-01,\n",
      "          5.8957e-02,  1.0035e-01, -3.9187e-02,  5.3825e-02, -4.3072e-01,\n",
      "         -1.3888e-01,  1.7276e-02, -4.7165e-02,  3.6706e-02, -2.3916e-02,\n",
      "          2.5166e-02,  1.4070e-02,  3.0972e-02,  5.8755e-03,  9.8622e-03,\n",
      "         -7.6990e-02, -5.2278e-03,  3.7724e-02, -5.4681e-02,  1.6052e-02,\n",
      "         -1.1008e-01,  7.5304e-02, -1.7103e-01,  1.3180e-01, -1.0370e-01,\n",
      "         -4.3339e-02, -2.9366e-02, -1.7684e-02,  2.6414e-01, -1.0518e-01,\n",
      "          6.9673e-03, -5.6732e-03, -1.9426e-01, -6.1598e-02],\n",
      "        [ 1.3428e-02,  1.4096e-01, -4.1057e-02, -1.2333e-01,  9.6459e-02,\n",
      "          3.5469e-02,  7.5127e-02,  5.9943e-02, -1.4828e-02, -2.4015e-01,\n",
      "         -1.9807e-01, -1.1571e-01,  3.7591e-02,  3.0947e-02, -6.7525e-03,\n",
      "         -1.1631e-01, -2.6197e-02,  1.1950e-01,  1.4410e-01,  8.1677e-02,\n",
      "         -3.6958e-03,  4.5062e-01,  2.3422e-01,  4.2917e-02,  1.2910e-01,\n",
      "         -3.5631e-02,  1.0502e-02, -1.0800e-01, -6.0450e-02, -2.2820e-01,\n",
      "         -8.4124e-03,  3.6005e-02, -8.6772e-02,  1.2638e-02, -7.0549e-01,\n",
      "         -3.3593e-01, -6.6910e-03, -1.1309e-01,  1.0552e-01,  1.5361e-02,\n",
      "          2.7856e-02, -5.6662e-02,  2.7393e-02, -2.9238e-02,  1.0278e-01,\n",
      "         -8.6127e-02,  1.8249e-02, -1.2523e-02,  3.4872e-02, -9.5702e-02,\n",
      "         -1.9653e-01, -5.5203e-02, -4.2152e-01,  6.5161e-02, -8.3734e-03,\n",
      "         -9.0787e-02, -2.5520e-02,  1.4272e-01,  4.3991e-01, -1.2743e-01,\n",
      "          4.7738e-02, -8.8850e-02, -1.3254e-01, -1.2475e-01],\n",
      "        [-4.2465e-02,  1.4028e-01, -3.7536e-02, -3.2799e-02,  1.3203e-01,\n",
      "         -1.5714e-02,  3.0501e-02,  7.2509e-02, -5.4934e-03, -3.1256e-01,\n",
      "         -1.0485e-01, -1.3827e-01,  9.1986e-03, -7.3129e-03,  3.9733e-02,\n",
      "         -3.8315e-02, -3.6731e-02,  4.3350e-02,  9.9431e-02,  1.1883e-01,\n",
      "          4.3964e-02,  2.0618e-01,  1.7611e-01,  6.4840e-02, -2.7085e-02,\n",
      "         -4.8421e-02, -4.2609e-03,  1.0575e-01, -6.2694e-02, -1.7104e-01,\n",
      "         -6.9112e-03,  1.8187e-02,  2.6189e-03,  5.5712e-02, -5.6788e-01,\n",
      "         -2.6710e-01, -5.7447e-03, -1.5888e-02,  8.7971e-02, -8.2887e-03,\n",
      "          1.1395e-01,  1.3099e-02, -1.4405e-02, -3.6109e-03,  5.3413e-02,\n",
      "         -3.7523e-02, -2.6772e-02, -1.1794e-02,  1.2888e-02, -2.2815e-02,\n",
      "         -1.5568e-01,  4.1928e-02, -2.5618e-01,  3.5652e-02, -9.5926e-02,\n",
      "         -1.1099e-01, -4.9382e-02,  2.7963e-02,  3.0822e-01, -5.5834e-02,\n",
      "          3.3564e-02, -2.5133e-02, -1.0990e-01, -1.5646e-02],\n",
      "        [-5.4742e-02,  7.9024e-04, -8.1827e-02,  3.2889e-02,  5.4520e-02,\n",
      "          6.3362e-02,  2.0391e-02,  2.3586e-02, -1.1432e-01, -2.0240e-01,\n",
      "         -1.4445e-01,  8.8403e-03, -2.4146e-02, -1.4768e-02, -2.3787e-02,\n",
      "         -2.5170e-02,  6.4874e-02,  5.4743e-02,  4.1887e-03,  1.7027e-02,\n",
      "         -3.4483e-02,  2.6322e-01,  5.2591e-02,  6.0811e-02,  1.6906e-02,\n",
      "          8.0844e-02,  1.6640e-02, -6.5486e-02,  7.4191e-03, -4.7811e-02,\n",
      "         -9.0337e-03,  4.6361e-02, -4.3911e-02,  4.4311e-02, -4.6407e-01,\n",
      "         -6.0213e-02,  8.9226e-03, -7.2649e-02,  4.1017e-02, -8.6215e-02,\n",
      "         -7.2687e-02,  6.1995e-02, -1.7578e-02, -2.9271e-02,  2.2675e-02,\n",
      "         -7.3520e-02, -4.2750e-02,  2.6912e-02,  6.4170e-02,  3.3075e-02,\n",
      "         -7.9658e-02,  4.4679e-02, -2.2896e-01, -7.9790e-02, -3.1236e-02,\n",
      "         -1.0169e-02, -8.8681e-02,  4.3698e-02,  1.8451e-01, -3.9582e-02,\n",
      "          8.2364e-02, -3.1297e-02, -1.9383e-01, -1.9686e-03],\n",
      "        [ 5.0462e-02,  2.8340e-02, -7.0527e-02, -1.0216e-01,  9.8259e-02,\n",
      "          5.8429e-02,  5.5649e-02, -2.4868e-02,  3.2355e-02, -1.8070e-01,\n",
      "         -1.9827e-01, -9.3652e-02, -3.3035e-02, -1.4003e-02,  7.9030e-02,\n",
      "         -2.7409e-03, -7.2812e-02, -1.2192e-02, -1.4841e-02,  1.0467e-01,\n",
      "          7.6618e-02,  1.9169e-01,  1.6241e-01,  1.2034e-01,  1.3655e-01,\n",
      "          2.6831e-03,  3.5465e-02, -9.2698e-03, -2.9316e-02, -1.5735e-01,\n",
      "         -8.7750e-02,  1.7735e-02,  3.0853e-02, -3.2003e-02, -5.2970e-01,\n",
      "         -1.8931e-01, -1.1604e-02, -2.7975e-02,  5.4761e-02,  6.0785e-02,\n",
      "          1.6292e-02, -8.9721e-02,  9.9038e-02, -3.1581e-02, -1.3433e-02,\n",
      "         -1.3158e-01, -5.4100e-02, -1.0583e-02, -4.8171e-02, -8.1603e-03,\n",
      "         -2.3808e-01,  2.3643e-03, -2.1377e-01, -1.6333e-02, -8.4893e-02,\n",
      "         -3.8646e-02, -4.2730e-03,  6.4953e-02,  3.0955e-01, -8.5065e-03,\n",
      "          6.2280e-02,  4.6099e-02, -1.0093e-01, -1.6053e-01]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_log_var_1.bias\n",
      "-->para Parameter containing:\n",
      "tensor([-0.7826, -0.6975, -0.5604, -0.5314, -0.6636, -0.5957, -0.4980, -0.5003])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.weight_ih_l0\n",
      "-->para Parameter containing:\n",
      "tensor([[-0.4227, -0.2322, -0.0399, -0.2537, -0.0543, -0.2127,  0.0248,  0.1267],\n",
      "        [-0.1968, -0.2218, -0.0026, -0.1060,  0.1463,  0.1316,  0.0444,  0.0342],\n",
      "        [-0.2143,  0.0045, -0.1127, -0.2168, -0.0839,  0.0077, -0.0217, -0.0224],\n",
      "        [ 0.0707, -0.1363, -0.2551,  0.2118,  0.0038, -0.0482,  0.0879, -0.0995],\n",
      "        [-0.4180, -0.2926, -0.2152,  0.2671, -0.4950, -0.0521, -0.0970,  0.0638],\n",
      "        [ 0.0782, -0.0347, -0.0416,  0.1000, -0.2690,  0.0127,  0.0944, -0.0910],\n",
      "        [-0.1221, -0.1185,  0.3835,  0.0307,  0.1247,  0.0362,  0.0253, -0.1696],\n",
      "        [-0.2029,  0.1128,  0.0545,  0.2275, -0.2066,  0.0755,  0.0208, -0.0225]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.weight_hh_l0\n",
      "-->para Parameter containing:\n",
      "tensor([[ 0.6461, -0.0967, -0.0084, -0.0413,  0.1886,  0.0018, -0.1506,  0.1284],\n",
      "        [-0.1480,  0.8215, -0.0523, -0.0517,  0.0235,  0.1815,  0.0148, -0.1712],\n",
      "        [ 0.3294,  0.0702,  0.3249,  0.0444,  0.3447,  0.4217,  0.0638,  0.3252],\n",
      "        [ 0.0306, -0.0950, -0.2078,  0.3756, -0.0788,  0.1775, -0.2222, -0.5446],\n",
      "        [-0.0968,  0.0262, -0.0176, -0.1812,  0.3689,  0.0770, -0.3874,  0.2049],\n",
      "        [-0.3085, -0.0613,  0.3537, -0.4055,  0.6590,  0.1949,  0.0087,  0.1394],\n",
      "        [-0.0173, -0.0806,  0.0848, -0.3791, -0.3078, -0.4489,  0.1681,  0.3190],\n",
      "        [-0.0967,  0.2831,  0.2862, -0.3639,  0.1235,  0.3181,  0.0496,  0.4576]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.bias_ih_l0\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.0746, -0.2336,  0.3125, -0.3000, -0.2889,  0.3359,  0.2494,  0.1874])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.bias_hh_l0\n",
      "-->para Parameter containing:\n",
      "tensor([-0.1924,  0.1988,  0.1912, -0.3019, -0.1522, -0.1326,  0.1286,  0.3418])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc1.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[-7.6616e-01,  7.3131e-02, -6.2267e-01,  4.1870e-01, -5.5087e-01,\n",
      "         -2.0277e-01, -5.6286e-01, -8.5538e-02],\n",
      "        [ 4.8124e-01,  6.7113e-01,  4.8094e-01, -3.2954e-01,  5.5771e-01,\n",
      "          2.5528e-01,  6.7915e-01,  4.8805e-01],\n",
      "        [-1.0793e-02,  9.5199e-01,  1.3030e-01, -1.9639e-01,  1.6539e-01,\n",
      "          4.9505e-02,  6.3446e-01,  5.0956e-01],\n",
      "        [-1.2289e+00, -1.2240e+00, -6.4963e-02,  4.4141e-02, -1.7668e+00,\n",
      "          6.0979e-02, -1.8087e-01, -2.3124e-01],\n",
      "        [-1.3596e+00, -1.9872e+00, -4.2213e-01,  4.6515e-01, -3.2820e-01,\n",
      "         -3.4312e-01, -1.0886e+00,  3.7499e-02],\n",
      "        [-5.3539e-01,  5.1538e-01, -8.5942e-01,  2.1301e-01,  4.0453e-01,\n",
      "         -2.2674e-01,  2.0681e-01, -5.8021e-01],\n",
      "        [-7.5532e-01,  1.0508e+00, -7.9846e-01,  5.4904e-01, -8.0733e-01,\n",
      "         -5.9944e-01, -4.6634e-01, -4.3727e-01],\n",
      "        [-1.0989e+00, -1.6662e+00,  3.6996e-01, -2.1120e-01, -8.6066e-02,\n",
      "          5.6824e-01,  1.4635e-01,  5.0391e-01],\n",
      "        [-4.0525e-01,  1.0642e+00,  1.5101e-01, -1.0376e-02, -3.5942e-01,\n",
      "          1.0455e+00,  9.4687e-01,  2.6692e-02],\n",
      "        [-9.0379e-01, -9.6050e-01, -4.6708e-01,  7.6580e-01, -3.6513e-01,\n",
      "         -2.8957e-01, -8.2172e-01, -7.3666e-01],\n",
      "        [-9.9099e-02,  8.5466e-01,  5.2749e-01, -5.2397e-01,  5.0312e-01,\n",
      "         -9.5372e-02,  2.5868e-01,  2.6168e-01],\n",
      "        [-8.3457e-01,  6.2412e-01, -5.6059e-01,  3.8049e-01, -1.0383e+00,\n",
      "          4.7825e-01,  4.9333e-01, -5.9806e-01],\n",
      "        [ 8.4827e-01,  4.6654e-01, -3.8378e-02, -2.6439e-02,  5.7800e-01,\n",
      "         -3.9347e-01,  1.3081e+00,  3.3580e-01],\n",
      "        [-2.3231e-01, -3.0670e-01,  7.6531e-01, -6.0454e-01,  1.8847e-01,\n",
      "          4.0184e-01, -3.6377e-01,  7.0467e-01],\n",
      "        [-4.8035e-01, -6.8142e-01, -5.7271e-01,  4.6815e-01,  1.2698e-02,\n",
      "          1.5487e-01, -1.9003e-01, -6.1181e-01],\n",
      "        [-2.6716e-01, -1.2551e+00, -6.6360e-01,  1.4129e-01, -6.8738e-01,\n",
      "         -9.4531e-01, -1.8427e-02, -6.2315e-01],\n",
      "        [-7.6480e-01,  7.2014e-01, -5.5628e-01,  6.7028e-01, -6.3861e-01,\n",
      "         -6.4455e-01,  3.9661e-01, -7.6466e-01],\n",
      "        [ 1.2081e+00, -2.9485e-01,  6.9258e-01, -2.1294e-01,  6.3118e-01,\n",
      "         -1.4150e-01,  3.3813e-01,  5.0650e-01],\n",
      "        [ 2.2379e-03, -1.1887e+00, -1.9401e-02,  3.2387e-01, -7.9405e-01,\n",
      "         -9.6912e-01, -1.2328e+00, -3.3960e-01],\n",
      "        [ 8.3674e-01,  4.3131e-01,  5.8166e-01, -5.5587e-01,  3.0181e-02,\n",
      "          1.8074e-01,  5.4926e-01,  5.2730e-01],\n",
      "        [ 4.1625e-01, -9.6135e-02,  1.8723e-02,  1.7220e-01, -9.2518e-01,\n",
      "         -8.8458e-01, -6.2565e-01, -4.0265e-01],\n",
      "        [ 3.2488e-01,  2.3550e+00,  1.2197e-01, -4.3412e-01,  3.3299e-02,\n",
      "          3.9715e-01,  6.0016e-01,  4.9808e-02],\n",
      "        [-3.9465e-01, -1.9501e-01, -6.7809e-01,  6.3240e-01, -5.8007e-01,\n",
      "         -5.0897e-01,  1.1720e-01, -4.4051e-01],\n",
      "        [ 4.6067e-01, -6.0336e-01,  5.7137e-01, -2.0856e-01,  2.8962e-01,\n",
      "          9.5169e-01,  3.3949e-01,  5.0966e-01],\n",
      "        [-1.0476e-01, -5.3341e-01, -1.9245e-01,  4.0203e-01,  9.8006e-02,\n",
      "         -1.9309e-01, -1.1161e+00, -8.2482e-02],\n",
      "        [ 4.3521e-01,  4.9506e-01,  5.3840e-01, -2.4159e-01,  6.7109e-01,\n",
      "         -1.0622e-01,  2.9273e-01,  5.8496e-01],\n",
      "        [-3.6672e-01,  7.2427e-01, -7.3891e-01,  4.0974e-01, -4.0877e-01,\n",
      "         -5.5903e-01,  6.2142e-01, -6.5926e-01],\n",
      "        [ 5.2008e-02, -1.5659e+00,  2.2454e-01,  4.2867e-01, -1.1501e+00,\n",
      "         -6.7217e-01, -1.4232e+00, -1.9930e-01],\n",
      "        [ 8.6832e-01, -2.0645e-01,  6.2612e-01, -6.0902e-02,  3.2634e-01,\n",
      "          9.2845e-02,  1.4673e-01,  3.9445e-01],\n",
      "        [-5.1834e-01, -1.8059e+00, -6.1054e-01,  5.8187e-01, -1.8316e-01,\n",
      "         -5.2696e-02, -8.1330e-01,  6.3523e-02],\n",
      "        [-1.1609e+00,  5.7776e-01, -5.9061e-01,  5.8960e-01, -1.5714e-01,\n",
      "          1.7921e-01,  2.0471e-02, -4.1968e-01],\n",
      "        [-7.0681e-01, -1.9430e+00, -5.5804e-01,  5.6850e-01,  2.7472e-01,\n",
      "         -7.3403e-01, -1.4418e+00, -3.3604e-01],\n",
      "        [ 1.1332e-01,  1.4103e+00, -6.5635e-01,  5.9536e-01, -1.0638e+00,\n",
      "         -4.8225e-01, -4.0956e-01, -5.7517e-01],\n",
      "        [-6.7740e-01, -2.3295e+00,  7.1223e-02, -6.4014e-01, -1.1050e+00,\n",
      "         -6.4265e-01, -7.2813e-02,  7.9830e-01],\n",
      "        [-7.6835e-01,  9.9237e-02, -9.4329e-01,  6.3311e-01,  5.3785e-01,\n",
      "         -2.9729e-01,  2.9464e-01, -3.3481e-01],\n",
      "        [-7.1467e-01, -1.0416e+00, -5.2525e-01,  7.1718e-01,  1.3837e-01,\n",
      "          3.1051e-01, -4.3062e-01, -5.6569e-01],\n",
      "        [-4.5935e-01, -1.0998e-01, -8.4019e-01,  6.8616e-01, -5.9158e-01,\n",
      "         -4.3359e-01,  5.6621e-01, -7.2518e-01],\n",
      "        [ 5.8282e-01, -4.2929e-01,  6.9597e-01, -5.2071e-01,  6.2744e-01,\n",
      "         -2.6308e-03,  2.1400e-01,  5.7653e-01],\n",
      "        [-1.4082e+00, -1.8939e+00, -1.0355e-01,  3.3304e-01, -5.6345e-01,\n",
      "         -5.8661e-02, -1.2436e+00, -5.1020e-01],\n",
      "        [-5.0767e-01, -5.8083e-01, -3.1385e-01,  8.0196e-02, -5.0963e-01,\n",
      "         -2.6586e-01, -6.8593e-01, -5.7245e-01],\n",
      "        [ 4.9859e-02, -4.6475e-02,  7.9720e-01, -7.1819e-01,  5.8821e-01,\n",
      "          4.9163e-01, -3.3038e-01,  9.0566e-02],\n",
      "        [ 5.4307e-01, -6.9758e-01,  6.8540e-01, -5.0717e-01,  8.5896e-01,\n",
      "          3.3482e-01,  8.6472e-02,  7.4590e-02],\n",
      "        [-3.5501e-01,  2.4793e-01, -6.9011e-01,  7.9431e-01, -1.3300e+00,\n",
      "         -2.8071e-01, -6.9966e-01, -4.1709e-01],\n",
      "        [ 9.9816e-01,  3.9587e-01,  4.6380e-02, -5.5057e-01, -1.2716e-01,\n",
      "         -4.5685e-01,  7.5411e-01,  6.8078e-01],\n",
      "        [ 1.9598e-01, -8.4720e-01, -4.3534e-01,  4.9644e-01,  1.2063e-01,\n",
      "          2.5429e-01, -1.2503e-01, -5.9365e-01],\n",
      "        [ 1.6097e-01,  6.6111e-01,  6.7242e-01, -7.4960e-01, -1.5569e-01,\n",
      "          5.3453e-01,  3.4602e-01,  6.4677e-01],\n",
      "        [ 9.6905e-02,  1.1614e+00,  5.2761e-01, -8.6515e-01, -8.9802e-01,\n",
      "          2.2670e-01,  1.0809e-02,  8.4871e-01],\n",
      "        [ 2.5333e-03, -7.4803e-02,  5.9516e-01, -5.1780e-01,  4.8261e-01,\n",
      "          1.7392e-01,  2.4099e-01,  4.3615e-01],\n",
      "        [-6.7510e-01,  7.8529e-01,  4.3808e-01, -5.5273e-01,  7.5389e-01,\n",
      "          7.5224e-01, -3.5663e-01,  5.5352e-01],\n",
      "        [ 2.6840e-01,  9.8759e-01,  4.8069e-02, -4.2861e-01,  4.1980e-01,\n",
      "          9.7299e-03,  6.6325e-01,  2.7559e-01],\n",
      "        [-3.5378e-01,  2.1195e-03, -8.1321e-02, -7.0289e-01,  3.9039e-01,\n",
      "          6.5093e-01,  8.4876e-02,  6.3394e-01],\n",
      "        [ 6.6361e-01,  1.9658e-02,  7.0149e-01, -4.9251e-01,  2.7089e-01,\n",
      "          2.7795e-01,  8.4974e-02,  5.5947e-01],\n",
      "        [ 1.3116e+00,  1.2229e+00,  8.9691e-02, -5.1607e-01,  3.7020e-01,\n",
      "          4.5302e-01,  6.9177e-01,  5.4231e-01],\n",
      "        [ 5.7942e-01,  2.2557e-01,  5.7815e-01, -2.0086e-01,  6.7809e-01,\n",
      "          5.3444e-01, -1.9341e-02,  6.9444e-01],\n",
      "        [ 1.2589e+00,  6.1385e-01,  4.7803e-01, -8.1761e-01, -1.3198e+00,\n",
      "         -5.8891e-01,  8.8278e-01,  8.6212e-02],\n",
      "        [-5.6594e-01, -8.2122e-01, -2.4675e-02,  3.9071e-01, -4.5763e-01,\n",
      "         -4.2642e-01, -6.4034e-01, -3.8978e-01],\n",
      "        [-3.8972e-01,  5.4142e-01, -1.2393e-01,  2.2841e-01, -7.6541e-01,\n",
      "         -4.1161e-01, -5.3801e-01, -4.5470e-01],\n",
      "        [ 4.7715e-01,  1.6617e+00,  2.9824e-01, -2.4708e-01, -1.1986e-02,\n",
      "          1.8549e-01, -7.0946e-02,  5.9525e-01],\n",
      "        [ 2.6214e-01, -1.1095e+00, -3.3242e-02, -8.7332e-02, -4.4157e-01,\n",
      "         -3.4476e-01, -9.6206e-01,  5.9782e-02],\n",
      "        [-2.3473e-01,  2.8371e-01,  5.1361e-01, -4.7845e-01,  5.3534e-01,\n",
      "          7.6281e-01,  1.1173e+00,  6.2213e-01],\n",
      "        [ 5.0797e-01, -3.0246e-01,  1.9025e-01, -3.7342e-01,  2.1218e-01,\n",
      "         -2.7262e-01,  6.7745e-01,  4.2056e-01],\n",
      "        [ 5.6408e-01,  4.6981e-02,  6.5832e-01, -6.6532e-01,  3.5219e-01,\n",
      "          3.7274e-01, -2.7259e-02,  4.3768e-01],\n",
      "        [-8.5399e-01, -6.9653e-01,  6.4865e-01, -4.8400e-01, -3.5764e-01,\n",
      "          2.2000e-01,  1.4768e-01,  6.6114e-01],\n",
      "        [-3.0685e-01,  1.2732e+00,  4.5951e-01, -6.1760e-01,  1.1159e+00,\n",
      "          9.6270e-01,  2.1590e-03,  2.5083e-01]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc1.bias\n",
      "-->para Parameter containing:\n",
      "tensor([-0.5219,  0.4732,  0.6261, -0.2185, -0.2015, -0.5077, -0.0110,  0.5655,\n",
      "         0.4598, -0.1872,  0.6269, -0.5120,  0.6095,  0.5133, -0.6520, -0.1294,\n",
      "        -0.2637,  0.6759, -0.3256,  0.4557, -0.4753,  0.6303, -0.4169,  0.4508,\n",
      "        -0.4155,  0.5827, -0.4940,  0.0736,  0.5252, -0.4267, -0.4751, -0.0465,\n",
      "        -0.1830,  0.4547, -0.6291, -0.4116, -0.2405,  0.4021, -0.1629, -0.5666,\n",
      "         0.5976,  0.4013, -0.4255,  0.5962, -0.7116,  0.3328,  0.7728,  0.3815,\n",
      "         0.4175,  0.5255,  0.6656,  0.4404,  0.0876,  0.4571,  0.6885, -0.5810,\n",
      "        -0.5603,  0.4567, -0.5319,  0.0902,  0.4966,  0.5633,  0.5658,  0.1154])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc2.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[ 0.2129, -0.0356,  0.0269,  ...,  0.0150, -0.1614,  0.0184],\n",
      "        [-0.1107, -0.0456, -0.1052,  ..., -0.0148, -0.2453, -0.0377],\n",
      "        [-0.0167, -0.1653, -0.4147,  ..., -0.0910, -0.2307,  0.0045],\n",
      "        ...,\n",
      "        [ 0.1974, -0.1520, -0.2006,  ..., -0.1960, -0.2003, -0.1826],\n",
      "        [ 0.1936,  0.0554,  0.1377,  ..., -0.0250,  0.1833, -0.0679],\n",
      "        [-0.0463, -0.1081,  0.0591,  ...,  0.2310,  0.1129, -0.3460]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc2.bias\n",
      "-->para Parameter containing:\n",
      "tensor([-1.1717e-01, -2.6814e-01, -2.7313e-01, -6.0605e-02, -6.6487e-02,\n",
      "        -8.9896e-02, -2.5013e-02,  8.4136e-02, -3.7236e-02, -1.2949e-01,\n",
      "         1.3288e-02, -1.0737e-01, -2.9270e-01,  1.6132e-01, -8.7536e-03,\n",
      "         8.4463e-02, -4.0271e-02, -2.9943e-02, -1.2688e-01, -5.7208e-02,\n",
      "        -6.4799e-02,  1.4848e-01,  6.9943e-03,  2.0829e-01, -7.7264e-02,\n",
      "        -1.6841e-01,  7.2306e-02,  6.9890e-02, -1.0111e-01,  1.9540e-01,\n",
      "        -4.1975e-03, -1.3873e-01,  1.2947e-01,  2.2198e-01, -1.9610e-01,\n",
      "        -6.3346e-04,  7.2892e-02,  1.4964e-01,  2.7828e-01, -7.9509e-02,\n",
      "         3.6005e-02,  1.7510e-01,  1.2952e-01,  8.4845e-03,  1.1830e-01,\n",
      "        -7.1837e-02,  1.6091e-01,  3.0840e-01, -9.3525e-02,  2.0269e-03,\n",
      "        -1.8747e-02, -1.7791e-03, -9.7316e-02,  1.6855e-01,  3.4658e-02,\n",
      "        -1.7617e-01,  9.5857e-02, -2.6633e-03,  1.6628e-01,  4.9808e-02,\n",
      "        -2.2950e-01, -4.9437e-02,  1.6264e-02, -2.7306e-03, -8.7228e-02,\n",
      "        -1.0507e-01,  4.8340e-02, -1.0055e-01, -1.3660e-01, -5.2645e-02,\n",
      "        -1.4681e-01,  1.2700e-01,  2.3620e-01, -6.5128e-02,  1.4167e-01,\n",
      "        -5.0234e-02, -8.7094e-02, -6.0417e-02, -1.9130e-01,  1.3068e-01,\n",
      "        -2.7196e-02,  3.1222e-02, -1.4355e-01, -4.1513e-02, -1.2469e-01,\n",
      "         1.7360e-01,  8.6834e-02,  4.8924e-02, -1.5348e-01, -6.0242e-02,\n",
      "         1.7725e-01, -2.2369e-01, -1.6409e-01, -2.2351e-01, -1.0707e-01,\n",
      "        -1.4699e-01,  1.5557e-01,  3.5590e-02,  1.3737e-01,  1.7473e-01,\n",
      "        -7.8336e-04, -1.3130e-01,  4.6025e-03,  1.6229e-01, -2.8888e-01,\n",
      "        -9.3108e-02,  1.2139e-01,  2.8080e-01, -2.5201e-01,  9.8164e-02,\n",
      "        -1.2162e-01, -1.7542e-01, -1.4477e-01, -1.1139e-01,  4.8819e-02,\n",
      "        -1.8686e-01, -1.9072e-01, -3.3781e-03, -1.3193e-01, -2.6423e-01,\n",
      "        -1.2294e-01, -4.2394e-02, -2.4884e-01, -1.5837e-01,  1.9968e-05,\n",
      "         1.7143e-01, -3.0886e-01, -1.8301e-01,  5.4767e-03,  4.3606e-02,\n",
      "        -1.3449e-01, -3.7974e-02,  4.0201e-02, -3.0271e-02,  2.9988e-04,\n",
      "        -2.8488e-01, -1.6153e-01,  1.5389e-02,  4.7755e-02,  1.8577e-01,\n",
      "         1.0369e-01, -8.1941e-02,  8.3142e-02, -5.7139e-02,  4.7308e-02,\n",
      "        -2.8400e-02, -1.9301e-01, -2.4431e-02, -5.8738e-02, -1.3514e-01,\n",
      "         2.3979e-02,  2.0171e-01,  1.6641e-02, -3.1415e-02,  1.1639e-01,\n",
      "         3.0763e-01,  1.7698e-01, -1.9988e-02, -1.4964e-02, -3.2240e-02,\n",
      "        -4.5846e-02,  3.5338e-02, -1.3818e-01, -7.5219e-02, -1.2084e-01,\n",
      "        -1.6502e-01, -4.1874e-02,  4.9370e-03,  4.1346e-02, -1.7161e-01,\n",
      "        -1.9297e-01,  2.4823e-02, -2.0238e-01, -1.1724e-02, -8.9322e-02,\n",
      "         2.7584e-02, -1.2402e-01, -1.2057e-01,  2.0978e-02, -1.0339e-01,\n",
      "        -1.3253e-01, -7.9800e-02,  3.6856e-02, -3.1296e-01, -9.0772e-03,\n",
      "         6.9597e-01,  9.9828e-02])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.weight_ih_l0\n",
      "-->para Parameter containing:\n",
      "tensor([[-1.4368e-02, -2.2121e-02, -3.3050e-02,  4.3924e-02,  1.9846e-02,\n",
      "          2.2947e-02, -5.2915e-02, -4.7777e-02],\n",
      "        [ 1.7383e-02,  1.6067e-02, -1.0461e-02, -3.3933e-02,  5.4167e-02,\n",
      "          1.6452e-02,  5.0426e-03, -2.2208e-02],\n",
      "        [-3.4026e-02,  5.7517e-02,  5.9441e-02, -1.3495e-02,  2.3720e-03,\n",
      "          1.2549e-02,  4.1178e-03, -5.8415e-02],\n",
      "        [-3.0725e-02,  2.0308e-02, -6.0721e-02,  2.2121e-02, -5.8011e-02,\n",
      "         -2.4912e-02,  3.6665e-02, -3.0983e-02],\n",
      "        [ 4.6634e-02, -2.7548e-02,  7.1491e-03,  3.2239e-02, -5.1565e-02,\n",
      "          4.6260e-03,  1.8811e-02, -3.5930e-02],\n",
      "        [-2.1087e-02, -4.0082e-02,  1.8442e-02, -9.9204e-04, -3.8334e-03,\n",
      "         -3.1644e-02,  5.4632e-02,  2.7744e-02],\n",
      "        [-4.4340e-02, -3.6218e-02,  2.5720e-03, -1.3942e-02, -3.4383e-03,\n",
      "         -3.7049e-02,  3.4741e-02,  5.0575e-05],\n",
      "        [-4.8560e-02, -2.8112e-02, -2.4674e-03,  1.9805e-02,  1.1498e-02,\n",
      "         -1.2846e-03,  5.8938e-02,  3.9208e-02]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.weight_hh_l0\n",
      "-->para Parameter containing:\n",
      "tensor([[ 0.0047, -0.0493,  0.0035,  0.0470,  0.0297,  0.0401, -0.0314, -0.0528],\n",
      "        [-0.0256,  0.0515,  0.0585,  0.0416,  0.0608,  0.0589, -0.0467,  0.0281],\n",
      "        [-0.0249,  0.0327, -0.0258, -0.0147,  0.0297,  0.0120, -0.0176,  0.0542],\n",
      "        [ 0.0152,  0.0039,  0.0569, -0.0187,  0.0437,  0.0049, -0.0144,  0.0264],\n",
      "        [ 0.0283,  0.0291,  0.0003, -0.0512,  0.0563,  0.0084,  0.0072, -0.0432],\n",
      "        [-0.0518, -0.0179, -0.0298, -0.0311,  0.0306,  0.0352, -0.0249, -0.0137],\n",
      "        [ 0.0390, -0.0414, -0.0551,  0.0125, -0.0288, -0.0291, -0.0473,  0.0060],\n",
      "        [-0.0525, -0.0542,  0.0191,  0.0404, -0.0547, -0.0516,  0.0438,  0.0304]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.bias_ih_l0\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.0146,  0.1368,  0.3103,  0.2123, -0.1209, -0.2112,  0.2328, -0.1193])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.bias_hh_l0\n",
      "-->para Parameter containing:\n",
      "tensor([ 0.1335, -0.0171,  0.0444,  0.3416,  0.3133, -0.2585, -0.1337,  0.1746])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_fc_minus_0.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[ 1.4534, -1.6630,  1.3558,  0.0491, -1.4952,  0.3348,  0.1626, -0.7754],\n",
      "        [ 1.4152,  1.2763, -1.1242, -1.0411, -1.5024,  1.3921,  0.9095,  0.9524]])\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_fc_minus_1.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[ 0.1203,  0.2806, -0.1348,  0.2496,  0.1276,  0.1836,  0.2208,  0.3199],\n",
      "        [-0.2979,  0.1693,  0.1886,  0.0770,  0.2392,  0.2455,  0.2546,  0.3301]],\n",
      "       requires_grad=True)\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_fc_minus_2.weight\n",
      "-->para Parameter containing:\n",
      "tensor([[-0.0577, -0.0317,  0.3475,  0.2796, -0.0193, -0.1064,  0.1367,  0.2718],\n",
      "        [-0.0645, -0.2749,  0.3213, -0.1694,  0.1505,  0.0619, -0.3239, -0.3387]],\n",
      "       requires_grad=True)\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Before update\")\n",
    "print(\"=\"*50)\n",
    "for name, parms in MLA_model.named_parameters():\n",
    "    print('-->name', name)\n",
    "    print('-->para', parms)\n",
    "    print('-->grad_requires', parms.requires_grad)\n",
    "    print('-->grad_value', parms.grad)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_loss.backward(retain_graph=True)\n",
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward\n",
      "==================================================\n",
      "-->name low_d_readin_s.weight\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name low_d_readin_s.bias\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name align_layer.weight\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name low_d_readin_t.weight\n",
      "-->grad_requires True\n",
      "-->grad_value tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "==================================================\n",
      "-->name low_d_readin_t.bias\n",
      "-->grad_requires True\n",
      "-->grad_value tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_ih_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_hh_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_ih_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_hh_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_ih_l1\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.weight_hh_l1\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_ih_l1\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name encoder_rnn.bias_hh_l1\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_mu_1.weight\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_mu_1.bias\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_log_var_1.weight\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name fc_log_var_1.bias\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.weight_ih_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.weight_hh_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.bias_ih_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_rnn.bias_hh_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc1.weight\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc1.bias\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc2.weight\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name sde_fc2.bias\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.weight_ih_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.weight_hh_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.bias_ih_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_rnn.bias_hh_l0\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_fc_minus_0.weight\n",
      "-->grad_requires False\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_fc_minus_1.weight\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n",
      "-->name vde_fc_minus_2.weight\n",
      "-->grad_requires True\n",
      "-->grad_value None\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"After backward\")\n",
    "print(\"=\"*50)\n",
    "for name, parms in MLA_model.named_parameters():\n",
    "    print('-->name', name)\n",
    "    # print('-->para', parms)\n",
    "    print('-->grad_requires', parms.requires_grad)\n",
    "    print('-->grad_value', parms.grad)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    }
   ],
   "source": [
    "print(MLA_model.low_d_readin_t.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, total loss is: 0.6069204211235046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheny/anaconda3/envs/torch/lib/python3.11/site-packages/ot/bregman/_sinkhorn.py:506: UserWarning: Warning: numerical errors at iteration 0\n",
      "  warnings.warn('Warning: numerical errors at iteration %d' % ii)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (epoch \u001b[38;5;241m==\u001b[39m n_epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total loss is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     current_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(logger_performance(MLA_model))\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_metric \u001b[38;5;241m>\u001b[39m key_metric:\n\u001b[1;32m     33\u001b[0m         key_metric \u001b[38;5;241m=\u001b[39m current_metric\n",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m, in \u001b[0;36mlogger_performance\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(test_trial_vel_tide.reshape((-1,2)))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(vel_hat_test.reshape((-1,2)))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m key_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m r2_score(test_trial_vel_tide\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)),vel_hat_test\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)), multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  key_metric\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1204\u001b[0m, in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[1;32m   1198\u001b[0m xp, _, device_ \u001b[38;5;241m=\u001b[39m get_namespace_and_device(\n\u001b[1;32m   1199\u001b[0m     y_true, y_pred, sample_weight, multioutput\n\u001b[1;32m   1200\u001b[0m )\n\u001b[1;32m   1202\u001b[0m dtype \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m-> 1204\u001b[0m _, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[1;32m   1205\u001b[0m     y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[1;32m   1206\u001b[0m )\n\u001b[1;32m   1207\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y_pred) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "total_loss_list = []\n",
    "for epoch in range(epoches):\n",
    "\n",
    "    optimizer_mla.zero_grad()\n",
    "\n",
    "    re_sp, _, distri_0, distri_k, latents_k, output_sh_loss, log_var = MLA_model(spike_day_0, spike_day_k, p, q, train_flag=True)\n",
    "\n",
    "    total_loss = output_sh_loss\n",
    "\n",
    "    latents_k = latents_k[:, None, :, :]\n",
    "    latents_k = torch.transpose(latents_k,3,2)\n",
    "\n",
    "    batch_size = latents_k.shape[0]\n",
    "    t = torch.randint(0, timesteps, (batch_size,), device=\"cpu\").long()\n",
    "    noise = torch.randn_like(latents_k)\n",
    "\n",
    "    z_noisy = q_sample(x_start=latents_k, t=t, noise=noise)\n",
    "    predicted_noise = diff_model(z_noisy, t)\n",
    "    total_loss += appro_alpha * F.smooth_l1_loss(noise, predicted_noise)\n",
    "    \n",
    "    total_loss += skilling_divergence(z_noisy,latents_k,t)\n",
    "\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer_mla.step()\n",
    "\n",
    "    total_loss_list.append(total_loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if (epoch % 5 == 0) or (epoch == n_epochs-1):\n",
    "            print(f\"epoch={epoch}, total loss is: {total_loss.item()}\")\n",
    "            current_metric = float(logger_performance(MLA_model))\n",
    "            if current_metric > key_metric:\n",
    "                key_metric = current_metric\n",
    "            if total_loss < pre_total_loss_:\n",
    "                torch.save(MLA_model.state_dict(),'vae_model_mla')\n",
    "                pre_total_loss_ = total_loss "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
